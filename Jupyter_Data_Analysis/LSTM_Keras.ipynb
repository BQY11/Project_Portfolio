{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM_model_common.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "KfcZ0g-4Z4VQ",
        "dtGqOKmXZx30",
        "a-vM6EcLh4OL"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfcZ0g-4Z4VQ"
      },
      "source": [
        "#Step 0 mount your own drive with shared files\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esvfgt8D1sRw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01041eca-56ab-4bcc-9266-3e3f69449a64"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtGqOKmXZx30"
      },
      "source": [
        "# Step 1 import all lib"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJDa9X_QzuTS"
      },
      "source": [
        "import sys\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import Sequential, layers, utils\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "np.set_printoptions(threshold=sys.maxsize)\n",
        "import matplotlib.pyplot as plt\n",
        "# %matplotlib notebook\n",
        "from math import hypot\n",
        "from itertools import combinations\n",
        "from sklearn.model_selection import KFold\n",
        "import gc\n",
        "class GarbageCollectorCallback(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        gc.collect()\n",
        "from tensorflow.keras import regularizers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vamPn3CKzqk"
      },
      "source": [
        "# Step 2 : Run this Block : \n",
        "## Prepare Data : contain 3 functions "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCzNCOvI_et4"
      },
      "source": [
        "def create_dataset(X, y):\n",
        "    features = []\n",
        "    targets = []\n",
        "    \n",
        "    for i in range(0, len(X)): \n",
        "        data = [[i] for i in X[i]] # 序列数据  \n",
        "        label = [y[i]] # 标签数据\n",
        "        \n",
        "        # 保存到features和labels\n",
        "        features.append(data)\n",
        "        targets.append(label)\n",
        "    \n",
        "    # 返回\n",
        "    return np.array(features), np.array(targets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QeOJmll_gPe"
      },
      "source": [
        "# 数据集切分\n",
        "# 功能函数：基于新的特征的数据集和标签集，切分：X_train, X_test\n",
        "\n",
        "def split_dataset(x, y, train_ratio=0.8):\n",
        "\n",
        "    x_len = len(x) # 特征数据集X的样本数量\n",
        "    train_data_len = int(x_len * train_ratio) # 训练集的样本数量\n",
        "    \n",
        "    x_train = x[:train_data_len] # 训练集\n",
        "    y_train = y[:train_data_len] # 训练标签集\n",
        "    \n",
        "    x_test = x[train_data_len:] # 测试集\n",
        "    y_test = y[train_data_len:] # 测试集标签集\n",
        "    \n",
        "    # 返回值\n",
        "    return x_train, x_test, y_train, y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txw2orHc_n5i"
      },
      "source": [
        "# def create_batch_dataset(X, y, train=True, buffer_size=1000, batch_size=128):\n",
        "#     batch_data = tf.data.Dataset.from_tensor_slices((tf.constant(X), tf.constant(y))) # 数据封装，tensor类型\n",
        "#     if train: # training set\n",
        "#         return batch_data.cache().shuffle(buffer_size).batch(batch_size)\n",
        "#     else: # testing set\n",
        "#         return batch_data.batch(batch_size)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQClaj-swd7-"
      },
      "source": [
        "# Step 3: Run this Block \n",
        "## Model build base on Delete NaN File \n",
        "### Need to change file path to your own drive file path"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ae1gjow6xxS0"
      },
      "source": [
        "seed = 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twvVIQcuu-7s"
      },
      "source": [
        "filenum = 1\n",
        "gsize = 2\n",
        "shuffle = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yx9eyRt_EcwZ"
      },
      "source": [
        "dataset_x = []\n",
        "dataset_y = []\n",
        "import random\n",
        "\n",
        "with open('/content/drive/MyDrive/ColabNotebooks/{size}mm_file/outfile{fnum}/trainingfile_{size}mm.txt'.format(size = gsize, fnum = filenum), 'r') as f:\n",
        "    lines = f.readlines()\n",
        "    if shuffle:\n",
        "      random.Random(seed).shuffle(lines)\n",
        "    else:\n",
        "      pass\n",
        "    # print(lines[10])\n",
        "    for line in lines:\n",
        "        line = line.strip(\"\\n\")\n",
        "        dataset_x.append(line.split(\"|\")[0].split(\",\"))\n",
        "        dataset_y.append(line.split(\"|\")[1])\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wRUN2Yy2-hF4",
        "outputId": "8ba8e543-4a00-4604-e517-57a076cada52"
      },
      "source": [
        "len(dataset_x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "27658"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hlo6KZsQqDb5"
      },
      "source": [
        "dataset_x\n",
        "lable = [float(y) for y in dataset_y]\n",
        "input_x = []\n",
        "for grp in dataset_x:\n",
        "  input_x.append([float(z) for z in grp])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_2YDzmqyJJU"
      },
      "source": [
        "input_x,lable = create_dataset(input_x, lable)\n",
        "x_train, x_test, y_train, y_test = split_dataset(input_x, lable, train_ratio=0.80)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGVLxf_CinQk"
      },
      "source": [
        "# input_x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-vM6EcLh4OL"
      },
      "source": [
        "## Data shape"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMdS18qjwXbi"
      },
      "source": [
        "# input_x.shape\n",
        "# lable.shape\n",
        "# print(x_train.shape , x_test.shape)\n",
        "# print(y_train.shape, y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-dcorDNiJ9R"
      },
      "source": [
        "## LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ir2ltCeTymXQ"
      },
      "source": [
        "file_path = \"/content/drive/MyDrive/ColabNotebooks/best_checkpoint.hdf5\"\n",
        "\n",
        "\n",
        "earlyStopping=tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=50, verbose=1, mode='auto')\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=file_path, \n",
        "                                                         monitor='loss', \n",
        "                                                         mode='min', \n",
        "                                                         save_best_only=True,\n",
        "                                                         save_weights_only=True)\n",
        "\n",
        "learning_rate_reduction = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', \n",
        "                                            patience=50, \n",
        "                                            verbose=1, \n",
        "                                            factor=0.5, \n",
        "                                            min_lr=0.00001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMJfYgBAK8X7"
      },
      "source": [
        "# paramemaers\n",
        "# LSTM\n",
        "lstm_units = 128\n",
        "acti = 'relu'\n",
        "drop = 0\n",
        "regu = None #regularizers.l2(1e-4)\n",
        "batch = 128\n",
        "\n",
        "# optimizer\n",
        "lr = 0.001\n",
        "mom = 0.01\n",
        "\n",
        "#complie\n",
        "callback_file = [checkpoint_callback, learning_rate_reduction] #[earlyStopping,checkpoint_callback] #None\n",
        "epo = 1000\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qPimxtIyjt-"
      },
      "source": [
        "model = Sequential([\n",
        "        #LSTM layer\n",
        "        layers.LSTM(units = lstm_units, \n",
        "                    input_shape=(9, 1),\n",
        "                    activation = acti, \n",
        "                    recurrent_activation='sigmoid', \n",
        "                    use_bias=True, \n",
        "                    kernel_initializer='glorot_uniform', \n",
        "                    recurrent_initializer='orthogonal', \n",
        "                    bias_initializer='zeros', \n",
        "                    unit_forget_bias=True, \n",
        "                    kernel_regularizer= regu, \n",
        "                    recurrent_regularizer= regu, \n",
        "                    bias_regularizer= regu, \n",
        "                    activity_regularizer=None, \n",
        "                    kernel_constraint=None, \n",
        "                    recurrent_constraint=None, \n",
        "                    bias_constraint=None, \n",
        "                    dropout= drop, \n",
        "                    recurrent_dropout=0.0, \n",
        "                    implementation=1, \n",
        "                    return_sequences=False, \n",
        "                    return_state=False, \n",
        "                    go_backwards=False, \n",
        "                    stateful=False, \n",
        "                    unroll=False),\n",
        "          \n",
        "          layers.Dense(256, \n",
        "                   activation=acti, \n",
        "                   use_bias=True, \n",
        "                   kernel_initializer='glorot_uniform', \n",
        "                   bias_initializer='zeros', \n",
        "                   kernel_regularizer=None, \n",
        "                   bias_regularizer=None, \n",
        "                   activity_regularizer=None, \n",
        "                   kernel_constraint=None, \n",
        "                   bias_constraint=None),             \n",
        "          # layers.Dense(128, activation=acti),\n",
        "          layers.Dense(64, activation=acti), #relu\n",
        "          layers.Dense(1) #no act\n",
        "  ])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oizeH-e6yp9n"
      },
      "source": [
        "opt = keras.optimizers.SGD(learning_rate=lr, momentum= mom, decay=0.0, nesterov= False)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=opt,\n",
        "    loss=\"mae\", #mse\n",
        "    metrics=None,\n",
        "    loss_weights=None,\n",
        "    weighted_metrics=None,\n",
        "    run_eagerly=None,\n",
        "    steps_per_execution=None,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5tDouPwyruz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cbcd9cf-2160-434d-90c0-d3c221355a2b"
      },
      "source": [
        "history = model.fit(\n",
        "          x_train,\n",
        "          y_train,\n",
        "          batch_size=batch,\n",
        "          epochs=epo,\n",
        "          verbose=\"auto\",\n",
        "          callbacks = callback_file,\n",
        "          validation_split= 0,\n",
        "          validation_data = (x_test, y_test),\n",
        "          shuffle= False, \n",
        "          class_weight=None,\n",
        "          sample_weight=None,\n",
        "          initial_epoch=0,\n",
        "          steps_per_epoch=None,\n",
        "          validation_steps=None,\n",
        "          validation_batch_size=None,\n",
        "          validation_freq=1,\n",
        "          max_queue_size=10,\n",
        "          workers=1,\n",
        "          use_multiprocessing=False,\n",
        "      )        "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "173/173 [==============================] - 8s 36ms/step - loss: 1.1292 - val_loss: 1.0714 - lr: 0.0010\n",
            "Epoch 2/1000\n",
            "173/173 [==============================] - 6s 32ms/step - loss: 1.0876 - val_loss: 1.0602 - lr: 0.0010\n",
            "Epoch 3/1000\n",
            "173/173 [==============================] - 6s 32ms/step - loss: 1.0646 - val_loss: 1.0484 - lr: 0.0010\n",
            "Epoch 4/1000\n",
            "173/173 [==============================] - 6s 32ms/step - loss: 1.0402 - val_loss: 1.0262 - lr: 0.0010\n",
            "Epoch 5/1000\n",
            "173/173 [==============================] - 6s 32ms/step - loss: 1.0310 - val_loss: 1.0210 - lr: 0.0010\n",
            "Epoch 6/1000\n",
            "173/173 [==============================] - 6s 32ms/step - loss: 1.0246 - val_loss: 1.0187 - lr: 0.0010\n",
            "Epoch 7/1000\n",
            "173/173 [==============================] - 6s 32ms/step - loss: 1.0204 - val_loss: 1.0006 - lr: 0.0010\n",
            "Epoch 8/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 1.0148 - val_loss: 1.0046 - lr: 0.0010\n",
            "Epoch 9/1000\n",
            "173/173 [==============================] - 6s 32ms/step - loss: 1.0106 - val_loss: 0.9921 - lr: 0.0010\n",
            "Epoch 10/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 1.0059 - val_loss: 0.9910 - lr: 0.0010\n",
            "Epoch 11/1000\n",
            "173/173 [==============================] - 6s 32ms/step - loss: 1.0366 - val_loss: 0.9884 - lr: 0.0010\n",
            "Epoch 12/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 1.0030 - val_loss: 0.9848 - lr: 0.0010\n",
            "Epoch 13/1000\n",
            "173/173 [==============================] - 6s 32ms/step - loss: 0.9912 - val_loss: 0.9680 - lr: 0.0010\n",
            "Epoch 14/1000\n",
            "173/173 [==============================] - 6s 32ms/step - loss: 0.9881 - val_loss: 0.9604 - lr: 0.0010\n",
            "Epoch 15/1000\n",
            "173/173 [==============================] - 6s 32ms/step - loss: 0.9849 - val_loss: 0.9645 - lr: 0.0010\n",
            "Epoch 16/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.9800 - val_loss: 0.9692 - lr: 0.0010\n",
            "Epoch 17/1000\n",
            "173/173 [==============================] - 6s 32ms/step - loss: 0.9740 - val_loss: 0.9520 - lr: 0.0010\n",
            "Epoch 18/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.9720 - val_loss: 0.9492 - lr: 0.0010\n",
            "Epoch 19/1000\n",
            "173/173 [==============================] - 6s 32ms/step - loss: 0.9674 - val_loss: 0.9411 - lr: 0.0010\n",
            "Epoch 20/1000\n",
            "173/173 [==============================] - 6s 32ms/step - loss: 0.9621 - val_loss: 0.9350 - lr: 0.0010\n",
            "Epoch 21/1000\n",
            "173/173 [==============================] - 6s 32ms/step - loss: 0.9588 - val_loss: 0.9435 - lr: 0.0010\n",
            "Epoch 22/1000\n",
            "173/173 [==============================] - 6s 32ms/step - loss: 0.9612 - val_loss: 0.9632 - lr: 0.0010\n",
            "Epoch 23/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.9581 - val_loss: 0.9530 - lr: 0.0010\n",
            "Epoch 24/1000\n",
            "173/173 [==============================] - 6s 32ms/step - loss: 0.9465 - val_loss: 0.9272 - lr: 0.0010\n",
            "Epoch 25/1000\n",
            "173/173 [==============================] - 6s 32ms/step - loss: 0.9539 - val_loss: 0.9416 - lr: 0.0010\n",
            "Epoch 26/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.9384 - val_loss: 0.9284 - lr: 0.0010\n",
            "Epoch 27/1000\n",
            "173/173 [==============================] - 6s 32ms/step - loss: 0.9426 - val_loss: 0.9158 - lr: 0.0010\n",
            "Epoch 28/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.9290 - val_loss: 0.9190 - lr: 0.0010\n",
            "Epoch 29/1000\n",
            "173/173 [==============================] - 6s 32ms/step - loss: 0.9338 - val_loss: 0.9322 - lr: 0.0010\n",
            "Epoch 30/1000\n",
            "173/173 [==============================] - 6s 32ms/step - loss: 0.9297 - val_loss: 0.9127 - lr: 0.0010\n",
            "Epoch 31/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.9269 - val_loss: 0.9328 - lr: 0.0010\n",
            "Epoch 32/1000\n",
            "173/173 [==============================] - 6s 32ms/step - loss: 0.9153 - val_loss: 0.8846 - lr: 0.0010\n",
            "Epoch 33/1000\n",
            "173/173 [==============================] - 6s 32ms/step - loss: 0.9257 - val_loss: 0.9239 - lr: 0.0010\n",
            "Epoch 34/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.9068 - val_loss: 0.9238 - lr: 0.0010\n",
            "Epoch 35/1000\n",
            "173/173 [==============================] - 6s 32ms/step - loss: 0.9072 - val_loss: 0.9259 - lr: 0.0010\n",
            "Epoch 36/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.8988 - val_loss: 0.9223 - lr: 0.0010\n",
            "Epoch 37/1000\n",
            "173/173 [==============================] - 6s 32ms/step - loss: 0.9025 - val_loss: 0.9161 - lr: 0.0010\n",
            "Epoch 38/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.8926 - val_loss: 0.9086 - lr: 0.0010\n",
            "Epoch 39/1000\n",
            "173/173 [==============================] - 6s 32ms/step - loss: 0.8906 - val_loss: 0.9320 - lr: 0.0010\n",
            "Epoch 40/1000\n",
            "173/173 [==============================] - 6s 32ms/step - loss: 0.8816 - val_loss: 0.9555 - lr: 0.0010\n",
            "Epoch 41/1000\n",
            "173/173 [==============================] - 6s 32ms/step - loss: 0.8784 - val_loss: 0.9165 - lr: 0.0010\n",
            "Epoch 42/1000\n",
            "173/173 [==============================] - 6s 32ms/step - loss: 0.8781 - val_loss: 0.9223 - lr: 0.0010\n",
            "Epoch 43/1000\n",
            "173/173 [==============================] - 6s 32ms/step - loss: 0.8696 - val_loss: 0.8628 - lr: 0.0010\n",
            "Epoch 44/1000\n",
            "173/173 [==============================] - 6s 32ms/step - loss: 0.8561 - val_loss: 0.8507 - lr: 0.0010\n",
            "Epoch 45/1000\n",
            "173/173 [==============================] - 6s 32ms/step - loss: 0.8955 - val_loss: 0.8932 - lr: 0.0010\n",
            "Epoch 46/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.8560 - val_loss: 0.8328 - lr: 0.0010\n",
            "Epoch 47/1000\n",
            "173/173 [==============================] - 6s 32ms/step - loss: 0.8491 - val_loss: 0.8897 - lr: 0.0010\n",
            "Epoch 48/1000\n",
            "173/173 [==============================] - 6s 32ms/step - loss: 0.8481 - val_loss: 0.8384 - lr: 0.0010\n",
            "Epoch 49/1000\n",
            "173/173 [==============================] - 6s 32ms/step - loss: 0.8507 - val_loss: 0.8568 - lr: 0.0010\n",
            "Epoch 50/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.8267 - val_loss: 0.7782 - lr: 0.0010\n",
            "Epoch 51/1000\n",
            "173/173 [==============================] - 6s 32ms/step - loss: 0.8272 - val_loss: 0.7696 - lr: 0.0010\n",
            "Epoch 52/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.8161 - val_loss: 0.7937 - lr: 0.0010\n",
            "Epoch 53/1000\n",
            "173/173 [==============================] - 6s 32ms/step - loss: 0.8521 - val_loss: 0.8261 - lr: 0.0010\n",
            "Epoch 54/1000\n",
            "173/173 [==============================] - 6s 32ms/step - loss: 0.8201 - val_loss: 0.9698 - lr: 0.0010\n",
            "Epoch 55/1000\n",
            "173/173 [==============================] - 6s 32ms/step - loss: 0.8274 - val_loss: 0.9129 - lr: 0.0010\n",
            "Epoch 56/1000\n",
            "173/173 [==============================] - 6s 32ms/step - loss: 0.9082 - val_loss: 0.8716 - lr: 0.0010\n",
            "Epoch 57/1000\n",
            "173/173 [==============================] - 6s 32ms/step - loss: 0.8455 - val_loss: 0.7799 - lr: 0.0010\n",
            "Epoch 58/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.7891 - val_loss: 0.8208 - lr: 0.0010\n",
            "Epoch 59/1000\n",
            "173/173 [==============================] - 6s 32ms/step - loss: 0.7827 - val_loss: 0.8834 - lr: 0.0010\n",
            "Epoch 60/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.7778 - val_loss: 0.7798 - lr: 0.0010\n",
            "Epoch 61/1000\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.7733 - val_loss: 1.0240 - lr: 0.0010\n",
            "Epoch 62/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.7901 - val_loss: 0.8889 - lr: 0.0010\n",
            "Epoch 63/1000\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.7586 - val_loss: 0.7948 - lr: 0.0010\n",
            "Epoch 64/1000\n",
            "173/173 [==============================] - 6s 32ms/step - loss: 0.8728 - val_loss: 0.8431 - lr: 0.0010\n",
            "Epoch 65/1000\n",
            "173/173 [==============================] - 6s 32ms/step - loss: 0.8557 - val_loss: 0.8446 - lr: 0.0010\n",
            "Epoch 66/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.8506 - val_loss: 0.8074 - lr: 0.0010\n",
            "Epoch 67/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.8038 - val_loss: 0.8313 - lr: 0.0010\n",
            "Epoch 68/1000\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.7421 - val_loss: 0.7009 - lr: 0.0010\n",
            "Epoch 69/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.7317 - val_loss: 0.8470 - lr: 0.0010\n",
            "Epoch 70/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.7455 - val_loss: 0.8144 - lr: 0.0010\n",
            "Epoch 71/1000\n",
            "173/173 [==============================] - 6s 32ms/step - loss: 1.0189 - val_loss: 0.9069 - lr: 0.0010\n",
            "Epoch 72/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.9089 - val_loss: 0.8889 - lr: 0.0010\n",
            "Epoch 73/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.9010 - val_loss: 0.8837 - lr: 0.0010\n",
            "Epoch 74/1000\n",
            "173/173 [==============================] - 6s 32ms/step - loss: 0.9731 - val_loss: 0.9131 - lr: 0.0010\n",
            "Epoch 75/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.9221 - val_loss: 0.8968 - lr: 0.0010\n",
            "Epoch 76/1000\n",
            "173/173 [==============================] - 6s 32ms/step - loss: 0.9096 - val_loss: 0.8934 - lr: 0.0010\n",
            "Epoch 77/1000\n",
            "173/173 [==============================] - 6s 32ms/step - loss: 0.9050 - val_loss: 0.8917 - lr: 0.0010\n",
            "Epoch 78/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.9020 - val_loss: 0.8884 - lr: 0.0010\n",
            "Epoch 79/1000\n",
            "173/173 [==============================] - 6s 32ms/step - loss: 0.8992 - val_loss: 0.8839 - lr: 0.0010\n",
            "Epoch 80/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.8969 - val_loss: 0.8821 - lr: 0.0010\n",
            "Epoch 81/1000\n",
            "173/173 [==============================] - 6s 32ms/step - loss: 0.8949 - val_loss: 0.8789 - lr: 0.0010\n",
            "Epoch 82/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.8924 - val_loss: 0.8776 - lr: 0.0010\n",
            "Epoch 83/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.8903 - val_loss: 0.8765 - lr: 0.0010\n",
            "Epoch 84/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.8879 - val_loss: 0.8711 - lr: 0.0010\n",
            "Epoch 85/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.8863 - val_loss: 0.8716 - lr: 0.0010\n",
            "Epoch 86/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.8834 - val_loss: 0.8678 - lr: 0.0010\n",
            "Epoch 87/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.8812 - val_loss: 0.8640 - lr: 0.0010\n",
            "Epoch 88/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.8793 - val_loss: 0.8637 - lr: 0.0010\n",
            "Epoch 89/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.8773 - val_loss: 0.8611 - lr: 0.0010\n",
            "Epoch 90/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.8760 - val_loss: 0.8582 - lr: 0.0010\n",
            "Epoch 91/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.8739 - val_loss: 0.8588 - lr: 0.0010\n",
            "Epoch 92/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.8724 - val_loss: 0.8610 - lr: 0.0010\n",
            "Epoch 93/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.8692 - val_loss: 0.8548 - lr: 0.0010\n",
            "Epoch 94/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.8688 - val_loss: 0.8542 - lr: 0.0010\n",
            "Epoch 95/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.8669 - val_loss: 0.8536 - lr: 0.0010\n",
            "Epoch 96/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.8666 - val_loss: 0.8529 - lr: 0.0010\n",
            "Epoch 97/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.8638 - val_loss: 0.8509 - lr: 0.0010\n",
            "Epoch 98/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.8611 - val_loss: 0.8436 - lr: 0.0010\n",
            "Epoch 99/1000\n",
            "173/173 [==============================] - 6s 32ms/step - loss: 0.8596 - val_loss: 0.8504 - lr: 0.0010\n",
            "Epoch 100/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.8565 - val_loss: 0.8429 - lr: 0.0010\n",
            "Epoch 101/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.8550 - val_loss: 0.8448 - lr: 0.0010\n",
            "Epoch 102/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.8510 - val_loss: 0.8365 - lr: 0.0010\n",
            "Epoch 103/1000\n",
            "173/173 [==============================] - 6s 32ms/step - loss: 0.8532 - val_loss: 0.8392 - lr: 0.0010\n",
            "Epoch 104/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.8513 - val_loss: 0.8395 - lr: 0.0010\n",
            "Epoch 105/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.8532 - val_loss: 0.8386 - lr: 0.0010\n",
            "Epoch 106/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.8440 - val_loss: 0.8504 - lr: 0.0010\n",
            "Epoch 107/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.8376 - val_loss: 0.8413 - lr: 0.0010\n",
            "Epoch 108/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.8347 - val_loss: 0.8375 - lr: 0.0010\n",
            "Epoch 109/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.8261 - val_loss: 0.8065 - lr: 0.0010\n",
            "Epoch 110/1000\n",
            "173/173 [==============================] - 6s 32ms/step - loss: 0.8229 - val_loss: 0.8096 - lr: 0.0010\n",
            "Epoch 111/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.8167 - val_loss: 0.8054 - lr: 0.0010\n",
            "Epoch 112/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.8151 - val_loss: 0.7955 - lr: 0.0010\n",
            "Epoch 113/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.8126 - val_loss: 0.8003 - lr: 0.0010\n",
            "Epoch 114/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.8083 - val_loss: 0.7971 - lr: 0.0010\n",
            "Epoch 115/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.8086 - val_loss: 0.8033 - lr: 0.0010\n",
            "Epoch 116/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.8059 - val_loss: 0.7958 - lr: 0.0010\n",
            "Epoch 117/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.8045 - val_loss: 0.8029 - lr: 0.0010\n",
            "Epoch 118/1000\n",
            "173/173 [==============================] - ETA: 0s - loss: 0.8025\n",
            "Epoch 00118: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.8025 - val_loss: 0.7939 - lr: 0.0010\n",
            "Epoch 119/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.7835 - val_loss: 0.7686 - lr: 5.0000e-04\n",
            "Epoch 120/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.7817 - val_loss: 0.7707 - lr: 5.0000e-04\n",
            "Epoch 121/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.7799 - val_loss: 0.7642 - lr: 5.0000e-04\n",
            "Epoch 122/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.7788 - val_loss: 0.7669 - lr: 5.0000e-04\n",
            "Epoch 123/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.7769 - val_loss: 0.7589 - lr: 5.0000e-04\n",
            "Epoch 124/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.7750 - val_loss: 0.7638 - lr: 5.0000e-04\n",
            "Epoch 125/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.7729 - val_loss: 0.7622 - lr: 5.0000e-04\n",
            "Epoch 126/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.7707 - val_loss: 0.7562 - lr: 5.0000e-04\n",
            "Epoch 127/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.7664 - val_loss: 0.7584 - lr: 5.0000e-04\n",
            "Epoch 128/1000\n",
            "173/173 [==============================] - 6s 32ms/step - loss: 0.7648 - val_loss: 0.7493 - lr: 5.0000e-04\n",
            "Epoch 129/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.7621 - val_loss: 0.7347 - lr: 5.0000e-04\n",
            "Epoch 130/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.7596 - val_loss: 0.7312 - lr: 5.0000e-04\n",
            "Epoch 131/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.7569 - val_loss: 0.7237 - lr: 5.0000e-04\n",
            "Epoch 132/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.7519 - val_loss: 0.7217 - lr: 5.0000e-04\n",
            "Epoch 133/1000\n",
            "173/173 [==============================] - 6s 32ms/step - loss: 0.7470 - val_loss: 0.7225 - lr: 5.0000e-04\n",
            "Epoch 134/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.7463 - val_loss: 0.7128 - lr: 5.0000e-04\n",
            "Epoch 135/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.7446 - val_loss: 0.7197 - lr: 5.0000e-04\n",
            "Epoch 136/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.7412 - val_loss: 0.7217 - lr: 5.0000e-04\n",
            "Epoch 137/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.7348 - val_loss: 0.6847 - lr: 5.0000e-04\n",
            "Epoch 138/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.7363 - val_loss: 0.7037 - lr: 5.0000e-04\n",
            "Epoch 139/1000\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.7266 - val_loss: 0.6806 - lr: 5.0000e-04\n",
            "Epoch 140/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.7264 - val_loss: 0.6529 - lr: 5.0000e-04\n",
            "Epoch 141/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.7250 - val_loss: 0.7055 - lr: 5.0000e-04\n",
            "Epoch 142/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.7210 - val_loss: 0.7076 - lr: 5.0000e-04\n",
            "Epoch 143/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.7280 - val_loss: 0.7172 - lr: 5.0000e-04\n",
            "Epoch 144/1000\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.7173 - val_loss: 0.6418 - lr: 5.0000e-04\n",
            "Epoch 145/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.7283 - val_loss: 0.7306 - lr: 5.0000e-04\n",
            "Epoch 146/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.7176 - val_loss: 0.7252 - lr: 5.0000e-04\n",
            "Epoch 147/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.8653 - val_loss: 0.8317 - lr: 5.0000e-04\n",
            "Epoch 148/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.8290 - val_loss: 0.8090 - lr: 5.0000e-04\n",
            "Epoch 149/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.7998 - val_loss: 0.7634 - lr: 5.0000e-04\n",
            "Epoch 150/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.7448 - val_loss: 0.7340 - lr: 5.0000e-04\n",
            "Epoch 151/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.7143 - val_loss: 0.7110 - lr: 5.0000e-04\n",
            "Epoch 152/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.7003 - val_loss: 0.7041 - lr: 5.0000e-04\n",
            "Epoch 153/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.6852 - val_loss: 0.6644 - lr: 5.0000e-04\n",
            "Epoch 154/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.6841 - val_loss: 0.6263 - lr: 5.0000e-04\n",
            "Epoch 155/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.6825 - val_loss: 0.6073 - lr: 5.0000e-04\n",
            "Epoch 156/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.7579 - val_loss: 0.8305 - lr: 5.0000e-04\n",
            "Epoch 157/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.7973 - val_loss: 0.7586 - lr: 5.0000e-04\n",
            "Epoch 158/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.6919 - val_loss: 0.6253 - lr: 5.0000e-04\n",
            "Epoch 159/1000\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.6572 - val_loss: 0.5512 - lr: 5.0000e-04\n",
            "Epoch 160/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.6603 - val_loss: 0.6400 - lr: 5.0000e-04\n",
            "Epoch 161/1000\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.6401 - val_loss: 0.6773 - lr: 5.0000e-04\n",
            "Epoch 162/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.6284 - val_loss: 0.5111 - lr: 5.0000e-04\n",
            "Epoch 163/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.6190 - val_loss: 0.6073 - lr: 5.0000e-04\n",
            "Epoch 164/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.6140 - val_loss: 0.5681 - lr: 5.0000e-04\n",
            "Epoch 165/1000\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.6052 - val_loss: 0.5303 - lr: 5.0000e-04\n",
            "Epoch 166/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.5911 - val_loss: 0.5350 - lr: 5.0000e-04\n",
            "Epoch 167/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.5857 - val_loss: 0.4874 - lr: 5.0000e-04\n",
            "Epoch 168/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.5792 - val_loss: 0.4844 - lr: 5.0000e-04\n",
            "Epoch 169/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.5797 - val_loss: 0.4940 - lr: 5.0000e-04\n",
            "Epoch 170/1000\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.5726 - val_loss: 0.4960 - lr: 5.0000e-04\n",
            "Epoch 171/1000\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.5650 - val_loss: 0.5598 - lr: 5.0000e-04\n",
            "Epoch 172/1000\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.5596 - val_loss: 0.5377 - lr: 5.0000e-04\n",
            "Epoch 173/1000\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.5584 - val_loss: 0.5419 - lr: 5.0000e-04\n",
            "Epoch 174/1000\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.5494 - val_loss: 0.5394 - lr: 5.0000e-04\n",
            "Epoch 175/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.5503 - val_loss: 0.4401 - lr: 5.0000e-04\n",
            "Epoch 176/1000\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.5490 - val_loss: 0.4828 - lr: 5.0000e-04\n",
            "Epoch 177/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.5416 - val_loss: 0.6364 - lr: 5.0000e-04\n",
            "Epoch 178/1000\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.5507 - val_loss: 0.5093 - lr: 5.0000e-04\n",
            "Epoch 179/1000\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.5279 - val_loss: 0.4984 - lr: 5.0000e-04\n",
            "Epoch 180/1000\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.5247 - val_loss: 0.4385 - lr: 5.0000e-04\n",
            "Epoch 181/1000\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.5327 - val_loss: 0.4548 - lr: 5.0000e-04\n",
            "Epoch 182/1000\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.5117 - val_loss: 0.4914 - lr: 5.0000e-04\n",
            "Epoch 183/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.5180 - val_loss: 0.4601 - lr: 5.0000e-04\n",
            "Epoch 184/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.5166 - val_loss: 0.4693 - lr: 5.0000e-04\n",
            "Epoch 185/1000\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.5041 - val_loss: 0.5270 - lr: 5.0000e-04\n",
            "Epoch 186/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.5154 - val_loss: 0.4719 - lr: 5.0000e-04\n",
            "Epoch 187/1000\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.4995 - val_loss: 0.4581 - lr: 5.0000e-04\n",
            "Epoch 188/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.4985 - val_loss: 0.3977 - lr: 5.0000e-04\n",
            "Epoch 189/1000\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.6280 - val_loss: 0.8584 - lr: 5.0000e-04\n",
            "Epoch 190/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.8052 - val_loss: 0.7533 - lr: 5.0000e-04\n",
            "Epoch 191/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.6028 - val_loss: 0.5748 - lr: 5.0000e-04\n",
            "Epoch 192/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.5204 - val_loss: 0.3891 - lr: 5.0000e-04\n",
            "Epoch 193/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.5045 - val_loss: 0.4068 - lr: 5.0000e-04\n",
            "Epoch 194/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.4937 - val_loss: 0.4305 - lr: 5.0000e-04\n",
            "Epoch 195/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.4908 - val_loss: 0.3856 - lr: 5.0000e-04\n",
            "Epoch 196/1000\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.4970 - val_loss: 0.4469 - lr: 5.0000e-04\n",
            "Epoch 197/1000\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.4878 - val_loss: 0.4152 - lr: 5.0000e-04\n",
            "Epoch 198/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.4793 - val_loss: 0.4249 - lr: 5.0000e-04\n",
            "Epoch 199/1000\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.4746 - val_loss: 0.4725 - lr: 5.0000e-04\n",
            "Epoch 200/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.4743 - val_loss: 0.4250 - lr: 5.0000e-04\n",
            "Epoch 201/1000\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.4706 - val_loss: 0.4759 - lr: 5.0000e-04\n",
            "Epoch 202/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.4709 - val_loss: 0.4102 - lr: 5.0000e-04\n",
            "Epoch 203/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.4607 - val_loss: 0.4033 - lr: 5.0000e-04\n",
            "Epoch 204/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.4713 - val_loss: 0.3745 - lr: 5.0000e-04\n",
            "Epoch 205/1000\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.4514 - val_loss: 0.3716 - lr: 5.0000e-04\n",
            "Epoch 206/1000\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.4548 - val_loss: 0.4169 - lr: 5.0000e-04\n",
            "Epoch 207/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.4673 - val_loss: 0.3693 - lr: 5.0000e-04\n",
            "Epoch 208/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.4593 - val_loss: 0.4000 - lr: 5.0000e-04\n",
            "Epoch 209/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.4492 - val_loss: 0.4465 - lr: 5.0000e-04\n",
            "Epoch 210/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.4624 - val_loss: 0.4436 - lr: 5.0000e-04\n",
            "Epoch 211/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.4555 - val_loss: 0.4027 - lr: 5.0000e-04\n",
            "Epoch 212/1000\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.4483 - val_loss: 0.5725 - lr: 5.0000e-04\n",
            "Epoch 213/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.4475 - val_loss: 0.3973 - lr: 5.0000e-04\n",
            "Epoch 214/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.4763 - val_loss: 0.4303 - lr: 5.0000e-04\n",
            "Epoch 215/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.4714 - val_loss: 0.4227 - lr: 5.0000e-04\n",
            "Epoch 216/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.4721 - val_loss: 0.4147 - lr: 5.0000e-04\n",
            "Epoch 217/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.4619 - val_loss: 0.4387 - lr: 5.0000e-04\n",
            "Epoch 218/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.4532 - val_loss: 0.5404 - lr: 5.0000e-04\n",
            "Epoch 219/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.4397 - val_loss: 0.4021 - lr: 5.0000e-04\n",
            "Epoch 220/1000\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.4391 - val_loss: 0.3638 - lr: 5.0000e-04\n",
            "Epoch 221/1000\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.4395 - val_loss: 0.3724 - lr: 5.0000e-04\n",
            "Epoch 222/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.4385 - val_loss: 0.3872 - lr: 5.0000e-04\n",
            "Epoch 223/1000\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.4326 - val_loss: 0.4093 - lr: 5.0000e-04\n",
            "Epoch 224/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.4300 - val_loss: 0.3737 - lr: 5.0000e-04\n",
            "Epoch 225/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.4287 - val_loss: 0.3777 - lr: 5.0000e-04\n",
            "Epoch 226/1000\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.4292 - val_loss: 0.3574 - lr: 5.0000e-04\n",
            "Epoch 227/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.4207 - val_loss: 0.4350 - lr: 5.0000e-04\n",
            "Epoch 228/1000\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.4233 - val_loss: 0.4514 - lr: 5.0000e-04\n",
            "Epoch 229/1000\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.4192 - val_loss: 0.4884 - lr: 5.0000e-04\n",
            "Epoch 230/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.4255 - val_loss: 0.3798 - lr: 5.0000e-04\n",
            "Epoch 231/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.4159 - val_loss: 0.3562 - lr: 5.0000e-04\n",
            "Epoch 232/1000\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.4225 - val_loss: 0.3570 - lr: 5.0000e-04\n",
            "Epoch 233/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.4144 - val_loss: 0.3634 - lr: 5.0000e-04\n",
            "Epoch 234/1000\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.4099 - val_loss: 0.4432 - lr: 5.0000e-04\n",
            "Epoch 235/1000\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.4140 - val_loss: 0.3774 - lr: 5.0000e-04\n",
            "Epoch 236/1000\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.4171 - val_loss: 0.3651 - lr: 5.0000e-04\n",
            "Epoch 237/1000\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.4159 - val_loss: 0.3682 - lr: 5.0000e-04\n",
            "Epoch 238/1000\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.4114 - val_loss: 0.3352 - lr: 5.0000e-04\n",
            "Epoch 239/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.4057 - val_loss: 0.3584 - lr: 5.0000e-04\n",
            "Epoch 240/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.3996 - val_loss: 0.4624 - lr: 5.0000e-04\n",
            "Epoch 241/1000\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.4001 - val_loss: 0.3989 - lr: 5.0000e-04\n",
            "Epoch 242/1000\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.3983 - val_loss: 0.3532 - lr: 5.0000e-04\n",
            "Epoch 243/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.3954 - val_loss: 0.5493 - lr: 5.0000e-04\n",
            "Epoch 244/1000\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.3981 - val_loss: 0.3955 - lr: 5.0000e-04\n",
            "Epoch 245/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.3892 - val_loss: 0.7025 - lr: 5.0000e-04\n",
            "Epoch 246/1000\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.3967 - val_loss: 0.3696 - lr: 5.0000e-04\n",
            "Epoch 247/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.3944 - val_loss: 0.3288 - lr: 5.0000e-04\n",
            "Epoch 248/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.3880 - val_loss: 0.3411 - lr: 5.0000e-04\n",
            "Epoch 249/1000\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.3937 - val_loss: 0.3807 - lr: 5.0000e-04\n",
            "Epoch 250/1000\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.3950 - val_loss: 0.3479 - lr: 5.0000e-04\n",
            "Epoch 251/1000\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.3810 - val_loss: 0.3636 - lr: 5.0000e-04\n",
            "Epoch 252/1000\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.3892 - val_loss: 0.3705 - lr: 5.0000e-04\n",
            "Epoch 253/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.3814 - val_loss: 0.5148 - lr: 5.0000e-04\n",
            "Epoch 254/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.3909 - val_loss: 0.3445 - lr: 5.0000e-04\n",
            "Epoch 255/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.3831 - val_loss: 0.3396 - lr: 5.0000e-04\n",
            "Epoch 256/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.3802 - val_loss: 0.3642 - lr: 5.0000e-04\n",
            "Epoch 257/1000\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.4025 - val_loss: 0.3750 - lr: 5.0000e-04\n",
            "Epoch 258/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.3792 - val_loss: 0.3090 - lr: 5.0000e-04\n",
            "Epoch 259/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.3750 - val_loss: 0.3648 - lr: 5.0000e-04\n",
            "Epoch 260/1000\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.3781 - val_loss: 0.4545 - lr: 5.0000e-04\n",
            "Epoch 261/1000\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.3781 - val_loss: 0.3617 - lr: 5.0000e-04\n",
            "Epoch 262/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.3734 - val_loss: 0.4006 - lr: 5.0000e-04\n",
            "Epoch 263/1000\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.3781 - val_loss: 0.4185 - lr: 5.0000e-04\n",
            "Epoch 264/1000\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.3790 - val_loss: 0.3704 - lr: 5.0000e-04\n",
            "Epoch 265/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.3652 - val_loss: 0.4539 - lr: 5.0000e-04\n",
            "Epoch 266/1000\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.3733 - val_loss: 0.3026 - lr: 5.0000e-04\n",
            "Epoch 267/1000\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.3675 - val_loss: 0.3428 - lr: 5.0000e-04\n",
            "Epoch 268/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.3615 - val_loss: 0.4023 - lr: 5.0000e-04\n",
            "Epoch 269/1000\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.3683 - val_loss: 0.3868 - lr: 5.0000e-04\n",
            "Epoch 270/1000\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.3646 - val_loss: 0.4869 - lr: 5.0000e-04\n",
            "Epoch 271/1000\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.3686 - val_loss: 0.3781 - lr: 5.0000e-04\n",
            "Epoch 272/1000\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.3615 - val_loss: 0.3584 - lr: 5.0000e-04\n",
            "Epoch 273/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.3614 - val_loss: 0.3169 - lr: 5.0000e-04\n",
            "Epoch 274/1000\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.3634 - val_loss: 0.3804 - lr: 5.0000e-04\n",
            "Epoch 275/1000\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.3631 - val_loss: 0.4256 - lr: 5.0000e-04\n",
            "Epoch 276/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.3544 - val_loss: 0.2857 - lr: 5.0000e-04\n",
            "Epoch 277/1000\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.3585 - val_loss: 0.3013 - lr: 5.0000e-04\n",
            "Epoch 278/1000\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.3610 - val_loss: 0.4525 - lr: 5.0000e-04\n",
            "Epoch 279/1000\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.3545 - val_loss: 0.3339 - lr: 5.0000e-04\n",
            "Epoch 280/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.3543 - val_loss: 0.3865 - lr: 5.0000e-04\n",
            "Epoch 281/1000\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.3555 - val_loss: 0.3001 - lr: 5.0000e-04\n",
            "Epoch 282/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.3541 - val_loss: 0.3308 - lr: 5.0000e-04\n",
            "Epoch 283/1000\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.3614 - val_loss: 0.3923 - lr: 5.0000e-04\n",
            "Epoch 284/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.3471 - val_loss: 0.3341 - lr: 5.0000e-04\n",
            "Epoch 285/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.3422 - val_loss: 0.3415 - lr: 5.0000e-04\n",
            "Epoch 286/1000\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.3505 - val_loss: 0.4467 - lr: 5.0000e-04\n",
            "Epoch 287/1000\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.3613 - val_loss: 0.3980 - lr: 5.0000e-04\n",
            "Epoch 288/1000\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.3842 - val_loss: 0.4018 - lr: 5.0000e-04\n",
            "Epoch 289/1000\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.3759 - val_loss: 0.3580 - lr: 5.0000e-04\n",
            "Epoch 290/1000\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.3704 - val_loss: 0.3492 - lr: 5.0000e-04\n",
            "Epoch 291/1000\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.3658 - val_loss: 0.3253 - lr: 5.0000e-04\n",
            "Epoch 292/1000\n",
            "173/173 [==============================] - 6s 33ms/step - loss: 0.3855 - val_loss: 0.3363 - lr: 5.0000e-04\n",
            "Epoch 293/1000\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.3811 - val_loss: 0.3799 - lr: 5.0000e-04\n",
            "Epoch 294/1000\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.3770 - val_loss: 0.4530 - lr: 5.0000e-04\n",
            "Epoch 295/1000\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.3694 - val_loss: 0.3828 - lr: 5.0000e-04\n",
            "Epoch 296/1000\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.3692 - val_loss: 0.3803 - lr: 5.0000e-04\n",
            "Epoch 297/1000\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.3619 - val_loss: 0.3448 - lr: 5.0000e-04\n",
            "Epoch 298/1000\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.3650 - val_loss: 0.4312 - lr: 5.0000e-04\n",
            "Epoch 299/1000\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.3620 - val_loss: 0.3389 - lr: 5.0000e-04\n",
            "Epoch 300/1000\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.3595 - val_loss: 0.3999 - lr: 5.0000e-04\n",
            "Epoch 301/1000\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.3635 - val_loss: 0.3939 - lr: 5.0000e-04\n",
            "Epoch 302/1000\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.3610 - val_loss: 0.4097 - lr: 5.0000e-04\n",
            "Epoch 303/1000\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.3554 - val_loss: 0.4393 - lr: 5.0000e-04\n",
            "Epoch 304/1000\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.3516 - val_loss: 0.4288 - lr: 5.0000e-04\n",
            "Epoch 305/1000\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.3502 - val_loss: 0.3881 - lr: 5.0000e-04\n",
            "Epoch 306/1000\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.3557 - val_loss: 0.4262 - lr: 5.0000e-04\n",
            "Epoch 307/1000\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.3464 - val_loss: 0.4272 - lr: 5.0000e-04\n",
            "Epoch 308/1000\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.3460 - val_loss: 0.3392 - lr: 5.0000e-04\n",
            "Epoch 309/1000\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.3491 - val_loss: 0.4272 - lr: 5.0000e-04\n",
            "Epoch 310/1000\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.3467 - val_loss: 0.3959 - lr: 5.0000e-04\n",
            "Epoch 311/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.3376 - val_loss: 0.3403 - lr: 5.0000e-04\n",
            "Epoch 312/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.3464 - val_loss: 0.3362 - lr: 5.0000e-04\n",
            "Epoch 313/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.3435 - val_loss: 0.2955 - lr: 5.0000e-04\n",
            "Epoch 314/1000\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.3407 - val_loss: 0.3482 - lr: 5.0000e-04\n",
            "Epoch 315/1000\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.3390 - val_loss: 0.3257 - lr: 5.0000e-04\n",
            "Epoch 316/1000\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.3443 - val_loss: 0.2783 - lr: 5.0000e-04\n",
            "Epoch 317/1000\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.3391 - val_loss: 0.3834 - lr: 5.0000e-04\n",
            "Epoch 318/1000\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.3384 - val_loss: 0.3813 - lr: 5.0000e-04\n",
            "Epoch 319/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.3352 - val_loss: 0.3705 - lr: 5.0000e-04\n",
            "Epoch 320/1000\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.3377 - val_loss: 0.4201 - lr: 5.0000e-04\n",
            "Epoch 321/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.3351 - val_loss: 0.2785 - lr: 5.0000e-04\n",
            "Epoch 322/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.3338 - val_loss: 0.2966 - lr: 5.0000e-04\n",
            "Epoch 323/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.3320 - val_loss: 0.3303 - lr: 5.0000e-04\n",
            "Epoch 324/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.3286 - val_loss: 0.3131 - lr: 5.0000e-04\n",
            "Epoch 325/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.3278 - val_loss: 0.3767 - lr: 5.0000e-04\n",
            "Epoch 326/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.3279 - val_loss: 0.3289 - lr: 5.0000e-04\n",
            "Epoch 327/1000\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.3310 - val_loss: 0.3836 - lr: 5.0000e-04\n",
            "Epoch 328/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.3288 - val_loss: 0.4499 - lr: 5.0000e-04\n",
            "Epoch 329/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.3265 - val_loss: 0.3046 - lr: 5.0000e-04\n",
            "Epoch 330/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.3263 - val_loss: 0.3546 - lr: 5.0000e-04\n",
            "Epoch 331/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.3261 - val_loss: 0.2883 - lr: 5.0000e-04\n",
            "Epoch 332/1000\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.3251 - val_loss: 0.3363 - lr: 5.0000e-04\n",
            "Epoch 333/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.3267 - val_loss: 0.3386 - lr: 5.0000e-04\n",
            "Epoch 334/1000\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.3254 - val_loss: 0.3089 - lr: 5.0000e-04\n",
            "Epoch 335/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.3250 - val_loss: 0.3003 - lr: 5.0000e-04\n",
            "Epoch 336/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.3207 - val_loss: 0.3687 - lr: 5.0000e-04\n",
            "Epoch 337/1000\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.3239 - val_loss: 0.2790 - lr: 5.0000e-04\n",
            "Epoch 338/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.3254 - val_loss: 0.3112 - lr: 5.0000e-04\n",
            "Epoch 339/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.3251 - val_loss: 0.2805 - lr: 5.0000e-04\n",
            "Epoch 340/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.3209 - val_loss: 0.3393 - lr: 5.0000e-04\n",
            "Epoch 341/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.3170 - val_loss: 0.3460 - lr: 5.0000e-04\n",
            "Epoch 342/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.3190 - val_loss: 0.3014 - lr: 5.0000e-04\n",
            "Epoch 343/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.3158 - val_loss: 0.4027 - lr: 5.0000e-04\n",
            "Epoch 344/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.3201 - val_loss: 0.4296 - lr: 5.0000e-04\n",
            "Epoch 345/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.3154 - val_loss: 0.3626 - lr: 5.0000e-04\n",
            "Epoch 346/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.3161 - val_loss: 0.3390 - lr: 5.0000e-04\n",
            "Epoch 347/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.3119 - val_loss: 0.3556 - lr: 5.0000e-04\n",
            "Epoch 348/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.3146 - val_loss: 0.2957 - lr: 5.0000e-04\n",
            "Epoch 349/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.3112 - val_loss: 0.3398 - lr: 5.0000e-04\n",
            "Epoch 350/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.3174 - val_loss: 0.2720 - lr: 5.0000e-04\n",
            "Epoch 351/1000\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.3138 - val_loss: 0.3033 - lr: 5.0000e-04\n",
            "Epoch 352/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.3101 - val_loss: 0.3653 - lr: 5.0000e-04\n",
            "Epoch 353/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.3106 - val_loss: 0.2843 - lr: 5.0000e-04\n",
            "Epoch 354/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.3125 - val_loss: 0.2801 - lr: 5.0000e-04\n",
            "Epoch 355/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.3106 - val_loss: 0.3100 - lr: 5.0000e-04\n",
            "Epoch 356/1000\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.3104 - val_loss: 0.2903 - lr: 5.0000e-04\n",
            "Epoch 357/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.3063 - val_loss: 0.2962 - lr: 5.0000e-04\n",
            "Epoch 358/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.3109 - val_loss: 0.2696 - lr: 5.0000e-04\n",
            "Epoch 359/1000\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.3083 - val_loss: 0.2914 - lr: 5.0000e-04\n",
            "Epoch 360/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.3069 - val_loss: 0.3206 - lr: 5.0000e-04\n",
            "Epoch 361/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.3059 - val_loss: 0.2839 - lr: 5.0000e-04\n",
            "Epoch 362/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.3046 - val_loss: 0.3371 - lr: 5.0000e-04\n",
            "Epoch 363/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.3076 - val_loss: 0.3390 - lr: 5.0000e-04\n",
            "Epoch 364/1000\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.3098 - val_loss: 0.2952 - lr: 5.0000e-04\n",
            "Epoch 365/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.3013 - val_loss: 0.3019 - lr: 5.0000e-04\n",
            "Epoch 366/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.3011 - val_loss: 0.3490 - lr: 5.0000e-04\n",
            "Epoch 367/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.3021 - val_loss: 0.3050 - lr: 5.0000e-04\n",
            "Epoch 368/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.3012 - val_loss: 0.2699 - lr: 5.0000e-04\n",
            "Epoch 369/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.3076 - val_loss: 0.2779 - lr: 5.0000e-04\n",
            "Epoch 370/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.3027 - val_loss: 0.2531 - lr: 5.0000e-04\n",
            "Epoch 371/1000\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.3014 - val_loss: 0.3105 - lr: 5.0000e-04\n",
            "Epoch 372/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.3043 - val_loss: 0.2586 - lr: 5.0000e-04\n",
            "Epoch 373/1000\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.3032 - val_loss: 0.3128 - lr: 5.0000e-04\n",
            "Epoch 374/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.3006 - val_loss: 0.2848 - lr: 5.0000e-04\n",
            "Epoch 375/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.2933 - val_loss: 0.3129 - lr: 5.0000e-04\n",
            "Epoch 376/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.2989 - val_loss: 0.2810 - lr: 5.0000e-04\n",
            "Epoch 377/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.2988 - val_loss: 0.2636 - lr: 5.0000e-04\n",
            "Epoch 378/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.2962 - val_loss: 0.3354 - lr: 5.0000e-04\n",
            "Epoch 379/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.2988 - val_loss: 0.2678 - lr: 5.0000e-04\n",
            "Epoch 380/1000\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.3004 - val_loss: 0.2705 - lr: 5.0000e-04\n",
            "Epoch 381/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.2918 - val_loss: 0.3027 - lr: 5.0000e-04\n",
            "Epoch 382/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.2967 - val_loss: 0.2981 - lr: 5.0000e-04\n",
            "Epoch 383/1000\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.2940 - val_loss: 0.3037 - lr: 5.0000e-04\n",
            "Epoch 384/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.2927 - val_loss: 0.2769 - lr: 5.0000e-04\n",
            "Epoch 385/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.2940 - val_loss: 0.2786 - lr: 5.0000e-04\n",
            "Epoch 386/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.2942 - val_loss: 0.3146 - lr: 5.0000e-04\n",
            "Epoch 387/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.2919 - val_loss: 0.2954 - lr: 5.0000e-04\n",
            "Epoch 388/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.2986 - val_loss: 0.3299 - lr: 5.0000e-04\n",
            "Epoch 389/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.2924 - val_loss: 0.3209 - lr: 5.0000e-04\n",
            "Epoch 390/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.2934 - val_loss: 0.3135 - lr: 5.0000e-04\n",
            "Epoch 391/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.2906 - val_loss: 0.2771 - lr: 5.0000e-04\n",
            "Epoch 392/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.2938 - val_loss: 0.2631 - lr: 5.0000e-04\n",
            "Epoch 393/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.2893 - val_loss: 0.2789 - lr: 5.0000e-04\n",
            "Epoch 394/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.2902 - val_loss: 0.3107 - lr: 5.0000e-04\n",
            "Epoch 395/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.2871 - val_loss: 0.2668 - lr: 5.0000e-04\n",
            "Epoch 396/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.2932 - val_loss: 0.2702 - lr: 5.0000e-04\n",
            "Epoch 397/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.2839 - val_loss: 0.2840 - lr: 5.0000e-04\n",
            "Epoch 398/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.2851 - val_loss: 0.3623 - lr: 5.0000e-04\n",
            "Epoch 399/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.2892 - val_loss: 0.2957 - lr: 5.0000e-04\n",
            "Epoch 400/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.2892 - val_loss: 0.2597 - lr: 5.0000e-04\n",
            "Epoch 401/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.2865 - val_loss: 0.2712 - lr: 5.0000e-04\n",
            "Epoch 402/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.2898 - val_loss: 0.3134 - lr: 5.0000e-04\n",
            "Epoch 403/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.2860 - val_loss: 0.2653 - lr: 5.0000e-04\n",
            "Epoch 404/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.2889 - val_loss: 0.3723 - lr: 5.0000e-04\n",
            "Epoch 405/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.2841 - val_loss: 0.2514 - lr: 5.0000e-04\n",
            "Epoch 406/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.2901 - val_loss: 0.2927 - lr: 5.0000e-04\n",
            "Epoch 407/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.2837 - val_loss: 0.2984 - lr: 5.0000e-04\n",
            "Epoch 408/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.2851 - val_loss: 0.2664 - lr: 5.0000e-04\n",
            "Epoch 409/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.2800 - val_loss: 0.2642 - lr: 5.0000e-04\n",
            "Epoch 410/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.2879 - val_loss: 0.2915 - lr: 5.0000e-04\n",
            "Epoch 411/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.2841 - val_loss: 0.2576 - lr: 5.0000e-04\n",
            "Epoch 412/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.2817 - val_loss: 0.2708 - lr: 5.0000e-04\n",
            "Epoch 413/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.2791 - val_loss: 0.3169 - lr: 5.0000e-04\n",
            "Epoch 414/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.2816 - val_loss: 0.2829 - lr: 5.0000e-04\n",
            "Epoch 415/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.2788 - val_loss: 0.2964 - lr: 5.0000e-04\n",
            "Epoch 416/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.2835 - val_loss: 0.2502 - lr: 5.0000e-04\n",
            "Epoch 417/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.2848 - val_loss: 0.2972 - lr: 5.0000e-04\n",
            "Epoch 418/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.2775 - val_loss: 0.3192 - lr: 5.0000e-04\n",
            "Epoch 419/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.2813 - val_loss: 0.2903 - lr: 5.0000e-04\n",
            "Epoch 420/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.2799 - val_loss: 0.2818 - lr: 5.0000e-04\n",
            "Epoch 421/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.2773 - val_loss: 0.2657 - lr: 5.0000e-04\n",
            "Epoch 422/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.2816 - val_loss: 0.2661 - lr: 5.0000e-04\n",
            "Epoch 423/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.2839 - val_loss: 0.2771 - lr: 5.0000e-04\n",
            "Epoch 424/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.2790 - val_loss: 0.2651 - lr: 5.0000e-04\n",
            "Epoch 425/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.2762 - val_loss: 0.2427 - lr: 5.0000e-04\n",
            "Epoch 426/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.2767 - val_loss: 0.2841 - lr: 5.0000e-04\n",
            "Epoch 427/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.2779 - val_loss: 0.2465 - lr: 5.0000e-04\n",
            "Epoch 428/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.2711 - val_loss: 0.2361 - lr: 5.0000e-04\n",
            "Epoch 429/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.2751 - val_loss: 0.2858 - lr: 5.0000e-04\n",
            "Epoch 430/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.2820 - val_loss: 0.2553 - lr: 5.0000e-04\n",
            "Epoch 431/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.2771 - val_loss: 0.2837 - lr: 5.0000e-04\n",
            "Epoch 432/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.2736 - val_loss: 0.2499 - lr: 5.0000e-04\n",
            "Epoch 433/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.2741 - val_loss: 0.2481 - lr: 5.0000e-04\n",
            "Epoch 434/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.2704 - val_loss: 0.2780 - lr: 5.0000e-04\n",
            "Epoch 435/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.2752 - val_loss: 0.2410 - lr: 5.0000e-04\n",
            "Epoch 436/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.2749 - val_loss: 0.2529 - lr: 5.0000e-04\n",
            "Epoch 437/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.2761 - val_loss: 0.2382 - lr: 5.0000e-04\n",
            "Epoch 438/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.2675 - val_loss: 0.2528 - lr: 5.0000e-04\n",
            "Epoch 439/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.2714 - val_loss: 0.2510 - lr: 5.0000e-04\n",
            "Epoch 440/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.2727 - val_loss: 0.2620 - lr: 5.0000e-04\n",
            "Epoch 441/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.2728 - val_loss: 0.2372 - lr: 5.0000e-04\n",
            "Epoch 442/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.2679 - val_loss: 0.3068 - lr: 5.0000e-04\n",
            "Epoch 443/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.2768 - val_loss: 0.2486 - lr: 5.0000e-04\n",
            "Epoch 444/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.2736 - val_loss: 0.2343 - lr: 5.0000e-04\n",
            "Epoch 445/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.2661 - val_loss: 0.2619 - lr: 5.0000e-04\n",
            "Epoch 446/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.2730 - val_loss: 0.3004 - lr: 5.0000e-04\n",
            "Epoch 447/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.2706 - val_loss: 0.2399 - lr: 5.0000e-04\n",
            "Epoch 448/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.2661 - val_loss: 0.3062 - lr: 5.0000e-04\n",
            "Epoch 449/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.2734 - val_loss: 0.2817 - lr: 5.0000e-04\n",
            "Epoch 450/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.2702 - val_loss: 0.2433 - lr: 5.0000e-04\n",
            "Epoch 451/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.2678 - val_loss: 0.2321 - lr: 5.0000e-04\n",
            "Epoch 452/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.2679 - val_loss: 0.2697 - lr: 5.0000e-04\n",
            "Epoch 453/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.2685 - val_loss: 0.2664 - lr: 5.0000e-04\n",
            "Epoch 454/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.2640 - val_loss: 0.2446 - lr: 5.0000e-04\n",
            "Epoch 455/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.2684 - val_loss: 0.2465 - lr: 5.0000e-04\n",
            "Epoch 456/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.2688 - val_loss: 0.2515 - lr: 5.0000e-04\n",
            "Epoch 457/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.2657 - val_loss: 0.3042 - lr: 5.0000e-04\n",
            "Epoch 458/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.2677 - val_loss: 0.2614 - lr: 5.0000e-04\n",
            "Epoch 459/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.2676 - val_loss: 0.2717 - lr: 5.0000e-04\n",
            "Epoch 460/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.2612 - val_loss: 0.2688 - lr: 5.0000e-04\n",
            "Epoch 461/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.2690 - val_loss: 0.2361 - lr: 5.0000e-04\n",
            "Epoch 462/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.2615 - val_loss: 0.2478 - lr: 5.0000e-04\n",
            "Epoch 463/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.2606 - val_loss: 0.2392 - lr: 5.0000e-04\n",
            "Epoch 464/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.2660 - val_loss: 0.2267 - lr: 5.0000e-04\n",
            "Epoch 465/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.2611 - val_loss: 0.3022 - lr: 5.0000e-04\n",
            "Epoch 466/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.2628 - val_loss: 0.2737 - lr: 5.0000e-04\n",
            "Epoch 467/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.2626 - val_loss: 0.2815 - lr: 5.0000e-04\n",
            "Epoch 468/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.2595 - val_loss: 0.3566 - lr: 5.0000e-04\n",
            "Epoch 469/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.2627 - val_loss: 0.3053 - lr: 5.0000e-04\n",
            "Epoch 470/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.2628 - val_loss: 0.2560 - lr: 5.0000e-04\n",
            "Epoch 471/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.2600 - val_loss: 0.2562 - lr: 5.0000e-04\n",
            "Epoch 472/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.2649 - val_loss: 0.2221 - lr: 5.0000e-04\n",
            "Epoch 473/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.2575 - val_loss: 0.2518 - lr: 5.0000e-04\n",
            "Epoch 474/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.2604 - val_loss: 0.2473 - lr: 5.0000e-04\n",
            "Epoch 475/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.2592 - val_loss: 0.2424 - lr: 5.0000e-04\n",
            "Epoch 476/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.2599 - val_loss: 0.2529 - lr: 5.0000e-04\n",
            "Epoch 477/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.2596 - val_loss: 0.2414 - lr: 5.0000e-04\n",
            "Epoch 478/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.2586 - val_loss: 0.2188 - lr: 5.0000e-04\n",
            "Epoch 479/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.2570 - val_loss: 0.2237 - lr: 5.0000e-04\n",
            "Epoch 480/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.2665 - val_loss: 0.2854 - lr: 5.0000e-04\n",
            "Epoch 481/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.2578 - val_loss: 0.2253 - lr: 5.0000e-04\n",
            "Epoch 482/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.2564 - val_loss: 0.2371 - lr: 5.0000e-04\n",
            "Epoch 483/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.2567 - val_loss: 0.2638 - lr: 5.0000e-04\n",
            "Epoch 484/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.2613 - val_loss: 0.2590 - lr: 5.0000e-04\n",
            "Epoch 485/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.2558 - val_loss: 0.2622 - lr: 5.0000e-04\n",
            "Epoch 486/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.2586 - val_loss: 0.2298 - lr: 5.0000e-04\n",
            "Epoch 487/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.2587 - val_loss: 0.2382 - lr: 5.0000e-04\n",
            "Epoch 488/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.2565 - val_loss: 0.2476 - lr: 5.0000e-04\n",
            "Epoch 489/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.2599 - val_loss: 0.2272 - lr: 5.0000e-04\n",
            "Epoch 490/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.2590 - val_loss: 0.2211 - lr: 5.0000e-04\n",
            "Epoch 491/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.2552 - val_loss: 0.2432 - lr: 5.0000e-04\n",
            "Epoch 492/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.2564 - val_loss: 0.2275 - lr: 5.0000e-04\n",
            "Epoch 493/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.2544 - val_loss: 0.2322 - lr: 5.0000e-04\n",
            "Epoch 494/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.2556 - val_loss: 0.2710 - lr: 5.0000e-04\n",
            "Epoch 495/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.2492 - val_loss: 0.2429 - lr: 5.0000e-04\n",
            "Epoch 496/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.2549 - val_loss: 0.2280 - lr: 5.0000e-04\n",
            "Epoch 497/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.2528 - val_loss: 0.2603 - lr: 5.0000e-04\n",
            "Epoch 498/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.2563 - val_loss: 0.2762 - lr: 5.0000e-04\n",
            "Epoch 499/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.2545 - val_loss: 0.2502 - lr: 5.0000e-04\n",
            "Epoch 500/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.2518 - val_loss: 0.2530 - lr: 5.0000e-04\n",
            "Epoch 501/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.2538 - val_loss: 0.2399 - lr: 5.0000e-04\n",
            "Epoch 502/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.2502 - val_loss: 0.2455 - lr: 5.0000e-04\n",
            "Epoch 503/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.2503 - val_loss: 0.2331 - lr: 5.0000e-04\n",
            "Epoch 504/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.2510 - val_loss: 0.2504 - lr: 5.0000e-04\n",
            "Epoch 505/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.2550 - val_loss: 0.2545 - lr: 5.0000e-04\n",
            "Epoch 506/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.2487 - val_loss: 0.2360 - lr: 5.0000e-04\n",
            "Epoch 507/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.2476 - val_loss: 0.2615 - lr: 5.0000e-04\n",
            "Epoch 508/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.2494 - val_loss: 0.2655 - lr: 5.0000e-04\n",
            "Epoch 509/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.2495 - val_loss: 0.2385 - lr: 5.0000e-04\n",
            "Epoch 510/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.2463 - val_loss: 0.2772 - lr: 5.0000e-04\n",
            "Epoch 511/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.2486 - val_loss: 0.2221 - lr: 5.0000e-04\n",
            "Epoch 512/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.2516 - val_loss: 0.2247 - lr: 5.0000e-04\n",
            "Epoch 513/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.2473 - val_loss: 0.2360 - lr: 5.0000e-04\n",
            "Epoch 514/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.2502 - val_loss: 0.2820 - lr: 5.0000e-04\n",
            "Epoch 515/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.2483 - val_loss: 0.2578 - lr: 5.0000e-04\n",
            "Epoch 516/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.2460 - val_loss: 0.2550 - lr: 5.0000e-04\n",
            "Epoch 517/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.2471 - val_loss: 0.2659 - lr: 5.0000e-04\n",
            "Epoch 518/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.2461 - val_loss: 0.2272 - lr: 5.0000e-04\n",
            "Epoch 519/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.2496 - val_loss: 0.2415 - lr: 5.0000e-04\n",
            "Epoch 520/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.2476 - val_loss: 0.2409 - lr: 5.0000e-04\n",
            "Epoch 521/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.2442 - val_loss: 0.2278 - lr: 5.0000e-04\n",
            "Epoch 522/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.2457 - val_loss: 0.2532 - lr: 5.0000e-04\n",
            "Epoch 523/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.2477 - val_loss: 0.2187 - lr: 5.0000e-04\n",
            "Epoch 524/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.2474 - val_loss: 0.2300 - lr: 5.0000e-04\n",
            "Epoch 525/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.2445 - val_loss: 0.2263 - lr: 5.0000e-04\n",
            "Epoch 526/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.2509 - val_loss: 0.2263 - lr: 5.0000e-04\n",
            "Epoch 527/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.2449 - val_loss: 0.3158 - lr: 5.0000e-04\n",
            "Epoch 528/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.2495 - val_loss: 0.2098 - lr: 5.0000e-04\n",
            "Epoch 529/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.2463 - val_loss: 0.2286 - lr: 5.0000e-04\n",
            "Epoch 530/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.2466 - val_loss: 0.2197 - lr: 5.0000e-04\n",
            "Epoch 531/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.2441 - val_loss: 0.2255 - lr: 5.0000e-04\n",
            "Epoch 532/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.2448 - val_loss: 0.2599 - lr: 5.0000e-04\n",
            "Epoch 533/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.2442 - val_loss: 0.2314 - lr: 5.0000e-04\n",
            "Epoch 534/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.2423 - val_loss: 0.2870 - lr: 5.0000e-04\n",
            "Epoch 535/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.2485 - val_loss: 0.2498 - lr: 5.0000e-04\n",
            "Epoch 536/1000\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.2431 - val_loss: 0.3047 - lr: 5.0000e-04\n",
            "Epoch 537/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.2437 - val_loss: 0.2366 - lr: 5.0000e-04\n",
            "Epoch 538/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.2414 - val_loss: 0.2901 - lr: 5.0000e-04\n",
            "Epoch 539/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.2401 - val_loss: 0.2080 - lr: 5.0000e-04\n",
            "Epoch 540/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.2410 - val_loss: 0.2472 - lr: 5.0000e-04\n",
            "Epoch 541/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.2456 - val_loss: 0.2760 - lr: 5.0000e-04\n",
            "Epoch 542/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.2437 - val_loss: 0.2309 - lr: 5.0000e-04\n",
            "Epoch 543/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.2403 - val_loss: 0.2657 - lr: 5.0000e-04\n",
            "Epoch 544/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.2458 - val_loss: 0.2329 - lr: 5.0000e-04\n",
            "Epoch 545/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.2466 - val_loss: 0.2608 - lr: 5.0000e-04\n",
            "Epoch 546/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.2449 - val_loss: 0.2523 - lr: 5.0000e-04\n",
            "Epoch 547/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.2393 - val_loss: 0.2398 - lr: 5.0000e-04\n",
            "Epoch 548/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.2385 - val_loss: 0.2425 - lr: 5.0000e-04\n",
            "Epoch 549/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.2361 - val_loss: 0.1991 - lr: 5.0000e-04\n",
            "Epoch 550/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.2421 - val_loss: 0.2391 - lr: 5.0000e-04\n",
            "Epoch 551/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.2388 - val_loss: 0.2909 - lr: 5.0000e-04\n",
            "Epoch 552/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.2452 - val_loss: 0.2252 - lr: 5.0000e-04\n",
            "Epoch 553/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.2433 - val_loss: 0.2269 - lr: 5.0000e-04\n",
            "Epoch 554/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.2394 - val_loss: 0.2127 - lr: 5.0000e-04\n",
            "Epoch 555/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.2370 - val_loss: 0.2178 - lr: 5.0000e-04\n",
            "Epoch 556/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.2382 - val_loss: 0.2299 - lr: 5.0000e-04\n",
            "Epoch 557/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.2417 - val_loss: 0.2347 - lr: 5.0000e-04\n",
            "Epoch 558/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.2391 - val_loss: 0.2158 - lr: 5.0000e-04\n",
            "Epoch 559/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.2363 - val_loss: 0.2205 - lr: 5.0000e-04\n",
            "Epoch 560/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.2361 - val_loss: 0.2310 - lr: 5.0000e-04\n",
            "Epoch 561/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.2368 - val_loss: 0.2284 - lr: 5.0000e-04\n",
            "Epoch 562/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.2436 - val_loss: 0.2387 - lr: 5.0000e-04\n",
            "Epoch 563/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.2351 - val_loss: 0.2383 - lr: 5.0000e-04\n",
            "Epoch 564/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.2415 - val_loss: 0.2479 - lr: 5.0000e-04\n",
            "Epoch 565/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.2350 - val_loss: 0.2470 - lr: 5.0000e-04\n",
            "Epoch 566/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.2335 - val_loss: 0.2194 - lr: 5.0000e-04\n",
            "Epoch 567/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.2373 - val_loss: 0.2276 - lr: 5.0000e-04\n",
            "Epoch 568/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.2358 - val_loss: 0.2805 - lr: 5.0000e-04\n",
            "Epoch 569/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.2337 - val_loss: 0.2061 - lr: 5.0000e-04\n",
            "Epoch 570/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.2352 - val_loss: 0.2375 - lr: 5.0000e-04\n",
            "Epoch 571/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.2377 - val_loss: 0.2205 - lr: 5.0000e-04\n",
            "Epoch 572/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.2376 - val_loss: 0.2614 - lr: 5.0000e-04\n",
            "Epoch 573/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.2336 - val_loss: 0.2401 - lr: 5.0000e-04\n",
            "Epoch 574/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.2397 - val_loss: 0.2267 - lr: 5.0000e-04\n",
            "Epoch 575/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.2336 - val_loss: 0.2114 - lr: 5.0000e-04\n",
            "Epoch 576/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.2364 - val_loss: 0.2328 - lr: 5.0000e-04\n",
            "Epoch 577/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.2342 - val_loss: 0.2501 - lr: 5.0000e-04\n",
            "Epoch 578/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.2355 - val_loss: 0.2304 - lr: 5.0000e-04\n",
            "Epoch 579/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.2359 - val_loss: 0.2324 - lr: 5.0000e-04\n",
            "Epoch 580/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.2323 - val_loss: 0.2456 - lr: 5.0000e-04\n",
            "Epoch 581/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.2339 - val_loss: 0.2490 - lr: 5.0000e-04\n",
            "Epoch 582/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.2331 - val_loss: 0.2424 - lr: 5.0000e-04\n",
            "Epoch 583/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.2344 - val_loss: 0.2501 - lr: 5.0000e-04\n",
            "Epoch 584/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.2307 - val_loss: 0.2261 - lr: 5.0000e-04\n",
            "Epoch 585/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.2353 - val_loss: 0.2061 - lr: 5.0000e-04\n",
            "Epoch 586/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.2317 - val_loss: 0.2476 - lr: 5.0000e-04\n",
            "Epoch 587/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.2322 - val_loss: 0.2091 - lr: 5.0000e-04\n",
            "Epoch 588/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.2324 - val_loss: 0.2281 - lr: 5.0000e-04\n",
            "Epoch 589/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.2319 - val_loss: 0.2563 - lr: 5.0000e-04\n",
            "Epoch 590/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.2337 - val_loss: 0.2159 - lr: 5.0000e-04\n",
            "Epoch 591/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.2321 - val_loss: 0.2159 - lr: 5.0000e-04\n",
            "Epoch 592/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.2338 - val_loss: 0.1941 - lr: 5.0000e-04\n",
            "Epoch 593/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.2305 - val_loss: 0.2274 - lr: 5.0000e-04\n",
            "Epoch 594/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.2341 - val_loss: 0.2453 - lr: 5.0000e-04\n",
            "Epoch 595/1000\n",
            "173/173 [==============================] - 6s 38ms/step - loss: 0.2298 - val_loss: 0.2072 - lr: 5.0000e-04\n",
            "Epoch 596/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.2323 - val_loss: 0.1999 - lr: 5.0000e-04\n",
            "Epoch 597/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.2346 - val_loss: 0.2314 - lr: 5.0000e-04\n",
            "Epoch 598/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.2286 - val_loss: 0.2146 - lr: 5.0000e-04\n",
            "Epoch 599/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.2304 - val_loss: 0.2806 - lr: 5.0000e-04\n",
            "Epoch 600/1000\n",
            "173/173 [==============================] - 6s 38ms/step - loss: 0.2274 - val_loss: 0.2705 - lr: 5.0000e-04\n",
            "Epoch 601/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.2305 - val_loss: 0.2134 - lr: 5.0000e-04\n",
            "Epoch 602/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.2308 - val_loss: 0.2051 - lr: 5.0000e-04\n",
            "Epoch 603/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.2297 - val_loss: 0.2369 - lr: 5.0000e-04\n",
            "Epoch 604/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.2287 - val_loss: 0.2387 - lr: 5.0000e-04\n",
            "Epoch 605/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.2297 - val_loss: 0.1915 - lr: 5.0000e-04\n",
            "Epoch 606/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.2293 - val_loss: 0.2851 - lr: 5.0000e-04\n",
            "Epoch 607/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.2255 - val_loss: 0.2299 - lr: 5.0000e-04\n",
            "Epoch 608/1000\n",
            "173/173 [==============================] - 6s 38ms/step - loss: 0.2250 - val_loss: 0.2197 - lr: 5.0000e-04\n",
            "Epoch 609/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.2254 - val_loss: 0.2143 - lr: 5.0000e-04\n",
            "Epoch 610/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.2297 - val_loss: 0.2075 - lr: 5.0000e-04\n",
            "Epoch 611/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.2293 - val_loss: 0.2037 - lr: 5.0000e-04\n",
            "Epoch 612/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.2276 - val_loss: 0.2076 - lr: 5.0000e-04\n",
            "Epoch 613/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.2301 - val_loss: 0.2120 - lr: 5.0000e-04\n",
            "Epoch 614/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.2257 - val_loss: 0.2316 - lr: 5.0000e-04\n",
            "Epoch 615/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.2276 - val_loss: 0.3079 - lr: 5.0000e-04\n",
            "Epoch 616/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.2257 - val_loss: 0.2694 - lr: 5.0000e-04\n",
            "Epoch 617/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.2268 - val_loss: 0.2080 - lr: 5.0000e-04\n",
            "Epoch 618/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.2261 - val_loss: 0.1961 - lr: 5.0000e-04\n",
            "Epoch 619/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.2226 - val_loss: 0.2018 - lr: 5.0000e-04\n",
            "Epoch 620/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.2290 - val_loss: 0.2645 - lr: 5.0000e-04\n",
            "Epoch 621/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.2258 - val_loss: 0.2082 - lr: 5.0000e-04\n",
            "Epoch 622/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.2242 - val_loss: 0.2504 - lr: 5.0000e-04\n",
            "Epoch 623/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.2215 - val_loss: 0.2293 - lr: 5.0000e-04\n",
            "Epoch 624/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.2251 - val_loss: 0.2072 - lr: 5.0000e-04\n",
            "Epoch 625/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.2295 - val_loss: 0.2402 - lr: 5.0000e-04\n",
            "Epoch 626/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.2271 - val_loss: 0.2310 - lr: 5.0000e-04\n",
            "Epoch 627/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.2298 - val_loss: 0.2040 - lr: 5.0000e-04\n",
            "Epoch 628/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.2245 - val_loss: 0.2036 - lr: 5.0000e-04\n",
            "Epoch 629/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.2257 - val_loss: 0.2175 - lr: 5.0000e-04\n",
            "Epoch 630/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.2238 - val_loss: 0.2156 - lr: 5.0000e-04\n",
            "Epoch 631/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.2275 - val_loss: 0.2287 - lr: 5.0000e-04\n",
            "Epoch 632/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.2220 - val_loss: 0.2151 - lr: 5.0000e-04\n",
            "Epoch 633/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.2276 - val_loss: 0.2435 - lr: 5.0000e-04\n",
            "Epoch 634/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.2240 - val_loss: 0.2154 - lr: 5.0000e-04\n",
            "Epoch 635/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.2220 - val_loss: 0.2098 - lr: 5.0000e-04\n",
            "Epoch 636/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.2273 - val_loss: 0.2104 - lr: 5.0000e-04\n",
            "Epoch 637/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.2235 - val_loss: 0.2157 - lr: 5.0000e-04\n",
            "Epoch 638/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.2214 - val_loss: 0.2294 - lr: 5.0000e-04\n",
            "Epoch 639/1000\n",
            "173/173 [==============================] - 6s 38ms/step - loss: 0.2259 - val_loss: 0.2391 - lr: 5.0000e-04\n",
            "Epoch 640/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.2238 - val_loss: 0.2051 - lr: 5.0000e-04\n",
            "Epoch 641/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.2240 - val_loss: 0.1933 - lr: 5.0000e-04\n",
            "Epoch 642/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.2245 - val_loss: 0.2128 - lr: 5.0000e-04\n",
            "Epoch 643/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.2245 - val_loss: 0.2137 - lr: 5.0000e-04\n",
            "Epoch 644/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.2220 - val_loss: 0.2184 - lr: 5.0000e-04\n",
            "Epoch 645/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.2215 - val_loss: 0.2036 - lr: 5.0000e-04\n",
            "Epoch 646/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.2240 - val_loss: 0.2126 - lr: 5.0000e-04\n",
            "Epoch 647/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.2232 - val_loss: 0.2272 - lr: 5.0000e-04\n",
            "Epoch 648/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.2224 - val_loss: 0.2082 - lr: 5.0000e-04\n",
            "Epoch 649/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.2184 - val_loss: 0.2563 - lr: 5.0000e-04\n",
            "Epoch 650/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.2200 - val_loss: 0.1957 - lr: 5.0000e-04\n",
            "Epoch 651/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.2194 - val_loss: 0.2137 - lr: 5.0000e-04\n",
            "Epoch 652/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.2225 - val_loss: 0.2105 - lr: 5.0000e-04\n",
            "Epoch 653/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.2199 - val_loss: 0.1945 - lr: 5.0000e-04\n",
            "Epoch 654/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.2226 - val_loss: 0.2173 - lr: 5.0000e-04\n",
            "Epoch 655/1000\n",
            "172/173 [============================>.] - ETA: 0s - loss: 0.2220\n",
            "Epoch 00655: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.2220 - val_loss: 0.2083 - lr: 5.0000e-04\n",
            "Epoch 656/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1767 - val_loss: 0.1862 - lr: 2.5000e-04\n",
            "Epoch 657/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1756 - val_loss: 0.1657 - lr: 2.5000e-04\n",
            "Epoch 658/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1762 - val_loss: 0.1814 - lr: 2.5000e-04\n",
            "Epoch 659/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.1762 - val_loss: 0.1855 - lr: 2.5000e-04\n",
            "Epoch 660/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1766 - val_loss: 0.1859 - lr: 2.5000e-04\n",
            "Epoch 661/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1756 - val_loss: 0.1781 - lr: 2.5000e-04\n",
            "Epoch 662/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1764 - val_loss: 0.1779 - lr: 2.5000e-04\n",
            "Epoch 663/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1757 - val_loss: 0.2021 - lr: 2.5000e-04\n",
            "Epoch 664/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1760 - val_loss: 0.1833 - lr: 2.5000e-04\n",
            "Epoch 665/1000\n",
            "173/173 [==============================] - 7s 39ms/step - loss: 0.1755 - val_loss: 0.1787 - lr: 2.5000e-04\n",
            "Epoch 666/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1763 - val_loss: 0.1798 - lr: 2.5000e-04\n",
            "Epoch 667/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1757 - val_loss: 0.1922 - lr: 2.5000e-04\n",
            "Epoch 668/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1756 - val_loss: 0.1949 - lr: 2.5000e-04\n",
            "Epoch 669/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1757 - val_loss: 0.1796 - lr: 2.5000e-04\n",
            "Epoch 670/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.1757 - val_loss: 0.1972 - lr: 2.5000e-04\n",
            "Epoch 671/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1753 - val_loss: 0.1917 - lr: 2.5000e-04\n",
            "Epoch 672/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1757 - val_loss: 0.1847 - lr: 2.5000e-04\n",
            "Epoch 673/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1766 - val_loss: 0.1856 - lr: 2.5000e-04\n",
            "Epoch 674/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.1763 - val_loss: 0.1922 - lr: 2.5000e-04\n",
            "Epoch 675/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1760 - val_loss: 0.1775 - lr: 2.5000e-04\n",
            "Epoch 676/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1765 - val_loss: 0.1854 - lr: 2.5000e-04\n",
            "Epoch 677/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1756 - val_loss: 0.1857 - lr: 2.5000e-04\n",
            "Epoch 678/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1757 - val_loss: 0.1856 - lr: 2.5000e-04\n",
            "Epoch 679/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1755 - val_loss: 0.1691 - lr: 2.5000e-04\n",
            "Epoch 680/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1751 - val_loss: 0.2031 - lr: 2.5000e-04\n",
            "Epoch 681/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1746 - val_loss: 0.1851 - lr: 2.5000e-04\n",
            "Epoch 682/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1752 - val_loss: 0.1761 - lr: 2.5000e-04\n",
            "Epoch 683/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1766 - val_loss: 0.1968 - lr: 2.5000e-04\n",
            "Epoch 684/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1776 - val_loss: 0.1835 - lr: 2.5000e-04\n",
            "Epoch 685/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1742 - val_loss: 0.1643 - lr: 2.5000e-04\n",
            "Epoch 686/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1763 - val_loss: 0.1940 - lr: 2.5000e-04\n",
            "Epoch 687/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1752 - val_loss: 0.1845 - lr: 2.5000e-04\n",
            "Epoch 688/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.1747 - val_loss: 0.1847 - lr: 2.5000e-04\n",
            "Epoch 689/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1757 - val_loss: 0.1815 - lr: 2.5000e-04\n",
            "Epoch 690/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.1751 - val_loss: 0.1931 - lr: 2.5000e-04\n",
            "Epoch 691/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1750 - val_loss: 0.1878 - lr: 2.5000e-04\n",
            "Epoch 692/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1743 - val_loss: 0.1754 - lr: 2.5000e-04\n",
            "Epoch 693/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.1762 - val_loss: 0.1905 - lr: 2.5000e-04\n",
            "Epoch 694/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1744 - val_loss: 0.1922 - lr: 2.5000e-04\n",
            "Epoch 695/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1765 - val_loss: 0.1808 - lr: 2.5000e-04\n",
            "Epoch 696/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1755 - val_loss: 0.1674 - lr: 2.5000e-04\n",
            "Epoch 697/1000\n",
            "173/173 [==============================] - 7s 39ms/step - loss: 0.1748 - val_loss: 0.1805 - lr: 2.5000e-04\n",
            "Epoch 698/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.1743 - val_loss: 0.1868 - lr: 2.5000e-04\n",
            "Epoch 699/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1752 - val_loss: 0.1908 - lr: 2.5000e-04\n",
            "Epoch 700/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1755 - val_loss: 0.1796 - lr: 2.5000e-04\n",
            "Epoch 701/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.1776 - val_loss: 0.1823 - lr: 2.5000e-04\n",
            "Epoch 702/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1735 - val_loss: 0.1858 - lr: 2.5000e-04\n",
            "Epoch 703/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1748 - val_loss: 0.1695 - lr: 2.5000e-04\n",
            "Epoch 704/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1748 - val_loss: 0.1892 - lr: 2.5000e-04\n",
            "Epoch 705/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1750 - val_loss: 0.1825 - lr: 2.5000e-04\n",
            "Epoch 706/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1740 - val_loss: 0.2050 - lr: 2.5000e-04\n",
            "Epoch 707/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1754 - val_loss: 0.1882 - lr: 2.5000e-04\n",
            "Epoch 708/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1752 - val_loss: 0.1858 - lr: 2.5000e-04\n",
            "Epoch 709/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1754 - val_loss: 0.1633 - lr: 2.5000e-04\n",
            "Epoch 710/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1756 - val_loss: 0.1869 - lr: 2.5000e-04\n",
            "Epoch 711/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1740 - val_loss: 0.1795 - lr: 2.5000e-04\n",
            "Epoch 712/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1761 - val_loss: 0.1837 - lr: 2.5000e-04\n",
            "Epoch 713/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.1771 - val_loss: 0.1911 - lr: 2.5000e-04\n",
            "Epoch 714/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1742 - val_loss: 0.1782 - lr: 2.5000e-04\n",
            "Epoch 715/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1739 - val_loss: 0.2055 - lr: 2.5000e-04\n",
            "Epoch 716/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1758 - val_loss: 0.1858 - lr: 2.5000e-04\n",
            "Epoch 717/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1730 - val_loss: 0.1851 - lr: 2.5000e-04\n",
            "Epoch 718/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1737 - val_loss: 0.1655 - lr: 2.5000e-04\n",
            "Epoch 719/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.1747 - val_loss: 0.1773 - lr: 2.5000e-04\n",
            "Epoch 720/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.1757 - val_loss: 0.1727 - lr: 2.5000e-04\n",
            "Epoch 721/1000\n",
            "173/173 [==============================] - 6s 36ms/step - loss: 0.1754 - val_loss: 0.1914 - lr: 2.5000e-04\n",
            "Epoch 722/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1747 - val_loss: 0.1919 - lr: 2.5000e-04\n",
            "Epoch 723/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1748 - val_loss: 0.1847 - lr: 2.5000e-04\n",
            "Epoch 724/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1728 - val_loss: 0.1830 - lr: 2.5000e-04\n",
            "Epoch 725/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1735 - val_loss: 0.1798 - lr: 2.5000e-04\n",
            "Epoch 726/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1739 - val_loss: 0.1790 - lr: 2.5000e-04\n",
            "Epoch 727/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1736 - val_loss: 0.1863 - lr: 2.5000e-04\n",
            "Epoch 728/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1758 - val_loss: 0.1800 - lr: 2.5000e-04\n",
            "Epoch 729/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1748 - val_loss: 0.1857 - lr: 2.5000e-04\n",
            "Epoch 730/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1749 - val_loss: 0.1815 - lr: 2.5000e-04\n",
            "Epoch 731/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1738 - val_loss: 0.1767 - lr: 2.5000e-04\n",
            "Epoch 732/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1747 - val_loss: 0.1830 - lr: 2.5000e-04\n",
            "Epoch 733/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1747 - val_loss: 0.1928 - lr: 2.5000e-04\n",
            "Epoch 734/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1739 - val_loss: 0.1815 - lr: 2.5000e-04\n",
            "Epoch 735/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1734 - val_loss: 0.1748 - lr: 2.5000e-04\n",
            "Epoch 736/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1745 - val_loss: 0.1966 - lr: 2.5000e-04\n",
            "Epoch 737/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1740 - val_loss: 0.1751 - lr: 2.5000e-04\n",
            "Epoch 738/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1735 - val_loss: 0.1715 - lr: 2.5000e-04\n",
            "Epoch 739/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1749 - val_loss: 0.1928 - lr: 2.5000e-04\n",
            "Epoch 740/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1743 - val_loss: 0.1901 - lr: 2.5000e-04\n",
            "Epoch 741/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1727 - val_loss: 0.1875 - lr: 2.5000e-04\n",
            "Epoch 742/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1739 - val_loss: 0.1854 - lr: 2.5000e-04\n",
            "Epoch 743/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1742 - val_loss: 0.1983 - lr: 2.5000e-04\n",
            "Epoch 744/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1735 - val_loss: 0.1888 - lr: 2.5000e-04\n",
            "Epoch 745/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1734 - val_loss: 0.1769 - lr: 2.5000e-04\n",
            "Epoch 746/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1744 - val_loss: 0.1863 - lr: 2.5000e-04\n",
            "Epoch 747/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1730 - val_loss: 0.1834 - lr: 2.5000e-04\n",
            "Epoch 748/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1737 - val_loss: 0.1916 - lr: 2.5000e-04\n",
            "Epoch 749/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1722 - val_loss: 0.1754 - lr: 2.5000e-04\n",
            "Epoch 750/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1739 - val_loss: 0.1825 - lr: 2.5000e-04\n",
            "Epoch 751/1000\n",
            "173/173 [==============================] - 6s 38ms/step - loss: 0.1732 - val_loss: 0.1755 - lr: 2.5000e-04\n",
            "Epoch 752/1000\n",
            "173/173 [==============================] - 6s 38ms/step - loss: 0.1737 - val_loss: 0.1849 - lr: 2.5000e-04\n",
            "Epoch 753/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1726 - val_loss: 0.1779 - lr: 2.5000e-04\n",
            "Epoch 754/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1767 - val_loss: 0.2018 - lr: 2.5000e-04\n",
            "Epoch 755/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1726 - val_loss: 0.1784 - lr: 2.5000e-04\n",
            "Epoch 756/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1745 - val_loss: 0.1876 - lr: 2.5000e-04\n",
            "Epoch 757/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1742 - val_loss: 0.1920 - lr: 2.5000e-04\n",
            "Epoch 758/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1738 - val_loss: 0.1783 - lr: 2.5000e-04\n",
            "Epoch 759/1000\n",
            "172/173 [============================>.] - ETA: 0s - loss: 0.1746\n",
            "Epoch 00759: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1746 - val_loss: 0.1837 - lr: 2.5000e-04\n",
            "Epoch 760/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1545 - val_loss: 0.1609 - lr: 1.2500e-04\n",
            "Epoch 761/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1546 - val_loss: 0.1581 - lr: 1.2500e-04\n",
            "Epoch 762/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1544 - val_loss: 0.1608 - lr: 1.2500e-04\n",
            "Epoch 763/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1538 - val_loss: 0.1631 - lr: 1.2500e-04\n",
            "Epoch 764/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1541 - val_loss: 0.1624 - lr: 1.2500e-04\n",
            "Epoch 765/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1541 - val_loss: 0.1644 - lr: 1.2500e-04\n",
            "Epoch 766/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1537 - val_loss: 0.1614 - lr: 1.2500e-04\n",
            "Epoch 767/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1540 - val_loss: 0.1612 - lr: 1.2500e-04\n",
            "Epoch 768/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1536 - val_loss: 0.1598 - lr: 1.2500e-04\n",
            "Epoch 769/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1538 - val_loss: 0.1657 - lr: 1.2500e-04\n",
            "Epoch 770/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1542 - val_loss: 0.1592 - lr: 1.2500e-04\n",
            "Epoch 771/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1543 - val_loss: 0.1663 - lr: 1.2500e-04\n",
            "Epoch 772/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1541 - val_loss: 0.1665 - lr: 1.2500e-04\n",
            "Epoch 773/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1539 - val_loss: 0.1660 - lr: 1.2500e-04\n",
            "Epoch 774/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1535 - val_loss: 0.1603 - lr: 1.2500e-04\n",
            "Epoch 775/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1543 - val_loss: 0.1603 - lr: 1.2500e-04\n",
            "Epoch 776/1000\n",
            "173/173 [==============================] - 7s 39ms/step - loss: 0.1543 - val_loss: 0.1540 - lr: 1.2500e-04\n",
            "Epoch 777/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1541 - val_loss: 0.1615 - lr: 1.2500e-04\n",
            "Epoch 778/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1535 - val_loss: 0.1658 - lr: 1.2500e-04\n",
            "Epoch 779/1000\n",
            "173/173 [==============================] - 7s 39ms/step - loss: 0.1533 - val_loss: 0.1574 - lr: 1.2500e-04\n",
            "Epoch 780/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1534 - val_loss: 0.1579 - lr: 1.2500e-04\n",
            "Epoch 781/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1542 - val_loss: 0.1603 - lr: 1.2500e-04\n",
            "Epoch 782/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1533 - val_loss: 0.1620 - lr: 1.2500e-04\n",
            "Epoch 783/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1538 - val_loss: 0.1566 - lr: 1.2500e-04\n",
            "Epoch 784/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1535 - val_loss: 0.1578 - lr: 1.2500e-04\n",
            "Epoch 785/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1533 - val_loss: 0.1630 - lr: 1.2500e-04\n",
            "Epoch 786/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1538 - val_loss: 0.1634 - lr: 1.2500e-04\n",
            "Epoch 787/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1538 - val_loss: 0.1583 - lr: 1.2500e-04\n",
            "Epoch 788/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1533 - val_loss: 0.1607 - lr: 1.2500e-04\n",
            "Epoch 789/1000\n",
            "173/173 [==============================] - 6s 38ms/step - loss: 0.1539 - val_loss: 0.1603 - lr: 1.2500e-04\n",
            "Epoch 790/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1530 - val_loss: 0.1620 - lr: 1.2500e-04\n",
            "Epoch 791/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1535 - val_loss: 0.1580 - lr: 1.2500e-04\n",
            "Epoch 792/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1543 - val_loss: 0.1592 - lr: 1.2500e-04\n",
            "Epoch 793/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1526 - val_loss: 0.1591 - lr: 1.2500e-04\n",
            "Epoch 794/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1536 - val_loss: 0.1568 - lr: 1.2500e-04\n",
            "Epoch 795/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1534 - val_loss: 0.1563 - lr: 1.2500e-04\n",
            "Epoch 796/1000\n",
            "173/173 [==============================] - 7s 39ms/step - loss: 0.1532 - val_loss: 0.1599 - lr: 1.2500e-04\n",
            "Epoch 797/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1533 - val_loss: 0.1632 - lr: 1.2500e-04\n",
            "Epoch 798/1000\n",
            "173/173 [==============================] - 6s 38ms/step - loss: 0.1536 - val_loss: 0.1573 - lr: 1.2500e-04\n",
            "Epoch 799/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1532 - val_loss: 0.1587 - lr: 1.2500e-04\n",
            "Epoch 800/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1535 - val_loss: 0.1643 - lr: 1.2500e-04\n",
            "Epoch 801/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1539 - val_loss: 0.1597 - lr: 1.2500e-04\n",
            "Epoch 802/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1535 - val_loss: 0.1617 - lr: 1.2500e-04\n",
            "Epoch 803/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1535 - val_loss: 0.1599 - lr: 1.2500e-04\n",
            "Epoch 804/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1536 - val_loss: 0.1641 - lr: 1.2500e-04\n",
            "Epoch 805/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1535 - val_loss: 0.1559 - lr: 1.2500e-04\n",
            "Epoch 806/1000\n",
            "173/173 [==============================] - 6s 38ms/step - loss: 0.1535 - val_loss: 0.1640 - lr: 1.2500e-04\n",
            "Epoch 807/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1534 - val_loss: 0.1580 - lr: 1.2500e-04\n",
            "Epoch 808/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1534 - val_loss: 0.1571 - lr: 1.2500e-04\n",
            "Epoch 809/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1530 - val_loss: 0.1573 - lr: 1.2500e-04\n",
            "Epoch 810/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1535 - val_loss: 0.1631 - lr: 1.2500e-04\n",
            "Epoch 811/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1535 - val_loss: 0.1632 - lr: 1.2500e-04\n",
            "Epoch 812/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1531 - val_loss: 0.1570 - lr: 1.2500e-04\n",
            "Epoch 813/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1530 - val_loss: 0.1581 - lr: 1.2500e-04\n",
            "Epoch 814/1000\n",
            "173/173 [==============================] - 7s 39ms/step - loss: 0.1537 - val_loss: 0.1680 - lr: 1.2500e-04\n",
            "Epoch 815/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1530 - val_loss: 0.1566 - lr: 1.2500e-04\n",
            "Epoch 816/1000\n",
            "173/173 [==============================] - 7s 39ms/step - loss: 0.1525 - val_loss: 0.1553 - lr: 1.2500e-04\n",
            "Epoch 817/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1525 - val_loss: 0.1610 - lr: 1.2500e-04\n",
            "Epoch 818/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1530 - val_loss: 0.1558 - lr: 1.2500e-04\n",
            "Epoch 819/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1530 - val_loss: 0.1613 - lr: 1.2500e-04\n",
            "Epoch 820/1000\n",
            "173/173 [==============================] - 6s 38ms/step - loss: 0.1526 - val_loss: 0.1546 - lr: 1.2500e-04\n",
            "Epoch 821/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1531 - val_loss: 0.1570 - lr: 1.2500e-04\n",
            "Epoch 822/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1526 - val_loss: 0.1595 - lr: 1.2500e-04\n",
            "Epoch 823/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1530 - val_loss: 0.1530 - lr: 1.2500e-04\n",
            "Epoch 824/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1534 - val_loss: 0.1692 - lr: 1.2500e-04\n",
            "Epoch 825/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1528 - val_loss: 0.1533 - lr: 1.2500e-04\n",
            "Epoch 826/1000\n",
            "173/173 [==============================] - 6s 38ms/step - loss: 0.1533 - val_loss: 0.1629 - lr: 1.2500e-04\n",
            "Epoch 827/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1525 - val_loss: 0.1619 - lr: 1.2500e-04\n",
            "Epoch 828/1000\n",
            "173/173 [==============================] - 7s 39ms/step - loss: 0.1523 - val_loss: 0.1669 - lr: 1.2500e-04\n",
            "Epoch 829/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1539 - val_loss: 0.1550 - lr: 1.2500e-04\n",
            "Epoch 830/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1526 - val_loss: 0.1554 - lr: 1.2500e-04\n",
            "Epoch 831/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1528 - val_loss: 0.1610 - lr: 1.2500e-04\n",
            "Epoch 832/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1534 - val_loss: 0.1631 - lr: 1.2500e-04\n",
            "Epoch 833/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1532 - val_loss: 0.1633 - lr: 1.2500e-04\n",
            "Epoch 834/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1526 - val_loss: 0.1557 - lr: 1.2500e-04\n",
            "Epoch 835/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1527 - val_loss: 0.1615 - lr: 1.2500e-04\n",
            "Epoch 836/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1529 - val_loss: 0.1578 - lr: 1.2500e-04\n",
            "Epoch 837/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1531 - val_loss: 0.1585 - lr: 1.2500e-04\n",
            "Epoch 838/1000\n",
            "173/173 [==============================] - 6s 38ms/step - loss: 0.1537 - val_loss: 0.1676 - lr: 1.2500e-04\n",
            "Epoch 839/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1528 - val_loss: 0.1687 - lr: 1.2500e-04\n",
            "Epoch 840/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1529 - val_loss: 0.1570 - lr: 1.2500e-04\n",
            "Epoch 841/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1533 - val_loss: 0.1554 - lr: 1.2500e-04\n",
            "Epoch 842/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1526 - val_loss: 0.1662 - lr: 1.2500e-04\n",
            "Epoch 843/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1535 - val_loss: 0.1525 - lr: 1.2500e-04\n",
            "Epoch 844/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1526 - val_loss: 0.1647 - lr: 1.2500e-04\n",
            "Epoch 845/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1527 - val_loss: 0.1653 - lr: 1.2500e-04\n",
            "Epoch 846/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1524 - val_loss: 0.1530 - lr: 1.2500e-04\n",
            "Epoch 847/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1532 - val_loss: 0.1536 - lr: 1.2500e-04\n",
            "Epoch 848/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1534 - val_loss: 0.1609 - lr: 1.2500e-04\n",
            "Epoch 849/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1526 - val_loss: 0.1629 - lr: 1.2500e-04\n",
            "Epoch 850/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1524 - val_loss: 0.1722 - lr: 1.2500e-04\n",
            "Epoch 851/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1525 - val_loss: 0.1609 - lr: 1.2500e-04\n",
            "Epoch 852/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1529 - val_loss: 0.1550 - lr: 1.2500e-04\n",
            "Epoch 853/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1524 - val_loss: 0.1626 - lr: 1.2500e-04\n",
            "Epoch 854/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1525 - val_loss: 0.1563 - lr: 1.2500e-04\n",
            "Epoch 855/1000\n",
            "173/173 [==============================] - 7s 42ms/step - loss: 0.1515 - val_loss: 0.1535 - lr: 1.2500e-04\n",
            "Epoch 856/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1527 - val_loss: 0.1606 - lr: 1.2500e-04\n",
            "Epoch 857/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1523 - val_loss: 0.1673 - lr: 1.2500e-04\n",
            "Epoch 858/1000\n",
            "173/173 [==============================] - 7s 40ms/step - loss: 0.1515 - val_loss: 0.1497 - lr: 1.2500e-04\n",
            "Epoch 859/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1516 - val_loss: 0.1573 - lr: 1.2500e-04\n",
            "Epoch 860/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1526 - val_loss: 0.1665 - lr: 1.2500e-04\n",
            "Epoch 861/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1521 - val_loss: 0.1572 - lr: 1.2500e-04\n",
            "Epoch 862/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1527 - val_loss: 0.1551 - lr: 1.2500e-04\n",
            "Epoch 863/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1517 - val_loss: 0.1619 - lr: 1.2500e-04\n",
            "Epoch 864/1000\n",
            "173/173 [==============================] - 6s 38ms/step - loss: 0.1523 - val_loss: 0.1628 - lr: 1.2500e-04\n",
            "Epoch 865/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1530 - val_loss: 0.1665 - lr: 1.2500e-04\n",
            "Epoch 866/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1522 - val_loss: 0.1620 - lr: 1.2500e-04\n",
            "Epoch 867/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1529 - val_loss: 0.1719 - lr: 1.2500e-04\n",
            "Epoch 868/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1521 - val_loss: 0.1579 - lr: 1.2500e-04\n",
            "Epoch 869/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1517 - val_loss: 0.1589 - lr: 1.2500e-04\n",
            "Epoch 870/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1526 - val_loss: 0.1586 - lr: 1.2500e-04\n",
            "Epoch 871/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1528 - val_loss: 0.1674 - lr: 1.2500e-04\n",
            "Epoch 872/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1523 - val_loss: 0.1600 - lr: 1.2500e-04\n",
            "Epoch 873/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1518 - val_loss: 0.1595 - lr: 1.2500e-04\n",
            "Epoch 874/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1520 - val_loss: 0.1659 - lr: 1.2500e-04\n",
            "Epoch 875/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1521 - val_loss: 0.1497 - lr: 1.2500e-04\n",
            "Epoch 876/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1523 - val_loss: 0.1493 - lr: 1.2500e-04\n",
            "Epoch 877/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1522 - val_loss: 0.1631 - lr: 1.2500e-04\n",
            "Epoch 878/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1519 - val_loss: 0.1618 - lr: 1.2500e-04\n",
            "Epoch 879/1000\n",
            "173/173 [==============================] - 6s 38ms/step - loss: 0.1519 - val_loss: 0.1538 - lr: 1.2500e-04\n",
            "Epoch 880/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1519 - val_loss: 0.1652 - lr: 1.2500e-04\n",
            "Epoch 881/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1516 - val_loss: 0.1588 - lr: 1.2500e-04\n",
            "Epoch 882/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1521 - val_loss: 0.1654 - lr: 1.2500e-04\n",
            "Epoch 883/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1521 - val_loss: 0.1615 - lr: 1.2500e-04\n",
            "Epoch 884/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1517 - val_loss: 0.1660 - lr: 1.2500e-04\n",
            "Epoch 885/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1521 - val_loss: 0.1661 - lr: 1.2500e-04\n",
            "Epoch 886/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1521 - val_loss: 0.1615 - lr: 1.2500e-04\n",
            "Epoch 887/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1533 - val_loss: 0.1658 - lr: 1.2500e-04\n",
            "Epoch 888/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1524 - val_loss: 0.1619 - lr: 1.2500e-04\n",
            "Epoch 889/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1517 - val_loss: 0.1596 - lr: 1.2500e-04\n",
            "Epoch 890/1000\n",
            "173/173 [==============================] - 6s 38ms/step - loss: 0.1521 - val_loss: 0.1628 - lr: 1.2500e-04\n",
            "Epoch 891/1000\n",
            "173/173 [==============================] - 6s 38ms/step - loss: 0.1520 - val_loss: 0.1597 - lr: 1.2500e-04\n",
            "Epoch 892/1000\n",
            "173/173 [==============================] - 6s 38ms/step - loss: 0.1522 - val_loss: 0.1576 - lr: 1.2500e-04\n",
            "Epoch 893/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1517 - val_loss: 0.1663 - lr: 1.2500e-04\n",
            "Epoch 894/1000\n",
            "173/173 [==============================] - 7s 39ms/step - loss: 0.1509 - val_loss: 0.1611 - lr: 1.2500e-04\n",
            "Epoch 895/1000\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.1518 - val_loss: 0.1663 - lr: 1.2500e-04\n",
            "Epoch 896/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1519 - val_loss: 0.1629 - lr: 1.2500e-04\n",
            "Epoch 897/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1517 - val_loss: 0.1713 - lr: 1.2500e-04\n",
            "Epoch 898/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1520 - val_loss: 0.1694 - lr: 1.2500e-04\n",
            "Epoch 899/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1519 - val_loss: 0.1525 - lr: 1.2500e-04\n",
            "Epoch 900/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1515 - val_loss: 0.1506 - lr: 1.2500e-04\n",
            "Epoch 901/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1518 - val_loss: 0.1577 - lr: 1.2500e-04\n",
            "Epoch 902/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1515 - val_loss: 0.1590 - lr: 1.2500e-04\n",
            "Epoch 903/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1515 - val_loss: 0.1613 - lr: 1.2500e-04\n",
            "Epoch 904/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1527 - val_loss: 0.1623 - lr: 1.2500e-04\n",
            "Epoch 905/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1523 - val_loss: 0.1625 - lr: 1.2500e-04\n",
            "Epoch 906/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1515 - val_loss: 0.1636 - lr: 1.2500e-04\n",
            "Epoch 907/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1514 - val_loss: 0.1740 - lr: 1.2500e-04\n",
            "Epoch 908/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1523 - val_loss: 0.1580 - lr: 1.2500e-04\n",
            "Epoch 909/1000\n",
            "173/173 [==============================] - 7s 39ms/step - loss: 0.1528 - val_loss: 0.1706 - lr: 1.2500e-04\n",
            "Epoch 910/1000\n",
            "173/173 [==============================] - 7s 39ms/step - loss: 0.1517 - val_loss: 0.1586 - lr: 1.2500e-04\n",
            "Epoch 911/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1514 - val_loss: 0.1730 - lr: 1.2500e-04\n",
            "Epoch 912/1000\n",
            "173/173 [==============================] - 7s 39ms/step - loss: 0.1521 - val_loss: 0.1622 - lr: 1.2500e-04\n",
            "Epoch 913/1000\n",
            "173/173 [==============================] - 7s 41ms/step - loss: 0.1509 - val_loss: 0.1603 - lr: 1.2500e-04\n",
            "Epoch 914/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1516 - val_loss: 0.1663 - lr: 1.2500e-04\n",
            "Epoch 915/1000\n",
            "173/173 [==============================] - 7s 39ms/step - loss: 0.1521 - val_loss: 0.1679 - lr: 1.2500e-04\n",
            "Epoch 916/1000\n",
            "173/173 [==============================] - 7s 40ms/step - loss: 0.1508 - val_loss: 0.1606 - lr: 1.2500e-04\n",
            "Epoch 917/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1516 - val_loss: 0.1632 - lr: 1.2500e-04\n",
            "Epoch 918/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1520 - val_loss: 0.1694 - lr: 1.2500e-04\n",
            "Epoch 919/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1520 - val_loss: 0.1675 - lr: 1.2500e-04\n",
            "Epoch 920/1000\n",
            "173/173 [==============================] - 7s 39ms/step - loss: 0.1506 - val_loss: 0.1618 - lr: 1.2500e-04\n",
            "Epoch 921/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1513 - val_loss: 0.1659 - lr: 1.2500e-04\n",
            "Epoch 922/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1517 - val_loss: 0.1556 - lr: 1.2500e-04\n",
            "Epoch 923/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1506 - val_loss: 0.1593 - lr: 1.2500e-04\n",
            "Epoch 924/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1507 - val_loss: 0.1649 - lr: 1.2500e-04\n",
            "Epoch 925/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1522 - val_loss: 0.1806 - lr: 1.2500e-04\n",
            "Epoch 926/1000\n",
            "172/173 [============================>.] - ETA: 0s - loss: 0.1518\n",
            "Epoch 00926: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1519 - val_loss: 0.1612 - lr: 1.2500e-04\n",
            "Epoch 927/1000\n",
            "173/173 [==============================] - 7s 42ms/step - loss: 0.1431 - val_loss: 0.1489 - lr: 6.2500e-05\n",
            "Epoch 928/1000\n",
            "173/173 [==============================] - 7s 39ms/step - loss: 0.1428 - val_loss: 0.1489 - lr: 6.2500e-05\n",
            "Epoch 929/1000\n",
            "173/173 [==============================] - 7s 39ms/step - loss: 0.1427 - val_loss: 0.1495 - lr: 6.2500e-05\n",
            "Epoch 930/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1428 - val_loss: 0.1500 - lr: 6.2500e-05\n",
            "Epoch 931/1000\n",
            "173/173 [==============================] - 7s 39ms/step - loss: 0.1427 - val_loss: 0.1486 - lr: 6.2500e-05\n",
            "Epoch 932/1000\n",
            "173/173 [==============================] - 7s 39ms/step - loss: 0.1424 - val_loss: 0.1489 - lr: 6.2500e-05\n",
            "Epoch 933/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1426 - val_loss: 0.1500 - lr: 6.2500e-05\n",
            "Epoch 934/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1427 - val_loss: 0.1493 - lr: 6.2500e-05\n",
            "Epoch 935/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1425 - val_loss: 0.1482 - lr: 6.2500e-05\n",
            "Epoch 936/1000\n",
            "173/173 [==============================] - 7s 39ms/step - loss: 0.1423 - val_loss: 0.1455 - lr: 6.2500e-05\n",
            "Epoch 937/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1424 - val_loss: 0.1451 - lr: 6.2500e-05\n",
            "Epoch 938/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1424 - val_loss: 0.1461 - lr: 6.2500e-05\n",
            "Epoch 939/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1426 - val_loss: 0.1495 - lr: 6.2500e-05\n",
            "Epoch 940/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1424 - val_loss: 0.1448 - lr: 6.2500e-05\n",
            "Epoch 941/1000\n",
            "173/173 [==============================] - 7s 39ms/step - loss: 0.1423 - val_loss: 0.1451 - lr: 6.2500e-05\n",
            "Epoch 942/1000\n",
            "173/173 [==============================] - 7s 39ms/step - loss: 0.1428 - val_loss: 0.1487 - lr: 6.2500e-05\n",
            "Epoch 943/1000\n",
            "173/173 [==============================] - 7s 39ms/step - loss: 0.1426 - val_loss: 0.1518 - lr: 6.2500e-05\n",
            "Epoch 944/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1426 - val_loss: 0.1484 - lr: 6.2500e-05\n",
            "Epoch 945/1000\n",
            "173/173 [==============================] - 7s 39ms/step - loss: 0.1425 - val_loss: 0.1485 - lr: 6.2500e-05\n",
            "Epoch 946/1000\n",
            "173/173 [==============================] - 7s 40ms/step - loss: 0.1422 - val_loss: 0.1451 - lr: 6.2500e-05\n",
            "Epoch 947/1000\n",
            "173/173 [==============================] - 7s 39ms/step - loss: 0.1423 - val_loss: 0.1491 - lr: 6.2500e-05\n",
            "Epoch 948/1000\n",
            "173/173 [==============================] - 7s 40ms/step - loss: 0.1421 - val_loss: 0.1468 - lr: 6.2500e-05\n",
            "Epoch 949/1000\n",
            "173/173 [==============================] - 7s 40ms/step - loss: 0.1420 - val_loss: 0.1479 - lr: 6.2500e-05\n",
            "Epoch 950/1000\n",
            "173/173 [==============================] - 7s 39ms/step - loss: 0.1423 - val_loss: 0.1452 - lr: 6.2500e-05\n",
            "Epoch 951/1000\n",
            "173/173 [==============================] - 7s 39ms/step - loss: 0.1422 - val_loss: 0.1440 - lr: 6.2500e-05\n",
            "Epoch 952/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1425 - val_loss: 0.1492 - lr: 6.2500e-05\n",
            "Epoch 953/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1421 - val_loss: 0.1471 - lr: 6.2500e-05\n",
            "Epoch 954/1000\n",
            "173/173 [==============================] - 7s 39ms/step - loss: 0.1419 - val_loss: 0.1443 - lr: 6.2500e-05\n",
            "Epoch 955/1000\n",
            "173/173 [==============================] - 7s 39ms/step - loss: 0.1424 - val_loss: 0.1486 - lr: 6.2500e-05\n",
            "Epoch 956/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1420 - val_loss: 0.1438 - lr: 6.2500e-05\n",
            "Epoch 957/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1420 - val_loss: 0.1456 - lr: 6.2500e-05\n",
            "Epoch 958/1000\n",
            "173/173 [==============================] - 7s 39ms/step - loss: 0.1425 - val_loss: 0.1458 - lr: 6.2500e-05\n",
            "Epoch 959/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1421 - val_loss: 0.1476 - lr: 6.2500e-05\n",
            "Epoch 960/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1426 - val_loss: 0.1468 - lr: 6.2500e-05\n",
            "Epoch 961/1000\n",
            "173/173 [==============================] - 7s 39ms/step - loss: 0.1425 - val_loss: 0.1449 - lr: 6.2500e-05\n",
            "Epoch 962/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1419 - val_loss: 0.1482 - lr: 6.2500e-05\n",
            "Epoch 963/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1421 - val_loss: 0.1474 - lr: 6.2500e-05\n",
            "Epoch 964/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1420 - val_loss: 0.1449 - lr: 6.2500e-05\n",
            "Epoch 965/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1420 - val_loss: 0.1456 - lr: 6.2500e-05\n",
            "Epoch 966/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1426 - val_loss: 0.1473 - lr: 6.2500e-05\n",
            "Epoch 967/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1424 - val_loss: 0.1490 - lr: 6.2500e-05\n",
            "Epoch 968/1000\n",
            "173/173 [==============================] - 7s 39ms/step - loss: 0.1420 - val_loss: 0.1458 - lr: 6.2500e-05\n",
            "Epoch 969/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1423 - val_loss: 0.1517 - lr: 6.2500e-05\n",
            "Epoch 970/1000\n",
            "173/173 [==============================] - 7s 40ms/step - loss: 0.1417 - val_loss: 0.1497 - lr: 6.2500e-05\n",
            "Epoch 971/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1419 - val_loss: 0.1541 - lr: 6.2500e-05\n",
            "Epoch 972/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1424 - val_loss: 0.1476 - lr: 6.2500e-05\n",
            "Epoch 973/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1419 - val_loss: 0.1452 - lr: 6.2500e-05\n",
            "Epoch 974/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1422 - val_loss: 0.1450 - lr: 6.2500e-05\n",
            "Epoch 975/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1423 - val_loss: 0.1468 - lr: 6.2500e-05\n",
            "Epoch 976/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1423 - val_loss: 0.1459 - lr: 6.2500e-05\n",
            "Epoch 977/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1422 - val_loss: 0.1486 - lr: 6.2500e-05\n",
            "Epoch 978/1000\n",
            "173/173 [==============================] - 7s 39ms/step - loss: 0.1422 - val_loss: 0.1465 - lr: 6.2500e-05\n",
            "Epoch 979/1000\n",
            "173/173 [==============================] - 7s 40ms/step - loss: 0.1416 - val_loss: 0.1460 - lr: 6.2500e-05\n",
            "Epoch 980/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1419 - val_loss: 0.1502 - lr: 6.2500e-05\n",
            "Epoch 981/1000\n",
            "173/173 [==============================] - 7s 39ms/step - loss: 0.1416 - val_loss: 0.1493 - lr: 6.2500e-05\n",
            "Epoch 982/1000\n",
            "173/173 [==============================] - 7s 39ms/step - loss: 0.1420 - val_loss: 0.1475 - lr: 6.2500e-05\n",
            "Epoch 983/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1420 - val_loss: 0.1436 - lr: 6.2500e-05\n",
            "Epoch 984/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1425 - val_loss: 0.1479 - lr: 6.2500e-05\n",
            "Epoch 985/1000\n",
            "173/173 [==============================] - 7s 39ms/step - loss: 0.1418 - val_loss: 0.1454 - lr: 6.2500e-05\n",
            "Epoch 986/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1419 - val_loss: 0.1459 - lr: 6.2500e-05\n",
            "Epoch 987/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1420 - val_loss: 0.1512 - lr: 6.2500e-05\n",
            "Epoch 988/1000\n",
            "173/173 [==============================] - 7s 39ms/step - loss: 0.1418 - val_loss: 0.1457 - lr: 6.2500e-05\n",
            "Epoch 989/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1420 - val_loss: 0.1493 - lr: 6.2500e-05\n",
            "Epoch 990/1000\n",
            "173/173 [==============================] - 7s 39ms/step - loss: 0.1419 - val_loss: 0.1448 - lr: 6.2500e-05\n",
            "Epoch 991/1000\n",
            "173/173 [==============================] - 7s 40ms/step - loss: 0.1418 - val_loss: 0.1544 - lr: 6.2500e-05\n",
            "Epoch 992/1000\n",
            "173/173 [==============================] - 7s 41ms/step - loss: 0.1416 - val_loss: 0.1476 - lr: 6.2500e-05\n",
            "Epoch 993/1000\n",
            "173/173 [==============================] - 7s 39ms/step - loss: 0.1417 - val_loss: 0.1507 - lr: 6.2500e-05\n",
            "Epoch 994/1000\n",
            "173/173 [==============================] - 7s 40ms/step - loss: 0.1415 - val_loss: 0.1469 - lr: 6.2500e-05\n",
            "Epoch 995/1000\n",
            "173/173 [==============================] - 7s 39ms/step - loss: 0.1419 - val_loss: 0.1467 - lr: 6.2500e-05\n",
            "Epoch 996/1000\n",
            "173/173 [==============================] - 7s 39ms/step - loss: 0.1417 - val_loss: 0.1451 - lr: 6.2500e-05\n",
            "Epoch 997/1000\n",
            "173/173 [==============================] - 7s 39ms/step - loss: 0.1422 - val_loss: 0.1456 - lr: 6.2500e-05\n",
            "Epoch 998/1000\n",
            "173/173 [==============================] - 7s 39ms/step - loss: 0.1417 - val_loss: 0.1466 - lr: 6.2500e-05\n",
            "Epoch 999/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1422 - val_loss: 0.1449 - lr: 6.2500e-05\n",
            "Epoch 1000/1000\n",
            "173/173 [==============================] - 7s 38ms/step - loss: 0.1419 - val_loss: 0.1483 - lr: 6.2500e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lrEHKKNy2O_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78e6f9b6-a05c-401a-f1cf-1836ec31a600"
      },
      "source": [
        "y_pred = model.predict(x_test, verbose=1)\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import r2_score\n",
        "score = r2_score(y_test, y_pred)\n",
        "print(\"RMSE:\", mean_squared_error(y_test, y_pred, squared=False))\n",
        "print(\"MSE:\", mean_squared_error(y_test, y_pred))\n",
        "print(\"Rsqr:\", score)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "173/173 [==============================] - 1s 4ms/step\n",
            "RMSE: 0.27721573027357743\n",
            "MSE: 0.07684856111111282\n",
            "Rsqr: 0.9633059836951142\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OL1sMWWyuUa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 501
        },
        "outputId": "81961bd8-932a-4c09-8410-33c9725130f8"
      },
      "source": [
        "plt.figure(figsize=(16,8))\n",
        "plt.plot(history.history['loss'], label='train loss')\n",
        "plt.plot(history.history['val_loss'], label='val loss')\n",
        "# plt.plot(history.history['lr'],  label='lr')\n",
        "plt.title(\"LOSS\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend(loc='best')\n",
        "plt.show()\n",
        "\n",
        "import pickle\n",
        "with open('/content/drive/MyDrive/ColabNotebooks/{size}mm_file/outfile{fnum}/Model_shuff{shuff}_bch{batchsize}_{rscore:.2f}.pickle'.format(size = gsize, fnum = filenum, shuff = str(shuffle), batchsize = batch, rscore = score), 'wb') as file_pi:\n",
        "    pickle.dump(history.history, file_pi)\n",
        "# with open('/content/drive/MyDrive/ColabNotebooks/{size}mm_file/outfile{fnum}/Model_shuff{shuff}_bch{batchsize}_{rscore:.2f}.txt'.format(size = gsize, fnum = filenum, shuff = str(shuffle), batchsize = batch, rscore = score), 'wb') as file_pi:\n",
        "#     history=pickle.load(file_pi)\n",
        "\n",
        "# plt.savefig('/content/drive/MyDrive/ColabNotebooks/{size}mm_file/outfile{fnum}/Model_shuff{shuff}_bch{batchsize}_{rscore:.2f}.png'.format(size = gsize, fnum = filenum, shuff = str(shuffle), batchsize = batch, rscore = score), dpi=300)\n",
        "# Traning loss 代表how well model fit 训练集\n",
        "# validation loss 代表 how well the model fits 测试集.\n",
        "# The training loss indicates how well the model is fitting the training data, \n",
        "# while the validation loss indicates how well the model fits new data."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA7AAAAHwCAYAAACfeoOHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdeZxcVZ3//9eppdfsG1khBFlCCAQJCKIsoqyKOCqguI2K4/51cPgN6rjgMuI2zqAo44LrgCij4oKDGxBUkH0JO4FAEgjZO713V9X9/XGruqu3dIeu6upKv56PRx5Vde+te0+34SFvPp9zToiiCEmSJEmSxrtEpQcgSZIkSdJIGGAlSZIkSVXBACtJkiRJqgoGWEmSJElSVTDASpIkSZKqggFWkiRJklQVDLCSJEmSpKpggJUkaQyEENaGEF4+yPEXhxD+HEJoDiE0hRB+HUI4uN81Hw0hPBlCaAkhrA8hXF10blkI4fchhG0hhB0hhDtDCKePxc8kSdJYM8BKklQhIYRjgN8D1wLzgX2Be4G/hhCW5K95K/Bm4OVRFE0CVgJ/KrrNr4E/AHOBOcAHgZ1j9TNIkjSWQhRFlR6DJEl7vBDCWuCdURT9sejYzcD9URS9t9+1vwM2R1H0lhDC14FMFEUfGuSes4DNwPQoinaU9QeQJGkcsAIrSVIFhBAagBcDPxvk9E+BV+Tf3wq8JYRwYQhhZQghWXTdVuBx4MchhLNCCHuVddCSJFWYAVaSpMqYQfz/w88Ocu5ZYBZAFEU/Bj4AnALcBGwKIfxr/lwEnAisBb4CPBtCWBVC2L/so5ckqQIMsJIkVcZ2IAfMG+TcPGBL4UMURf8TRdHLgWnAu4HPhBBOyZ9bH0XR+6Mo2g/YB2gFfljuwUuSVAkGWEmSKiCKolbgFuD1g5w+m74LNRW+0x1F0c+A+4BDBjm/DrhssHOSJO0JUpUegCRJE0g6hFBX9Pki4PoQwsPA94j/f/nDwDHAkQAhhLcRL9S0iri6egqwDPh7CGE68CHgR8ATxG3JbyeeNytJ0h7HCqwkSWPnOqC96M+pxIH0H4jnvT4FHA68JIqix/Lf2Ql8FHga2AF8EXhPFEV/AbqAxcAf89etBjqBt43JTyNJ0hhzGx1JkiRJUlWwAitJkiRJqgoGWEmSJElSVTDASpIkSZKqggFWkiRJklQVDLCSJEmSpKpQdfvAzpo1K1q8eHGlhyFJkiRJKoM777xzSxRFswc7V3UBdvHixdxxxx2VHoYkSZIkqQxCCE8Ndc4WYkmSJElSVTDASpIkSZKqggFWkiRJklQVqm4OrCRJkiRVWnd3N+vXr6ejo6PSQ6ladXV1LFy4kHQ6PeLvGGAlSZIkaTetX7+eyZMns3jxYkIIlR5O1YmiiK1bt7J+/Xr23XffEX/PFmJJkiRJ2k0dHR3MnDnT8Po8hRCYOXPmblewDbCSJEmS9DwYXkfn+fz+DLCSJEmSVGV27NjBN77xjef13dNPP50dO3aM+PpPfepTfPnLX35ezyo1A6wkSZIkVZldBdhMJrPL71533XVMmzatHMMqOwOsJEmSJFWZiy66iDVr1rBixQouvPBCbrzxRl760pdy5plncvDBBwNw1llnccQRR7Bs2TK+9a1v9Xx38eLFbNmyhbVr17J06VLOP/98li1bxsknn0x7e/sun3vPPfdw9NFHc+ihh/Ka17yG7du3A3DppZdy8MEHc+ihh3LuuecCcNNNN7FixQpWrFjB4YcfTnNz86h/blchliRJkqRRuPjXD/DgMztLes+D50/hk69aNuT5Sy65hNWrV3PPPfcAcOONN3LXXXexevXqnlV9r7jiCmbMmEF7eztHHnkkr33ta5k5c2af+zz22GNcddVVfPvb3+bss8/mf//3f3nTm9405HPf8pa38LWvfY3jjz+eT3ziE1x88cX853/+J5dccglPPvkktbW1Pe3JX/7yl7nssss49thjaWlpoa6ubrS/FiuwkiRJkrQnOOqoo/psSXPppZdy2GGHcfTRR7Nu3Toee+yxAd/Zd999WbFiBQBHHHEEa9euHfL+TU1N7Nixg+OPPx6At771raxatQqAQw89lPPOO48f//jHpFJxnfTYY4/lggsu4NJLL2XHjh09x0fDCqwkSZIkjcKuKqVjqbGxsef9jTfeyB//+EduueUWGhoaOOGEEwbdsqa2trbnfTKZHLaFeCi//e1vWbVqFb/+9a/53Oc+x/33389FF13EGWecwXXXXcexxx7L9ddfz0EHHfS87l9gBVaSJEmSqszkyZN3Oae0qamJ6dOn09DQwMMPP8ytt9466mdOnTqV6dOnc/PNNwPwox/9iOOPP55cLse6des48cQT+cIXvkBTUxMtLS2sWbOG5cuX86//+q8ceeSRPPzww6MegxVYSZIkSaoyM2fO5Nhjj+WQQw7htNNO44wzzuhz/tRTT+Xyyy9n6dKlHHjggRx99NElee4PfvAD3v3ud9PW1saSJUv43ve+Rzab5U1vehNNTU1EUcQHP/hBpk2bxsc//nFuuOEGEokEy5Yt47TTThv180MURSX4McbOypUrozvuuKPSw5AkSZI0gT300EMsXbq00sOoeoP9HkMId0ZRtHKw620hLqEoitjZ0U1Hd7bSQ5EkSZKkPY4BtoSe29nJoZ/6PT+/a0OlhyJJkiRJexwDbAk11CYBaO3MVHgkkiRJkrTnMcCWUGNNvCZWa5cBVpIkSZJKzQBbQslEoC6dsAIrSZIkSWVggC2xSbUpWrtcxEmSJEmSSs0AW2INNSkrsJIkSZLGnUmTJu3W8fHIAFtijbUpWjutwEqSJElSqRlgS6yxJkmbizhJkiRJKqOLLrqIyy67rOfzpz71Kb785S/T0tLCSSedxAtf+EKWL1/OtddeO+J7RlHEhRdeyCGHHMLy5cu5+uqrAXj22Wc57rjjWLFiBYcccgg333wz2WyWt73tbT3XfvWrXy35zziY1Jg8ZQJprE2xo62r0sOQJEmSNFZ+dxFsvL+095y7HE67ZMjT55xzDh/60Id43/veB8BPf/pTrr/+eurq6vjFL37BlClT2LJlC0cffTRnnnkmIYRhH/nzn/+ce+65h3vvvZctW7Zw5JFHctxxx3HllVdyyimn8LGPfYxsNktbWxv33HMPGzZsYPXq1QDs2LGjND/3MAywJdZYm2TDDluIJUmSJJXP4YcfzqZNm3jmmWfYvHkz06dPZ9GiRXR3d/PRj36UVatWkUgk2LBhA8899xxz584d9p5/+ctfeMMb3kAymWSvvfbi+OOP5/bbb+fII4/k7W9/O93d3Zx11lmsWLGCJUuW8MQTT/CBD3yAM844g5NPPnkMfmoDbMk1uoiTJEmSNLHsolJaTq9//eu55ppr2LhxI+eccw4A//M//8PmzZu58847SafTLF68mI6OjlE957jjjmPVqlX89re/5W1vexsXXHABb3nLW7j33nu5/vrrufzyy/npT3/KFVdcUYofa5ecA1ti8SJOBlhJkiRJ5XXOOefwk5/8hGuuuYbXv/71ADQ1NTFnzhzS6TQ33HADTz311Ijv99KXvpSrr76abDbL5s2bWbVqFUcddRRPPfUUe+21F+effz7vfOc7ueuuu9iyZQu5XI7Xvva1fPazn+Wuu+4q14/ZhxXYEmusTdLalSWKohH1mUuSJEnS87Fs2TKam5tZsGAB8+bNA+C8887jVa96FcuXL2flypUcdNBBI77fa17zGm655RYOO+wwQgh88YtfZO7cufzgBz/gS1/6Eul0mkmTJvHDH/6QDRs28I//+I/kcjkAPv/5z5flZ+wvRFE0Jg8qlZUrV0Z33HFHpYcxpMtueJwvXf8ID3/mVOrSyUoPR5IkSVIZPPTQQyxdurTSw6h6g/0eQwh3RlG0crDrbSEusUm1cVHbNmJJkiRJKi0DbIk11MRV17YuVyKWJEmSpFIywJZYoQLbYgVWkiRJkkrKAFtik+riANvcYYCVJEmS9mTVtp7QePN8fn8G2BKb0VgDwLbWzgqPRJIkSVK51NXVsXXrVkPs8xRFEVu3bqWurm63vuc2OiU2a1ItAFtauio8EkmSJEnlsnDhQtavX8/mzZsrPZSqVVdXx8KFC3frOwbYEpveUKjAGmAlSZKkPVU6nWbfffet9DAmHFuIS6wmlWBqfZqtLbYQS5IkSVIpGWDLYGZjmi1WYCVJkiSppAywpbRjHXz1EF6dvMUKrCRJkiSVmAG2lCbPheaNHBSeYquLOEmSJElSSRlgSymZhtkHsjj3FFttIZYkSZKkkjLAltqcg5nfsYbtbV1ksrlKj0aSJEmS9hgG2FLbaxmTuzYxOWphe1t3pUcjSZIkSXsMA2yp7bUMgAPDera2upCTJEmSJJWKAbbU5hwMwEGJp9nmQk6SJEmSVDIG2FKbMp9s7VQOCuvcC1aSJEmSSsgAW2ohkJt9MAcm1rkXrCRJkiSVkAG2DFLzDuHAsI6tzQZYSZIkSSoVA2wZhJn7MTm0075zU6WHIkmSJEl7DANsOUyaA0DGACtJkiRJJWOALYfGOMBGLc9VeCCSJEmStOcwwJbDpL0AyDVvIoqiCg9GkiRJkvYMBthymDQbgNqOLWzY0d7n1Nn/fQu/uHt9JUYlSZIkSVXNAFsOddPIJWqYHZq46+kdPYe7Mjlue3IbdxcdkyRJkiSNjAG2HEIgTJrNXokm7npqe8/hpvZuAJo7MpUamSRJkiRVLQNsmYRJc1jS0MaNj/TOgy0E2J35V0mSJEnSyBlgy6VxDovSO1m7tY3HN7UAVmAlSZIkaTQMsOWy4AhmND/KorCJX9y9AeitvO7sGMcV2HW3wecXQdu2So9EkiRJkvowwJbLijcCgU/s9Vd+dMtT7Ozo7gmu47oCe/N/QOdOePrWSo9EkiRJkvowwJbLtEVw2Bt4xY6fsaL7Ln5594beObDjuQIb8n8lotygp5/b2cFLvvBn1m5pHcNBSZIkSZIBtrxe+VVI1nDm5Ef5+V0baGqLg2tLZ4ZcLqrw4IYQQvw6RID99b3PsH57Oz+85akxHJQkSZIkGWDLK10HU+Zz+LQ27lm3g9vWxvNKowhausZpG/EwFdhEPuDmonEawCVJkiTtsQyw5TZlIYvT22moSXLzY1t6Do/brXSGqcAmE/H57HitIEuSJEnaYxlgy23KfFLNz3L2ykV9Do/fhZzyAZbBA2qiEGCtwEqSJEkaY2ULsCGEK0IIm0IIq4c4H0IIl4YQHg8h3BdCeGG5xlJRUxdA8zOc/8LJJELv4fFbgS20EA8eUJOFFmIrsJIkSZLGWDkrsN8HTt3F+dOA/fN/3gV8s4xjqZwpCyCXYcF3lvOxE+dw7pFxJXbcVmCHC7D507YQS5IkSRprZQuwURStArbt4pJXAz+MYrcC00II88o1noqZNKfn7Tv2b+cDJ+0PwMadHZUa0a4Ns4hT6FnEaawGJEmSJEmxSs6BXQCsK/q8Pn9sgBDCu0IId4QQ7ti8efOYDK5kFr0IGmbG7zc/wvypdTTWJHnsuebKjmsowwTYpKsQS5IkSaqQqljEKYqib0VRtDKKopWzZ8+u9HB2z+S5cOEaqJ0Cmx8hhMABcyfzyLgNsK5CLEmSJGl8qmSA3QAUL827MH9szxMCzDoANj8MwIF7TebR51oqPKghDLcPrKsQS5IkSaqQSgbYXwFvya9GfDTQFEXRsxUcT3nNXQ5P3wq3fpMD9prMttYuntnRXulRDWKYbXQKBVoDrCRJkqQxVs5tdK4CbgEODCGsDyG8I4Tw7hDCu/OXXAc8ATwOfBt4b7nGMi687N/gBSfB/13EabX3kk4GLvndw5Ue1UA9LcS73kbHFmJJkiRJYy1VrhtHUfSGYc5HwPvK9fxxp3EWvP4H8O0TmffXT/DB46/kK39+irMOn8/LDtqr0qPrNewqxPFrdvDTkiRJklQ2VbGI0x4jXQev+DTseIp3z32UA/aaxGd/8xC58VTNHCbAFgqzrkIsSZIkaawZYMfa3scAkG5ay3tO2I8ntrTytzVbKzyoIsME2ELWtoVYkiRJ0lgzwI612klQPx2a1nHaIfOY3pDmJ7c/XelR9RpmDmyh8moFVpIkSdJYM8BWwtRFsGMddekkpy2fx58e2kR7V7bSo4oNW4GNg6sVWEmSJEljzQBbCdP2hqZ1ALzy0Hm0d2f58a1PVXhQeYUAm8sMerp4DuxfH9/CnU9tG6OBSZIkSZroDLCVkK/AEkW8aN+ZvOygOXzuuoe4fe0Yh8Hm5yDb3e9gvoV4iADb00Kcg/O+83de+81byjhASZIkSeplgK2EaYuguxXatpL8/ulc/oLbmNaQ5turnhi7MXR3wFcOgN/8c9/jw1RgexZxcg6sJEmSpDFmgK2EmfvHrzf8Ozz9N2rW/ZU3HrU3f3joOZ7Z0T42Y8h2xa8P/KLv8cIiTrnB5+Q6B1aSJElSpRhgK2GfYyAk4Y7vxp+b1nHukXsTRfDzu9bDLd+AbWWuxkb5gDqghThvyDmwUZ9XSZIkSRorBthKqJ0MC4+M39fPgKb17D2zgaOXzOCqm+6F6z9Cx3dOL+8YcvlVhguV2IKeVZpsIZYkSZI0vhhgK2X562D2QXDU+dC+DbpaueAVBzK5azMAdW3P0tQ+RHW0FHoCar8gWtg+J9p1C3Ema4CVJEmSNLYMsJVy1Pnwvr/DzBfEn5vWc9S+M7jwxVN6Lrnpked2757tO+D+a0Z27RAV1p4AO8Qc2OJtdCRJkiRpLBlgK23qwvg1vy/sSQtyPad++Ns/86/X3Dfye913NfzvO+LtcYYzRIW1N8Dueg5s8SJOLugkSZIkaSwYYCtt2j7x69q/xq/Nz/acmtq6lqvvWEcmmxvki4No3hi/djQNf+2QFdjsLs8XsmpX0Zg6uocIw5IkSZJUQgbYSpu6AJa/Hv76X7Dl8T4BdkZoBuDZpo6R3as1nj9LV/Pw1+aGCMXDVGALrcPtXb3fbzfASpIkSRoDBtjx4KRPxJXPNX+Kq6jTFwMwnTiIrtveNrL7tG6JXztHEmCHmwO76wpse1fv+fYuA6wkSZKk8jPAjgfT9o5bidfeDDufgRn7ESVrecmCAMDvH3huZCsSFyqwnS3DXztsgB28QluYA1tcdbUCK0mSJGksGGDHi8UvhTU3wqaHYOYLCA0zOW7TlXw7/WW+/7e1/NsvVw9/j54W4hEE2CEXcRpuH9go/9p7zAqsJEmSpLFggB0v9jsxnrua7YR9joGGmQCclLibOjpZs2kEobSnAlv+FuJiVmAlSZIkjQUD7HhxwCm97/d+MTTMACARIvYNG4cPiV2t0J2fKzuiADu6RZyKGWAlSZIkjQUD7HhROxnmLoeQgMl7Qf30nlPvPSTHk1taePs3fseOtffBxvsHfr9QfYXRVWBz2b6v/QySX+mwhViSJEnSGEhVegAq8o4/QLYrfp/p3Tpnv/AMb0g+wec3fRe+nz/4qX57vRZWIIaRzYEtDrBRBCHk3++6AhsNkmDbDLCSJEmSxoABdjxJ18d/ADp6A+pe3U9zVvKxXX+3TwV2NxdxynT0Ptc5sJIkSZLGKVuIx6sFR8Svsw5gRvtalkzpmxy/c/MTbHvkL3DjJfF81kKArZ0CnTuHv39xQO1u731fCLBDrFI82BzYDgOsJEmSpDFgBXa8OumTcNgb4J4rCXdcwezJe/U5fcQfXs+MxOPxh6mLoGVT/H764ngv2c4WqJ009P2LF3EaLMAOMQd20AqsLcSSJEmSxoAV2PEqVQNzD4HZB0CmHbav7XP68MTjNCemwNS94aZL4gpszWRonA3P3AXfPXnX9y+uwBbNtx1uH9jB5sA2dw6xIJQkSZIklZABdrybdcCQp66ccwGccBHseBoe+wM0zoKmdfHJTQ/s+r7DtRCPcBudxpokT25p3fWzJEmSJKkEDLDjXXGA7RdmN3bUwIGnQSIF29bE1dfCAk7T9tn1ffsv4tRzfPcWcVo2fyprNo1g0ShJkiRJGiUD7HjXMBMOPB1e/Y3ehZ3y1rcloWEG7PPi+EDjbDjvp/H72sm7vm/xHNc+FdjCPrDDV2CPSTzAKVOeYu3WVjozzoOVJEmSVF4G2PEuBHjDVXD4eTDvsD6n1rUkyOUiWHJi/kgEc5fDIa/rG0oH0yfAtvW+H2YRp+IO4qtqPsc7Hv0nchGs3dI26PWSJEmSVCoG2GryonfDa7/b87EpV8cjzzXDkuPjA83Pxq+pur5twYMpqrD++b4nexdnGm4V4sGWIQbufnr78OOXJEmSpFEwwFaTEPq0BrdSx2n/dTMtM5fDyrfDGf8Rn0jvXoD9v7sfZ3NzZ/xhmDmwM9ufZG3dG3lReKjn2NLZtfzwlqcGXaFYkiRJkkrFAFtt0g09b198cLxQ0y/ueZatJ1wCC14Yn0jVQfcwAbZoEadJdNDenf88zDY681tWA3Be6o89x963sp4Hn93Jz+/asDs/iSRJkiTtFgNstSkKsP9xbryo08d/uZp//P7tvdek6uK9Y3dVES1qEW6gg47ufOV1mBbi1tR0APYLz/QcO21hhqMWz+CTv3qAdducCytJkiSpPAyw1aamN8A21KR63t+3vqn3mnRdHESz3UPfpyigTgodvasID9NCnM1n4oNSG3uOJXeu4ytnH0YA3n/lXbR3uSKxJEmSpNIzwFabogoswPf/8UiWzGoEYFNzvm04VQ9AR0cr196zYfC5qfmAmgnpISqwgwfYEMXHk7nO3oMb72fRjAa+fPZh3LehifdfeReZbO75/HSSJEmSNCQDbLWpaezz8YQD5/CF1x0KwP2FKmy6DoC/3PMgD/7sMzyxuXngffIBtSM5mcbQTkdhDmyhMhsNXkVN5PpVdWcsgTu+Bxvv55Rlc/n0mcv408Ob+LdfrnZRJ0mSJEklZYCtNun6AYeWzZ9CQ02Sr/z+UVo6M/EcWGD5nf/GR9JXwaPXQ1e/uan5gNqenMQkOujMjGwObKJ/ZfbMr0P9dLjiNNi6hjcfs5j3nbgfP7l9HZ/77UOGWEmSJEklY4CtNqmBAbahJsV/nrOCB5/dyfWrN/YE2Pr2eJ7q3jdfCNe8ve+Xcr0BNm4h7jcHdoj5swMqsHOWwjv/CCEBv70AgH85+UDeesw+fOcvT3LJ7x42xEqSJEkqCQNstUkM/j/ZSUv3YnJtirue3t5TpU1lWgFId26HpvV9v1AIsKGRSWGQADvEHNgBAbZuKkxbBCdcBE/cCE//nRACnzpzGW8+eh/+e9UTfP3Pj+/+zylJkiRJ/Rhgq9WUhX0+JhOBFXtP486ntvdUYGuy7b0XdPdrIc5lICRpD/VxBTaTg799DTbelz/fPeg2PImoX7BNJOPXI94KddPgTxdDVxshBD796mW85vAFfPWPj/K3NVtG9eNKkiRJkgG2Gr3/Tnj3zQMOH7HPdB55rpmtnfH/rKmoq/fkYAE2kaI91NMYOujsysDv/23gNf0konwF9rXfhZM+2XuiphFO+Xd46m9w7fsACCHwmbMOYd9ZjZz/gzu4fe223f9ZJUmSJCnPAFuNZr0AGmYMOPwPhy+kJpngk9etGfid/gE2ykIiSRt1NNJBd3fXwO9kBx5LFiqwB54OL72g78nDz4MTPwoP/BxuvAS6O5hUm+Kq849mryl1vO2K2wyxkiRJkp43A+weZO+ZDXz61ct4dNsg81e72/t+zmUhkaKVehppJ9PZNvA7gyzk1DMHNlkz+CCO/RAcfBbc+Hn41gmQ6WTOlDp+8q44xJ737b9zxV+e3L0fTJIkSZIwwO5xzjlyb8495oCBJ7JdkC0Ktrm4AttKHTUhC10tA77y0IatbGru6HOsZw5sYe5rf6kaOPsHcNblsPkheOR3AMyZUsc173kxxx84m0//5kG+dP3DZHOuTixJkiRp5Aywe6BDF88Z/ESmqAqbX8SpNYoXfEq2bx1w+du+8zeO++INfY4lchm6SEEIwwzibJg8D372VvjzZwGY0VjDN897IeesXMRlN6zh8psGaXWWJEmSpCEYYPdAy/aZO/iJ7n4BNpGiPUoDkOxsGnB5OmTo6M71OZaMusmSGn4QiSS84jOQSMNfvgpb47CaSib4wusO5bRD5vLVPzzKO39wO01tg+85K0mSJEnFDLB7oPqGxsFPFC/klF/EqSMXtwIniluIE3FATZMdcItkrptMGEGABTj09fDPD8TzZW/4XJ9T//6a5bz1xYtZ9egWjvjsH/ivPz42sntWSFNbt1sBSZIkSRVmgN0T5feBHaCrKMDm58B25uK/AslMUYBN1gKQJp7vmiuaq5qIMnSPpAJbMHkvOPq9sPp/4UevgfYdAExvrOHjrzyYq//paE44cA5f/eOj3PDwppHfd4y960d38MZv/53WzkEWyJIkSZI0Jgywe6IQ6Eg0DDzep4U4XoW4PV+BTXUXBdhUIcDGFditrb3b6SSjzMhaiIu95J/hRe+BJ1fBrz4ArVvg/z4CO5/l8L2n8/U3Hs5Bcyfz4Z/dy4PP7Ny9e4+RNZtbAQywkiRJUgUZYPdQTY2LBx4sbiHOL+JUaCFOZwYLsHFY27CjN/gmo8zIW4gLaifBaZfAyz4OD/0KvrQf3PoN+Ps3AahLJ7nsvBeSTgZed/nfeGpr6+7dfww01MS/p9augW3VY6ptG9z4Bcjlhr9WkiRJ2sMYYPdQnZP3GXjwB6+E1T+P3+cXcSq0EKczRaExH2BThQC7vTjA7sYc2P5e/EFY/npYcET8+aHf9ASx/WZP4ufvPZZkCLz/yrt5bmfHLm409goBtrmjwgtOXfcvcOO/w5o/VXYckiRJUgUYYPdQ8/YZZC9YgDuuACDKZYkSSdqy8V+B2mxxgI3n0KZDXG0sXrzoebUQFyQS8NrvwPl/hlddCtvWwFcPht9+GLY8zoJp9Xzl7MNYs7mFt15xG9+48XGuf2Ajv7nvmef3vBKqzwfYlo4KtxB354N9ZnwFfEmSJGksPM8kovEuPX3RoMc7aqZTB9y1dgvzk91xC3ECanNF7cX5CmxNvgJ77T3P8LEzltJQk3p+LcSDeeFbIN0AD10L91wZL/J07lWcnNrBRact5RPXPsDD/2rO8zcAACAASURBVPcIAAeHtZw850Rq5h48+uc+T/XpfAW20nNgE/E4yFW4lVmSJEmqACuwe6qDz4Jpe0PN5D6HH22Jq6sdXV20ZQLt+QpsXa6oApvsbSF+yQtm0dKZ4aFnd0IUkYq6nn8FtlgI8TY75/wY3vM36GqFH54JV53LOQcEXnXYfC485UBmT67lutqPUnP5MaN/5ij0BNhKV2DzWxyRczEpSZIkTTwG2D3VpNnwofth4RF9DreESfGbXJauHHTlw2j9IBXYNFlmN8bBra0rCxdP55DOu0tTgS02Y1/Y+2jIxqsd1675PV973VLed+IL+PW7j+y9LtNZ2ufuht4W4grPge0JsFZgJUmSNPEYYPd0/faE7erqJJPNEXIZOrIJMsTBrD4q2mInH2DfmbqOrz5yEick7o4DLPF+sNmQLv049zup9/11/xL/AeZm1vcef/a+0j93hAqLOLVUvIXYCqwkSZImLgPsnm6QANvamSUZcnRkA935Cmxj1DbgOysTjwJwQFhPW1dvYMqWugIL8erES8+Ew94Yf777R3GVccOdvc9d+9fSP3eEEiEA46GFOP+PrAFWkiRJE5CLOO3p0vV9Pma6OtjZ0U2SHF1RqqeFeFIYWIEtaAyd+Qps/h7lCLBTF8A5P4IogkVHwW8+BJ+e0XP6vty+HHjLN0ke/U8DfqaxkMnF1efKL+JkBVaSJEkTlxXYPV0hjB7+ZgCy3Z20dGZIkiNLkkwUt8ZOoijAJvsG2Em0014UYHPlCLAFIcDil/R+ftF7yLzkX/haOI/ato3kHr2+fM/ehVw+wFZ8G53gKsSSJEmauKzA7ulS9UCAM7/G9gf+SK4zDrC1ZMmS6Gkhrg9dRd8ZGGCfK67AlvuvzcwXwMs+DgedAXOWkgJOTd8BN3yWh9Y8ybJl5X38YHoqsONlEafIACtJkqSJxwrsnm7mfjBr/7iymawhketia0snKXJ9Amwf/ebNHp+8j+Me/kzvgfx80LIJAY77F5iztOfQmUfHe8A+++yz5X32EF604zc8UXsebR2DrIQcRfE+tmOxSrKrEEuSJGkCM8Du6Y46H95/e/w+VUOaLHev20GCHBmSdOdXIe6jXwV2btjGis3X9nyujcZ+O5t0bQNdpNm+bfOYPxvgVVu+SyJEJNu3Djz55Cq45u3wh0+WfyCJQguxc2AlSZI08RhgJ5BEqpYauvnvm54gRZbckBXY2oHHilQiwAJ0p6eQadvO9tau4S8usdbEZIDBA2xXS/y646nyD8RFnCRJkjSBGWAnkMmNDczKL+BbF7rooKZnH9g+Er2hNptqGHC6UgE2NWkGU2nly79/ZMyf3ZIPsDWdgwTYwu8rOwbzY20hliRJ0gRmgJ1AEqlaDpgVV1en0EpT1AgEcon0kN/prp8z4Fht1FGuIe5S7aQZHDg1x1W3Pc3WlrEN0YUAOzmzna5Mru/JnlA5lgHWCqwkSZImHgPsRJKqpZYMCXJMCe3sjBo4Z+Uiov4Btqi6l2kYGGBrKlSBpW4q8+s6yUXw+wefG9NHN4c4wM4KTTS19wuqyfzvb0wqsM6BlSRJ0sRlgJ1IkjWEXBeTaAPgXacczuf/YXlvACuIeiuM2YbZA25TqQosddOoy+xk8cwGrvjLkzy3c+zG0UkNUAiw/ebgFv4DwFiESgOsJEmSJjAD7ESSrIFMF1951WIAGqfMJJEIvQGsoGiP0VzdtAG32ZKeX85RDq1+GqGjiU+euYxndrTzlu/eRkvn2AS5RBQ/Z3ZoYkdbv0pryP9jNJZzYKPcrq+TJEmS9kAG2IkkVQvZTl6xJL/Pa93U+DVZ0/e6ohbiRKLvIk93JQ/l2/MvLucoh1Y3DTqaOHHJFP77zSt5dFMz37zx8TF5dCHAzmKQFuJCmByLObDBCqwkSZImLgPsRJJMx1XCjqb4cz7AhnxI7U7kg21RBTb0C7C3JlfSnpxS/rEOpm4qEMGX9uMli2o4ddlcLrthDRf/+oGBCyuVWE+AHawCWwiw2bEIlVH8YoCVJEnSBFTWABtCODWE8EgI4fEQwkWDnN87hHBDCOHuEMJ9IYTTyzmeCS9ZC5nOgQE2H4a6UpPi47neMNg974g+t8iQIBFC+cc6mEKluKsFbv82/+/l+zNvah3f++taPnHt6rKG2GQ+1E+mjR2VrMBGcYDd3txe/mdJkiRJ40zZAmwIIQlcBpwGHAy8IYRwcL/L/g34aRRFhwPnAt8o13hEHACzXQMCLNl4VeGOZLzSbnEFtvvg13Hz4g/0fM6SIFGh/Mqy18AJH4GFR8F9P+WguVO45fx9+O6Bt7H6jpu44L+vpbVMc2ILFdiG0ElTW79FnHoqsGMQYPMV2JsefmYMniVJkiSNL6ky3vso4PEoip4ACCH8BHg18GDRNRFQ6EedCvhv5eWUGjzAhmwcyNqThQpsb4BNJhO0TN6v53MmqmAFdtJsOCFfyL/xElj9c7juXzipbSsn1ULHpjQ/vu023vnSJSV/dLI4wA6owOZ/X0W/t7LJV2CjsXiWJEmSNM6UM8AuANYVfV4PvKjfNZ8Cfh9C+ADQCLx8sBuFEN4FvAtg7733LvlAJ4xCC3H7DiBATb7imokrsO2JfIAtqsCmk4FUqneV4myUIFQqwBbMPxyI4Jp/7HO4LnSzbmtLWR7ZU4Glk6a2fvvg5iuwuWzXGEwqj/LjMcBKkiRp4qn0Ik5vAL4fRdFC4HTgRyGEAWOKouhbURStjKJo5ezZA/cl1Qgla4AI2rZC3RRI5H/V+bmbbaEh/ly0RUsyEUilewNsppItxAXzDx/63JZHy/LIJL2BMdPZ1vdkvira1NLGk1tay/L8/s8qHo8kSZI0UZSzArsBWFT0eWH+WLF3AKcCRFF0SwihDpgFbCrjuCauVH4RpNZNvfNfi7SGxvhNrrgCm+gbYCvZQlwwaQ7MXQ6Ljob9XgabH4ZHr4d1tzJrx73A60r+yEILMQDd/QJs/veVJMuG7e3sO6ux5M/vkf+PCwlyRFFU+Wq4JEmSNIbKGWBvB/YPIexLHFzPBd7Y75qngZOA74cQlgJ1wOYyjmliK6zi27wR6qcPON2cr8B2ZzJ8Yr9f8ocHnuWWRCCV6t0nNkOip3BbUe/+S+/7g06HYz9E7tMzaWgrzzTqVHHLbme/Kms+VKbJUpsu9y8nrsCmyJKLIGl+lSRJ0gRStn/bjqIoA7wfuB54iHi14QdCCJ8OIZyZv+zDwPkhhHuBq4C3RVG+R1KlVwiwO5+BxoGt2DtzcYC9+dHnuOqBNrYwlVQikC6qwHZHyfFZ9Usk6EhNpa57O93Z0m+nk6S3Ahsy/VuI4+elyJJOljnA9rQQ58jm/EdFkiRJE0s5K7BEUXQdcF2/Y58oev8gcGw5x6Aiqdr4decGWPzSAacfborD1862jp5jIfQNsFkC6XGYXwG6a6czvbOZzc2dzJ9WX9J7J6Is3aGGdNRF6OpfgY2rs+mQHYNQWVyBNcBKkiRpYhkPzaAaK8neVmAaZw04vaUrPp+kbwUznS5qIY4SBMZngs01zGRm2MnaMiyklCLTs09uIjt4BRYgU4bqb99nxS8JcgZYSZIkTTgG2IlkmABbCKqJYQJsxVchHkLDtDnMSrTwgavu5u6nt5f03kmydKTiAJvsv4hTcYAdswpsDjuIJUmSNNEYYCeSPgG2aA7s3OUAHLA0fr0vt1+fr9XU9N1GZ1zOgQVqp8xhcX07DbVJXnf5LXzp+ocp1ZTqVJSlIxnvk5vOtvc9mesNsOWYf9tHYQ7smLQrS5IkSeNLWefAapwprro2FL1/+/XQ1copnY2cfN8XeSw3v8/XaooqsNnxsI3OUBpmkuzYzq8+/GI+97tHuOyGNbR2Zvnkqw4edehOkaGlUIHtH2D7tBCP3RxY1zuTJEnSRGOAnUjmHdb7vrgCW9MINY3sMwn2PvCFPPpQ3214+65CPH5biGmcBVGO6Yk2vvS6Q5lan+a7f3mSXBRx8ZnLRhViU2TpTE0BoC5qpzub611xuE8LcbkrsPH9XYVYkiRJE5EtxBNJYRViGHQOLMAHXrY/e89o6HOspqb3e5koQWK8JtiGmfHrX/+LAPzbGUt513FL+OEtT3Hxrx8c1a1TZOhKxxXYBjrp6C7aF7Zoj9juzNi0EKfz+8BKkiRJE4kBdqLZ+8Xx6yD7wAIctmgaf/rw8X2OFc+B7Y4SjNcOYhpmxK9//U9YfwchBD5y2kG89Zh9+P7f1rJ6Q9Pzum0uF5EiSzZRQzZRQ2PopKO7KKgWVWCz2a7R/AQjUGghzrgKsSRJkiYcA+xEc95P4R1/gJqGIS/paY3Nq60pWoWY5PidAzvvcFh0dPz+uy+Ha99PCIEPn3Igk2tTfO63D9HSmdnt22ajiBQ5SKTIJOupp6NfBbY3wOa6yxxgo945sLYQS5IkaaIxwE40tZNh0VG79ZWQ6J0qnYnCOJ4DOxPecT1M3Tv+fPeP4Oo3M2XdjXzsjKX8/cmtvOuHd+x28MvmIlJkiBJpsqmGAS3EmUxvKI66O0ryowytuIXYACtJkqSJxQCr4RUF2O7xvApxwTk/giUnxu8f+hX8z+s4d0kXl7z2UP62Zitf/cOju3W7bDZLMkSQSJGtmczk0N6nhbiruzfA5jJlDrCFCmzIUu71oiRJkqTxxgCr4fULsOM8vsL8FfDqrxcdCHDVuZxddxvnrFzE1294nD8//NyIb5fNxG3BUTJNrmYqU2mlvagC29ldXIF1DqwkSZJULgZYDeriM5fxsdOXxh+KAmyWxKj3VB0TUxfCiR+Dd/4J5iyFrY/BNW/n4lcv4+B5U/jnq+8d8XzYbCGUJlLk6qYxLbT0aSHu6u7ueR9lOkv6YwxQtApx1gArSZKkCcYAq0G99cWLOf+4JfGHRLLneJYqaCEuOP7/g4Ur4Yz/6DlU176Jz73mEJrau/nl3RtGdJtcNh9QE2mon86U0LcCm80WLeKUy/b/emnlF4xKkSUywEqSJGmCMcBqeCGQzf9VyZIcv4s4DWWfY+JKLMD621ixaBrL5k/hx7c+NaIQWGghJpmG+mlMo28FNpvLFF3b3f/rJVa8CnGZHyVJkiSNMwZYjUiWuAqbjRIkqi7BAnOXQ7IW1t9OCIHzXrQPD29s5q6ntw/71VwhlCZTJBqm0xg66epo7zmfzRbNgc3t/jY9uyUfuBPk3EZHkiRJE44BViOSC3GAzZCgWjqI+0jVwswXwJbHAHj1ivlMrk3x7h/fxbX37LqVuCfAhhTJhunxsfYdveeLW4izZQ6weUlyLuIkSZKkCccAqxHJFSqwJKlLJYe5epyasS9sexKAxtoU333bkSycXs//+8k9/P6BjUN+rRBgQypNetLM+Fhbb+U2m+1tJ47K3UKcD63JEBlgJUmSNOEYYDUihQpslgSLZzVUeDTP04x9YftaChuoHrXvDK5+1zEcuNdkLv71g3RlBp9U2jOvNZGmZnIcYLNt23rO5/q0EJd5ESd6Q2vOjWAlSZI0wRhgNSIhGW+lkyXBvrMmVXg0z9OMJZDthOZneg7VpBJccPIBbNjRzi1PbB30a1EuXsQpJNOE+mkA5Fp7A2w2V9xCPDYVWOg791aSJEmaCAywGpFkKg3EAXbh9PoKj+Z5mr5v/LrtiT6Hjz9gNgBvveI2rrrt6QFfyxX2eU3G2+gARB3Fc2CLqq7lDpVRUdU1W+5qryRJkjS+GGA1IqmeAJsknazSvzYzXxC/Pv33Pofr0kleddh8AD5/3UMDvhblq6qhKMCmOnrnwBa3DZd9H9jiFmIrsJIkSZpgqjSJaKwVV2Cr1rRFsP8p8LdLobVvu/BnzzqEfzh8Ac2dGZo7+rYBT3v4KiDfRl07lSwJ0p1DLOI0RtvoAOTK/SxJkiRpnKniNKKxFBLxHNhfvO+lFR7JKJ1wEXTuhMeu73N4an2aM1fMJ4rg/g1NADyysZmHH32EmY9eDUBI1kAiQUtyOg3dRYs4FVddc2WeA1tUgY1cxEmSJEkTjAFWI5NIQUhw6KLplR7J6MxbAY2zYc0NA04dujBeoOmN3/47v7r3GU75z1Wc/72/9pxPhPgfl7b0dCZleiuwxQE2Kve81D4VWOfASpIkaWIxwGpkEqn4T7VLJGDJibDmz7Dxfuho6jk1o7GGfz31IA6eN4UPXnU3ADX0tunmGuItdDpqZzItt4NcLg6TfSqhZW/rLa7AGmAlSZI0sRhgNTKJ5J4RYAGOeCu0b4PLXwLXXQi3fAO++RLo7uA9J+zHz9/74p5LU8Qh8f/rPp+u6fsDkKmbxUx20toVh9VchebAlv1ZkiRJ0jhjgNXIJFIQkpUeRWksfgm8+Rfx+/uuhus/As/dDy3PAfGqxF9/4+G8cOEk6oj3gN0aTaGjOw6quYZZzApNNLXF56KoeA7sGFZgs86BlSRJ0sRigNXIJFJxFXZPseQEeMVn+h7rbO55+8pD5/O/Xe/l2tpPANBNihmNNfHJSbOpD10074z3gs0VB8kx3AfWVYglSZI00RhgNTJ7WoAFWHBE389FAZYoIuxc3/Pxa296EYcsmApAcvJeALRt3xhfWjQXNURjt4hT2ReMkiRJksYZA6xGZk+aA1sw77D4db+T4tfiALvtiT6XTp3U2PN+8sx5AOzYvAGIW4gz5MP9WLYQlzssS5IkSeOMAVYjsyfNgS2onQQffQZO/Xz8uXNn77kNd/W9NpnueTtzn2UARM/cE7/mcnSTP1/ulYF78ys594GVJEnSBGOA1cjsKdvo9FfTCLVT4veFCmxnM/zt0r7XJXoDbGrWEp4OC5i/aRUQB9hMyP9uorGrwAbnwEqSJGmC2QMTicoikYr3UN0T1U6OX/9+OdRPi6uoG++D+unQvj0+l6zp85UHJh3Ny5uvhe72uIU4pCGCUOYKbBTlCPn3uWraB7Z1K6Rq46q3JEmS9DztoYlEJbcnzoEtqGkEAmx+GH72tt5W4sUv6b2mqIUYoGP6AaTJELVsglwuDrCUfxGnqM8+sFUUYK98Pfzp05UehSRJkqrcHppIVHL7nQjT96n0KMojhLgKWwiu3R3xa9203mv6VWCnTJ0BwM6mHURRliiMzSJOxQG27PNtS6lta/xHkiRJGgUDrEZm5dsrPYLyKg6G3W3xa/3QAXbG9DjAbty8GXI5opAgS5JE2bfRKd4HtooCbJTrM3ZJkiTp+bCFWALoKtpCp6MpXnG5ZnLvsX4txLNnzgRg67atPRXYXEgSyryIU5Sr0gpsFNFnCWVJkiTpeTDASv3teBrSDZAqqrr2q8DOmT0LgG3btxHlckCCXEiVfxEnequYVTUHNoqswEqSJGnUDLBSfzuehnQdJGt7j/WrwNY0TAWguWkbIcpBSBCFBOQyZHPlqzT2rcBWUSC0hViSJEklYICV+tvxFKTr+1ZgE30DLDXxdjCtO3dAlCNKJCCRIkmOlo7ytREXL+JUfXNgbSGWJEnS6BhgJYCpi3rft22NW4gLFdjB9sDN7x3btGMbmWyWiARRIkmSHDs7uss2zKi4ilnm+balFRlgJUmSNGoGWAngXTfC+X/u/Zyuh1Q+wPab/xofSxOl6jh+cT1JcqSSSUIiRZIszWWtwBZ9qLYKrIs4SZIkaZTcRkcCaJwFDTMhVQeZjnwFNt823G/+a0GomcTKeWnaU9Op7W6iqylFKuRo7uiGzua4zTiEEg+0uAJbRXNKnQMrSZKkErACKxWEAPXx/q6k63tbiAerwELcRtzZTH0qkEj0VmA7t2+Azy+EW75e8iFGRQs3Rdkqq8DaQixJkqRRMsBKxRqKAmxhEachA+wk6GyJW3lDgpBMkSJL2PZ4fP7h60o+vOJFnKKomgKs2+hIkiRp9GwhlorVT49f+y/iNJiaydDVEr8PSRLJNElydLXnj9U0lHx4xQG2urbRiXAOrCRJkkbLCqxUrBBgU3W7XsQJ8i3EO+NwFhIk8hXYTCHApksfYIkiMlEi/7aaKrDOgZUkSdLoGWClYj0txA29wXW4FuIoCyGQSCZJhxzdna299yixiBxZkgCEagqw2EIsSZKk0TPASsV6FnEqrsAOvgpxYREnohwkkpBIUZuIyBUCbDlaiHMRmcI/ttW2jY6LOEmSJGmUDLBSsdrJ8WsuO3wFtmZSPAc2ykFI5ANsjmc3b4vPl6OFmKinAltdc2BtIZYkSdLoGWClYun6+DXTMbI5sN1tkO3qCbDdmQztbc35e5VpDizVOAfW6qskSZJGzwArFUvVxa/d7UUV2CFWIS5Uazt2QkhCIkl9MqKezvh4Ilny4UVRrqgCW00B1gqsJEmSRm9EATaE0BhCSOTfHxBCODOEMMTEQKmKFVdgR9JCDPFKxPkK7EF7NTC3Lh8syxEwo4hMIcBWVQXWObCSJEkavZFWYFcBdSGEBcDvgTcD3y/XoKSKWbAyfj3olSNoIc4H2I6mPnNg927MB8syBMwoisjm/7ENzoGVJEnSBDNEb+QAIYqithDCO4BvRFH0xRDCPeUcmFQRs14AH98atw0XAuKQqxBPiV9zGUgkgBTkMkxJtMeHs5mS9+hHUUQmSkKguiqwbqMjSZKkEhjpv1+HEMIxwHnAb/PHSj/BTxoPCnNeE3FVddgWYshXYJOQyzKJOMC2d3aXfmxFFdiqCbA9rcO2EEuSJGl0RhpgPwR8BPhFFEUPhBCWADeUb1jSOJGs3UUFtn+AjSuw9VEbAG2dnWUYUO8c2FA1ATbX91WSJEl6nkbUQhxF0U3ATQD5xZy2RFH0wXIOTBoX0nVxiB1MYRViiAMsAXIZarMdQHkqsPEc2GT+fZUEwkIF1kWcJEmSNEojXYX4yhDClBBCI7AaeDCEcGF5hyaNA6+6FF70T4OfqykOsMmeCmw6m6/AdpShAhvlevaBpVoWcbICK0mSpBIZaQvxwVEU7QTOAn4H7Eu8ErG0Z1v6Sph94ODnBm0hzhKyXQDsbOsY+XMyXXDVG2Dj6l1fV7QPbNW1EDsHVpIkSaM00gCbzu/7ehbwqyiKuvHfRjXRpWp7F3gqWsQpZOPW4Z1tu1GBfe5+eOQ6uPZ9u7wsiqLeCmzVVDQLLcTVMl5JkiSNVyMNsP8NrAUagVUhhH2AneUalFQ1CisRJ3oXcSJfgW1u7yAa8bzPkH8d7vqIXGEf2GqrwPqfvCRJkjRKIwqwURRdGkXRgiiKTo9iTwEnlnls0vhXWMip0EKc7erZ3iaXzbJx5wjbiEMY/hriCmwuSpCLAqFaKprOgZUkSVKJjHQRp6khhP8IIdyR//MV4mqsNLH1D7Dd7T2nEuRYvWE3GxWGq9hGERGQJVFFFVhbiCVJklQaI20hvgJoBs7O/9kJfK9cg5KqRsOM+DUk83Nge7fOSZHjnnXbR3ijkVVg4wAbyJEgqrZViO0hliRJ0iiNaB9YYL8oil5b9PniEMI95RiQVFUmz49fQyJe1KnItPok96zbsZs3HMkc2EAuBALVUoG1hViSJEmlMdIKbHsI4SWFDyGEY4H2XVwvTQxT5vW+T/YNsDPqE9y3rolMdgTBrTAHdtgO4lxPBbZ65sBGfV8lSZKk52mkAfbdwGUhhLUhhLXA14F/KtuopGpRqMC2b4dUTZ9TcxrTNHdm+NPDm4a/T65QTR1+DiwQB9hclVRg3UZHkiRJJTLSVYjvjaLoMOBQ4NAoig4HXlbWkUnVYEo+wDZvHFCBndmYZO6UOq667enh71MIoyNaxCmQI0mgSgKhc2AlSZJUIiOtwAIQRdHOKIoKy6peUIbxSNWl0ELc8tyACmwiynH68nncsmYrXZlhwmY0sgpslA+wUaimVYidAytJkqTS2K0A288Il02V9mCFFuKW5wZUYMllOGpRPZ2ZHKufadr1fXKZET4w3kbHObCSJEmaiEYTYIf9t9EQwqkhhEdCCI+HEC4a4pqzQwgPhhAeCCFcOYrxSGNv0pz4dd5hA1Yh5okbOfn6k0iS5c61w2yns5stxFFIVE9Fs6cCa4CVJEnS6OxyG50QQjODB9UA1A/z3SRwGfAKYD1wewjhV1EUPVh0zf7AR4BjoyjaHkKYs5vjlyorkYT33gpTFsATNw483b6VA6ZF3LN+mO10RtgO3LeFuNoCbJWMV5IkSePWLgNsFEWTR3Hvo4DHoyh6AiCE8BPg1cCDRdecD1wWRdH2/PNGsFyrNM7MWRq/FldgE6metuBlMwMPbm7d9T1Gugox+QBLonr2ge35mazASpIkaXRG00I8nAXAuqLP6/PHih0AHBBC+GsI4dYQwqllHI9UXsmiRZxSvQ0KS6bA2q2tRLtqoR1xC3EOQiAXktWzjY4VWEmSJJVIOQPsSKSA/YETgDcA3w4hTOt/UQjhXSGEO0IId2zevHmMhyiNUHEFNl3X83bxpCxtXVk2t3QO/d2RrigcRRCcAytJkqSJqZwBdgOwqOjzwvyxYuuBX0VR1B1F0ZPAo8SBto8oir4VRdHKKIpWzp49u2wDlkaleBXiogrswoY4nK7d0jb0d3tWIR6+hTgQiKqqAltYhbhKArckSZLGrXIG2NuB/UMI+4YQaoBzgV/1u+aXxNVXQgiziFuKnyjjmKTyKd4HtqgaO6++G4Ant7QM/d2RhtF8BZaQGHnVttIi58BKkiSpNMoWYKMoygDvB64HHgJ+GkXRAyGET4cQzsxfdj2wNYTwIHADcGEURVvLNSaprJKDtxDPTHcxo7GGVY9uGfq7I50DS0ToCbBVUtF0DqwkSZJKZJerEI9WFEXXAdf1O/aJovcRcEH+j1TdUoMv4pToauHVK+bzP7c+zY62LqY11Az8bqGaOlzIiyLItxBXTSA0wEqSJKlEKr2Ik7TnGKICS2cLr33hQrqyOf5v9cbBvzvi+ayFFuJk9ewDW2gddhEnSZIkjZIBViqVPqsQN/S+79zJsvlTWDCtnutWb6QzM0hYHekiTlEEIUEUr+/SXgAAIABJREFUEoSqmQPrKsSSJEkqDQOsVCp99oEtqsB2tRBC4ORle7Hq0c2c9l83D9wTNhrpPrD5CmwiSYiyu95bdrzoqRRXwVglSZI0rhlgpVLpU4HtnQNLZzMA73zpEuZOqeOJza2s2dxvReLdaCEOxC3ESSIyuSoIhW6jI0mSpBIxwEqlkihaEy2Z7n3fGYfVBdPqueY9xwBwU/8ViXsC7MgrsImQoyszilDY1QodTc//+yNlC7EkSZJKxACr/5+9846zo6rb+DO3797t2U3PphcSICRACKF3EAQUUVBfXhUbCorCy4v6qlFEQaw0QTpKFQQC0ntoCQkQ0iC9bLJJttfb77x/nDl3zsyduffuJrtJdp/v55PPtDNnzp3dBJ77/ArZU2iasu819w0HFgBGVxZjQk0YC9c2WO8tOJ9VtNHRPB54kEYitRsC9oWfAQ9d2Pv7C4VViAkhhBBCyB6CApaQvsCjCNh4h+XSsZNr8N6GJkQTimiVRZwK6AMLzWOEEKcR3x0B29UAdO7q/f0Fo9u2hBBCCCGE9A4KWEL6AhcHFgCOmVyNaCKNpZtbzJMFhhBrunRgvYYDuxuiMJ3qgfO7GzAHlhBCCCGE7CEoYAnpC1QHNmYt2DR3whAEvB68vHqnebIHIcQwBKwXu5kDq6d6UDxqN2AIMSGEEEII2UNQwBLSF3jcHdhw0IfTp1VgwYd1pgCVQjJvBLERQuwRVYh3Kwc2neofUZlxYBlCTAghhBBCdg8KWEL6AjWEOBUDknHL5RvXn47/id+Kqx5bhlRaVwRsbkdUgzWEmA4sIYQQQggZTFDAEtIXqA4sAMSVMGJDNF7gex1PfrQdSzY1m0Wc5NYVm4DdbQe2HwUsizgRQgghhJDdhAKWkL5AOrC+IrFVw4iTMcvQD7a0mkIyj4CVRZw8Hg+8SCOxWw5sun8dWIBhxIQQQgghZLeggCWkL5AObFGl2FoEbDSzO6E6jMeWbkV7txC18UQC6XQukadD0zzQPL79pwqx6rxSwBJCCCGEkN2AApaQPcnp1wEn/dJ0YEPlYquGECsO7KzaSqxv6MLjSzaJYfE4Xvs0V29WHYBZhXi3ijjpKSDdx3mpHz0I3PdZ5ZnMgyWEEEIIIb3Ht7cXQMiAYu4lYrvwj2IbKhNbiwMbyez+8MRJmFVbgZJXvEAC8CGFaMJd5GmACCH2euHRdMR2J4Q4nex7B/aZH1mPKWAJIYQQQshuQAeWkL5AM/5qSQfWJQe2NhzHV+eOxbiqEADAizSau60Viy1kcmD3gAOb7o8qxJrtmCHEhBBCCCGk91DAEtIXSKcx6OTAmjmw+P1EYOdKDC8VwRB+LYXGduW6DQ1pwOOB5vXtgRDidLYD27QeaN3S+zntaDYBSweWEEIIIYTsBhSwhPQFUqjlyYGFngL+9XUMLfFnTjV1miHGdtQQYm13+8CmU9mC8qbZwF8O6v2c+WARJ0IIIYQQshtQwBLSF0ihJnNgW7eY4bpJm8PavAEexQlt6ezONTE0zbNnQoj1vRBCTAeWEEIIIYTsBhSwhPQFUqh5A2K76Dbg3ZvFvnRgv/RP4NirgHTCIiRbOyLY3hpBq0MurAYjB9YIIY7vbhsd6H3ritpDiJkDSwghhBBCdgMKWEL6AilINeWv2M5VYpswQoSrJgBeI3Q4ZYYVt3Z1Y951r+Lzf3sna1rNKOLk9Xjhgb77Dqy61v6ADiwhhBBCCNkNKGAJ6QukUNO8QKhC7IerxVY6sL4Q4DE6WSXMsOLtzSJfdkNDl+PUmuYxHdjdyoE17u3TVjr2EGI6sIQQQgghpPdQwBLSF0hRqGnAj1eJasTSeZU5sL6g6cAqvWF9sInSRbcDD10opsuEEO+hHFigbx1YViEmhBBCCCF7EApYQvqCjAPrAQJh4cImjOJMqgMrc2SVysReCEFZEjTc2W0fAFveha7rkEWcoHnhQRrx3e0DC9CBJYQQQggh+w0UsIT0BaqABYBAMRA3QoJVB1aGECuViR/51uE4fcZwdMaS6I4nhTubiCKV1uGBDs2jAR4vPNpuhhDvjRxYFnEihBBCCCG7AQUsIX2BdBqlgPUXOziwRWYIsZIDO7YigJOnDwMA/GtJHVLxCJCMIJlKZ0KIoXnghY6uaKL3a8w4sH0Y1ptVhJghxIQQQgghpPdQwBLSF0ih5vGKbSAMxKWAjYjiTl4f4MnOgUU6haGlQQDALxesxIfr6wEA2xqboQGZEGIA2N7qXOipsDX2hwNbQAgxw4oJIYQQQkiBUMAS0hfY2+j4i4CEDCGOifxXQCniFFPuTWJoWTBzqKWEO3veja8KB9ajAR4xb33LbghYexXivhCS+Yo4bX4H+O1IoLt5zz+bEEIIIYQMOChgCekL7Dmw/mLFgY2K/FfAMYQY6SSGloYyhyHEM1sNOjyKA7uzrRupdC+Fp92BTSd7N0/PHmo9bNkkQqu7Gvvh2YQQQgghZH+HApaQvmDuJUD1FGDG58RxIKzkwEZNBzYTQqwI2FQCVeEAfn7WdLz84+MQ0kSea5EWN0KItUxocjqVwo525d6eYK9CnNqNfFpX8jiw/VIJmRBCCCGEDBQoYAnpC4ZMBC59HygZKo79ahXiWLYDm4oBJaJwE1o2AgAuPno8Jg0tQYlHCEvhxMoiTkLAepFGXXN379aY5cD2gYDNCiG2ObDS9e3XSsiEEEIIIWR/hQKWkP4gUOzswEoBCwBDpwtHdsdyy61FmilgRQ6sJ+PAakhjS28FrBSP0hVN9UUIcT4HVq6BApYQQgghhOSHApaQ/sAfBlJxIRITUcBvCyEGRKGnmmlZAjYsHVgtDk8mB1b81S32A5/u6Oj5etKKkOxLB9aOXcDKYzqwhBBCCCGkAChgCekPAsVim+hyd2A9XmD4QcDOFZZbvUYVYtOB1TICdkpNMVbVt/d8Parj2Zc5sPYQYnsRJ7sLTAghhBBCSA4oYAnpD/yGgI13i1xYmQPr8ZljNK/Ine3cCSSMvrCpREZgfnPuMGgaUF0ayoQQTxsWxqr6dvzq6ZX41v1LCl+P6njK/VS8N58sD/lyYPujFy0hhBBCCBkoUMAS0h8EwmL7/p3A9g+AETPFsTdgjvF4TaErqxJLIQvgqNoSeKCjqjiYKeI0taYYrd0J3PP2Jry0aie64wXmsTo5sP3RRsetiBNzYAkhhBBCSAFQwBLSH0hhuuRuked64i/EsSWE2Gc6s4kI8OxVQN1i83rSELNKG5054yosj/loa2th63F0YPujCrFLGx06sIQQQgghpAAoYAnpD2QObKQZGDsP8Bqhw/YQYn+R2O+oBxbfDqx8wryecWPNNjpjKgL49TkzMLxM5NQu3dRS2HosDqwspKQIWLtTqrL25axCU+7kyYHV2QeWEEIIIYQUDgUsIf1BSHFKRx1q7tuLOMniTp0NYtuxw7weN9rlaGYRJ+g6LjpyHN776UmYPLQES7cUKGCdqhCrbXRyOaIPnAfcdnRhz8nrwPayD+zH/wIeOL9n9xBCCCGEkP0eClhC+oORswCvER48crZ53uMiYLt2ia0qYBNdYquZfWBV4XfYuEp8sLkF6XQO91TimAOrOLB7LB+20D6wPaxCvG0psOGN3i+LEEIIIYTsl1DAEtIfeLzADz4ATr8OGHqAed5SxMln9oftlAK23rwuHVioDqwpRGfXVqI9msTaXZ3515MvB9ZNwOYKLS6EPVWFOJ3on761hBBCCCFkn4IClpD+onw0MPcSa1itV8mBDYQBn5ED29Uott1N5vWEDCGGiwNbBQBYtFG5x418DqxbTqpSFbkgCi3i1NMc2FRCzJVm/1hCCCGEkMEEBSwhexM1hDhQYjqwMoRYxaGIkyr8xg0pxtRhpXh48VboDZ+KYktuKML34UWbsLq+3ebAugjKWEeOD+NEgUWceuzAytxZurCEEEIIIYMJClhC9iZqEadAWCni5CRglSJODg6spmn473njsLV+B7Rb5ohiS24owvf55dtwxl8XFhZC3GMBa3+uWw5sLxxY9X5CCCGEEDIooIAlZG/isYcQSwe2wTouVA7EZW6r6sBaBeH54yJYFvp25vjNNQ34vyeXY1d71DqfEnrrgdi/8tGlynU3Adue8+NkkRVCbM+B7WUV4lTc2NKBJYQQQggZTPjyDyGE9BmqwAuUmH1g7Q5s1QSgdat5T6aIk1XA+ju3ATDPXXT34sz+r84+ELquw+f1WBxPrzE+GY8DsqZUX4UQuxVx6mkV4ozwpQNLCCGEEDKYoANLyL5CoATwGa12Is3Wa2PmAi0bjQMN8Bh/de1CM1OpWCIEY9DnxXl/ewfH/+H1rPs8SOOu/z4Mfk3tA5svhNie2+pC1rA9VIVYOq90YAkhhBBCBhV0YAnZV1CrEKuU1wI1U8xjzeNYxAmAmScrp0QScfgRS6bw0dZWAEAknkKyO4pSY4wHOibUlKC6yAtI3ZrPgfUX5/4siYhRoCpPFWK9l1WI08yBJYQQQggZjFDAErKvEAibDqxK9WSgaqJ57FLECQAQ77IcBpFAHH5sbDTP3/7merz66ttYYIQLe5FGRZEftRU+wOjek9eBldWS3bh2ODDp5OzzbkWceuvAsgoxIYQQQsiggiHEhOwrBEqEOPXZxOH0c4Ahk5QTzm10AGQ5sAfUiCrH76w3e8Pe/sYGaLq1iFNZkR+1FQHzRjdHVBZxcnKK7ax7uQdFnHropGZCiOnAEkIIIYQMJihgCdlXCITFVgrYSacAl7wLzL4IKB1hjrM4sDZH05YD++jFs3H+oaMzurHI70UkkcoUbgKAkoAGr0fD2AqlpU8+B1br5T8dWQ5sb4s40YElhBBCCBmMUMASsq8gBaysRBwsBYZNNwSr+lfVvY0OEtYQYiRjGFYmBLHXo+HMg4UQ9tgELAAML/Ga9+UTsIUKR7vALrSI0xs3AP/+jnm89mVg4R/N41x9YGOdDs/Nw7YPssKvCSGEEELIvgcFLCH7CoESsZV5sOWjrNc9hkOqaWZorj3U116FOBlDUUAI09NnDMes2goAQNhvhvaWGgLWp87lWsTJCCHOFfKrhglnFW0qsIjT9g+BbUvM4wfOA175tbI+4/n2EOJ4N/C7UcDLv3Bfn514N3DXqcCHDxR+z+6QTveiHREhhBBCCAEoYAnZd/AaNdWihkisGGu9nsmNzVHEycGBPfOgETjnkJH47ecOwtRhovbwgSNKMkOKfIaYVV1VNwHbuVNsc7WvUcWtXZi65sDaxqXi4o8bbkWcpIu67BH3e+0kusU80bbC79kd3rge+N1ooLs5/1hCCCGEEGKBApaQfQ3ZA7Z8jPW8zyiypDkUcXr+J8C9Zzk4sFGMqw7jrxfMQnmxH7NGleD50ffhuxPNok5VRcZciiiNxGLOa2vdKra5HNhUDiHsVoXYLnRT8dwFmqS4tQvpTNEom1AGgOWPAQ+cn30+aXzWZNT9eXuSjx8W20hL/zyPEEIIIWQAwTY6hOyrVNRaj71GaLHmyXZg37tVbKecbr3HJsq8ndsxrfEFoPGFzLnTptcYc5mCcUdrJ8bb15NOA211Yl8VjjtWiLDn6snGOOVavhBimatqF7rpZG4H1q16sXR47U4vADx+sfNcqZh129dIYe7hP7+EEEIIIT2FDiwh+yoVLg4s4F7EyV6IKGkTZXaHFko+rCJKd7R0Zq+nc4cQp0VVVpF621HAzYeZx6kcIcRZRZykA2v7HKl47kJRbkWcMs9zELD2eyXJuHXb18g121sMEUIIIYSQvFDAErKvEiy1HsscWC1HDqy9OJA9LNap0m6mErAp7P7+xjoc9puXcfsb681xrVvEtmqCEJxulX4tubR2B9YmLHWXKsSpeJ48W9kH1jYmI4hzCNgskR+1bu28fj3w3P+6z9dT3PJ+CSGEEEJIXihgCdnb/M964Mq15vHxPwWmnZU9TlYnhmb2YbU7lx07rMd2B9Ze5AkwRWTMdF2PnliBCTVh/O65T1DXYri2Mv91yESxdcuDTeUKIXZzYO0CNpGniJMUgXYBW4ADm7C50Jl8Wpfnvf5bYNFt7vP1FDfXmRBCCCGE5IUClpC9TbgaKBlqHh//v8AFDi1dMjmwqoC1Cb9Ou4C1O7DZIcQZ0dfdCPhFL9qLjxyDP54/EwDwzMf14nq7kf9aOc64z8UhteTAFljEycmBTSfdndS0SwhxTgfWCNlNRKynCy3iZP9yoLe45e8SQgghhJC8UMASsr+gOrCZEOIksHGh+z1ZDqyDgG34ROR/djUCpcMy846pKsbs2grc+/YmNHXGTPEbLBNbtxBfNQc2K0zW7sAagtbJgXV6hhyfcU3tObBSIDsIWK/RR9ceQiyLN8kc2OWPAWtezL6/fln2ud7AEGJCCCGEkF5DAUvI/oJPdWANAfvOzcB9DuHGkkJyYD9+BHj3JqCrASgZLs4ZQvFXZx+I5u44PnvTW3hiyQbo3qApBN0E2B5xYF36vOopsTY5j1sIsZMB6zWKYNlFvBSuUsi+/Rdg8e3GPMpE9R87TNoLeuPAfvhPYH6588+PEEIIIWQQQQFLyP6CUxudprXu4wCHKsQuAmjty0B3E1AqBawQVweNLscD3zwCmqahpb0TSc2Xaf/SFY04z2XpA+vS5sZ+3akKsbpVx6uiVT4r0gpseU95Xg4HNisHVjqwsp1OwpxXLYrVsBq47WjgpsOAa2qce8oWQm8E7KvXim1XY++eSQghhBAyQKCAJWR/wVLEyes+rmq82WPU7sA6FXECgK2G+CsdIY4VcXX4uCosvOoEhH1pdKe8eH+rEHU3PLsCDR0OvVNzCbOsok5uVYjdQohTzgL5gfOBu08zRahTDqx0YO15wJk2OlLAxs15Iy3muNXPADuWiy8NUnFgrUOYcU/oSRGnzM8tR3EqQgghhJBBAAUsIfsLPsVZ9eT4q3viz4HvvAkESp37wEpxqyLFlJIDq+LxaBgSArrTPjy0RBR1enXVdsy51kHE5Wp/kxVCLEN+HYo4Oc1ld2DlOusWi21GsOcQsK4ObNR8pny+KmBTDmI9F93N2e9fpScOrBTdueYjhBBCCBkEUMASsr8gBWwqntuBrZ0LDJshxmc5sN2ZSsOOyBxYu6AEMKnKj4TuRRJCAPuRRACKCJOup1t1YjHIeph2cWDlHIlu4J2blNvTVlFrF7ipXA6sSwix6rzKrdyPthqDtOz58vH78cA/z3O/3pMiTvJ95KuUTAghhBAywKGAJWR/wRcS22TMbKPjRLjaHC/F2congUcvEjmwAQcBK0OHMw5strgaV+HHmKGVOO/wsQAAL9LwKwJ2zjXP42dPLEcqkaN/q1sRJ1Uwp9Pm+XdvAV78P+t4SwixTcDKcGAnB9YjqxC7CFhVyKZsIcTlYxw/Tl425agQ3Zs2OnRgCSGEEDLIoYAlZH9BhsCmYmYRJzulI8191YHd/Daw+mlDwBZn33fA2WJbNlpsncRVKg7NG8Rx08QzhoU9GFVq/hPS1d2FBxZtwfsbdrl/BrciTmlF2KqitGmdbXzKVsTJts6cDqwMIXZro6MWcbKFEJePzp4vF4W4qw4ud14oYAkhhBAyyKGAJWR/weLAOgjYU34NXLpYGa8I2Hi3cD8jLYDfQcAefzXw5X9lVSG2kIwBvkAmFPcfXzsUlx47NnP5+nOmYEJ1GK+s3Ob+GezCUnfIgVUrD3faxHA6aesza1tnRuDlKHaUsFVPtrfRUUOII0YIcYWLA+uW72t/hhO96QNLAUsIIYSQQQ4FLCH7Cz7DQUy6OLBlo4BgqXkcKDHbrsQ7xbar0TmEuLgKmHKqWeBJiqvnrgae+bHYT8WFi5kZk8Cp0yozU5wxrRL/deRYbGtqz/EhCsiBVUVh507b7TYH1h5CLIWnkwMrx9pDiFUHVtdtVYibxRcHxdXOHyfa5nzenmfrRKEhxOr7YA4sIYQQQgY5fSpgNU07XdO0TzVNW6dp2tU5xp2naZquadphfbkeQvZrZH9XtxxY6dBKxs4Dti0RIksKqu5GZwdWYhewi/4GLLlL7NsFbCqBIEzh6U3HcebBIxDQcjiLhVQhVgVbpoiSMl51aO0OaC4HVo61hxCrbXSkqJTPaKszvhgoyZ4PMB1aOwUJ2AIdWLUSMgUsIYQQQgY5fSZgNU3zArgFwBkApgO4UNO06Q7jSgH8EMCivloLIQMCKVBTMUBzqIrrtwnYKacJQbb+NZH7CgBdDc4OrEQ6u64hxEGzmm86aW0tk4xiaGkI04YWuc/vVsTJEHNdsST0XO1q0ilrGK1bCHEuB9Ye3iufl05YW+kAQMtmoKLW/Z3ZBbbE7vI6UagD292krDVHgSxCCCGEkEFAXzqwcwCs03V9g67rcQAPAzjHYdw1AK4HQGuBkFyoIcSO120CdvQc4ZhuW2oK2HRSiLGDv+Q8h6aJ/Np0IlsEphKGA6sKWEVQGeLv0NEubiWQo4hTEq3dcRz2m5fx1qf17vc3rwfuOiX7/swaczmwxtjl/wKWPaysW3mfMSPUOpUAElGgdTNQOVaEYwPZ79hNwEqR7A1az6vFqgot4qS6vHRgCSGEEDLI6UsBOwrAVuW4zjiXQdO02QDG6Lr+nz5cByEDA7WIk+N1m/Pp9QFFVSIENa6EzfqLgXNuAa5c6zyPv1gIMDV0VdeFOPQGxLyAEHlJVcCKdR04PEeIsurAptPICE09jbqWCCKJFLY2uOSVAkCjbc1ZIcQ5HEo1X/b13ylzKPfIXOFEF3DtMOF+Vow1c4uLh1jndA0hNt63rHyceZbqHhcoYFPZ75gQQgghZLCy14o4aZrmAfAnAFcUMPbbmqYt0TRtSUNDQ98vjpB9kZqpYjvqUOfrvmD2uWJDwKo5maFyEQZcMtR5nmApEGu3VgBORoU49AUVBzbh6MAWed0rAH+0VYjiaCKFaFy5N51CQ6cQZ+1dOcJvZfhy5r6k1dXN1UZHFbsjZ2WtG4D43HZUB7aoynrNtYiT4cD6bAJWfVahIcSq8KaAJYQQQsggx9eHc28DoPaeGG2ck5QCOBDA65rI5xsOYIGmaWfrur5EnUjX9b8D+DsAHHbYYTn6YxAygBk5C/jBR0DlOOfrfofc06JK4RJKZxEAiirM/UveFYJWJVgiQmm7FAEbbTccWL+liJOjO2ivDKzw8OLN+Kh6I+Y/vQrnz6zGDfKCnkJjh7i/sytHC5q4vYdrwirqchVxSieBOd8BNr5pzYNVXdtYZ/Z9ZaPMz1lsE7BOglddpz2EOGkV7QWRooAlhBBCCJH0pQP7PoDJmqaN1zQtAOACAAvkRV3X23Rdr9Z1fZyu6+MAvAcgS7wSQhSqxjsXcAKcHdiiSiOEWHVgFQE7bDpQPsp6T7AUiHVYHdhYu1GFWC3ilHJ0YO1hveuOvB5HRm8CAGjQMf/pVQCAt9fsyIxpbI9kHNiuSA4H1i5g00nrOaciTjfOApbeK8b6Q0KEqveoYb1xm4CtHAeMOMQs4mQPIU645KTuSQeWbXQIIYQQQjL0mYDVdT0J4FIALwBYDeBRXddXapr2a03Tzu6r5xIy6JCOqD0HFhBua+dOqyuqOrBOBEuFkLML2EwIsdkH1tH9tAmz8UPL4Q8I0TupRuTHBnwedEbMez+tb8X6XUJUdnX3wIFNJ6xtcexFnFJJoHmDyJ1NJUT4cyBsFaoWB7bDOv/59wrRG5A5sDYHNumyVhmyneXAKu+r0CJODCEmhBBCCMnQpzmwuq4/q+v6FF3XJ+q6fq1x7he6ri9wGHs83VdCesDp1wFff05UDQay3T5AOLDdjdZzoTwCNlBiOLCmQ2qGEAdMBzaVcHYHbQ6s1+fHFadOAwAcMrocxQEvfnDiJHhgFnTyamn8Z/l2AEB3pCchxHYHVhGjT19uCsxERAhGrxSw4p76tgiiUcXxtQtY+aVA6XDx3oYfZL1ub8mTOW/MmdOBLTSEWPlCIFeLIUIIIYSQQcBeK+JECNlN5l4CjJ0HHP0jcezowFY6nMvnwJYJIdey2TwXbRPOalYbHdWBNcRjOmG6tACgaTjnkNEAgNljyvH+z07GnPFD4FMErAdpRBPiOBrLESYbNwTmYReLnOB0whoera5n6T3AzlXGfYbj6vHi/e1xRLpE7uo1z6zCxp0t0OW7yxKwhoMaKgOu3gxMOd163S5gu5uB9/5mVnD22IpOJXtRhTjNEGJCCCGEEAkFLCH7Oyf8BJjfZra3UXESsPkc2KDhwLZsBKqniHPdTWLrCwAew/F16QObCdWVaF5AE//UaNAR7q7D5OhyqwOr7us5ckOl23rSL4SYTifdw4HlGoGMMNU9fqxoSCLeLQRsXUsE3nQc7f5qMW7N89b77YWxsgSpTVB++hzw/NXAW382Tuju43uaA+srYggxIYQQQgY9FLCEDGQcBWx59jkVWcSpeRMw/GBxrv4jsfUGzd6myZhjH1jh1PpNR9jjNQtP6Trw15mofORsHDQynLl1+nCxXxUOIACrsIt5S8wDKWB9ISEmU0mrC2oPsZXXDAEbTXnQhRCKEEMsmcLO9igCSOLd6DikDjwf2LrIer+9MJa9jY/dgbXntdpdVnV8wW10jHHBEgpYQgghhAx6KGAJGcg4CdhgWe57gqVCiMXagBGGgP3gfrH1BkQOqccvwmRVB1aG96aMEGJ/SBxrHkXAmk7rXV89xFymF1j805PwxDdmZAnYlqQiGuOdADQhLP0hUcBJLaRkF3iyzY0hYDuTGrr1IAJaCh9u2IWGjhjKfXE0J/xY4j0EWdjDsr1KTqs3mC1gbfm/FgH73NXA4tvN40KLOMk5A2EKWEIIIYQMevqyDywhZG9TVJV9zpPne6uA4ngOmWS95gsIMRquAboazfYyleNFtV9A5Gx6/ZmwYTWE2BJSm1ByV/UUhqZ2AHfMxOWlkwA1tRbezH6suwNBX0isoWS4yHF17AMrb7AShui8AAAgAElEQVQK2I4E0A0hrJ96fw3SOlCixaAFw/hgZxpH2N9FLgc2VJYdQmx3VdXjRX+zXStUwBpfEgRKmQNLCCGEkEEPHVhCBjJDD+j5PapDWzUBmP3f5rFsCxOuBroahLjSPMCwGUDDp+JaKikcWp/iwCLbgc3k1QJAOg1sfgcAMCa2zrKc8rDpgnZ1tJrObulw0SLIracrYBZlkgI2DnQZAvaN5ZugIQ1fKoLqqios3eEQ0mvvuevxmlWfg2VWEQ5kO7C5XNaeFnEKlu7fDuwLPwMe/NLeXgUhhBBC9nMoYAkZyHj9wGUfAEdcUvg9wVJzf8hk4OwbzdBZ6UCGa4SATRqtdWqmAY1rgOWPGQ6szyyA5PGYDqwqYJsMoeorEkJPCmBbiHPpoaboCaYjiGtBPPr+VqzsDIv72urMwfYiTlGrA9seB7p1IWCLtShCiEODjpFDq1Efc2hD5IR8F6FyIGF3YA2xKZ3vXHmuTtc+fACoX2Y9J9voBML7dxudpvVA88a9vQpCCCGE7OdQwBIy0BkyETjjOuC/ngC+/K/844NGCLE3YFY2loJMhtTKEOJUQriyNaLPKx6/GGj4xObAes19NWd0+eMiV3bcUcKNbFzrvJ4TfobPxX4lHqvFUN+l46rHP8ZfFhmVhVs2mWPdHFgjT7YtDkQ08RnCiCJsxCpPHj0Mk2tH5n838r0ARgixPQfWEJs/XgUc/KXcLquTO/vU94Dbj7WeSyfEO/SHnB3Y1q3Z7X/2RdIJa0sgQgghhJBeQAFLyGBh4onAlFMLGGiEzQ470Dwli0FlhRDHhCs7XBnbuUuckw6spgkh7CuyCq3NbwG1R4q59RTQsFqcl3mrEo8Hw0aOzhxGdCEgd+piTV31ivC1O7C2uVpjOrwh4TB//fAajAoLRzhQVIbfXjAv603c/dZGxJI2oSld6FB5dhEnKdB8IaPNj3GvRchqgD/cgxxYI6fYF3LOgf3LgcA9ZxQ2194klSi88jIhhBBCiAsUsIQQKyNnAZNOAT6nVMwtNhxY2QM2XCPcxx3LgZKhItf2jBvEta5G4axKtzaltIFRe7YCwPhjhbuYtoUCa9Z/mv7wxUMz+1H4cc4hI/Hfpx0JAEg2bchcS9sEnm4XsFEgWCQE7LkzKvHUt2aKC4EwikuzC179+plVmL9glfWkdGCDZdkhxLICs6aJrRRsqnPqL7JeyyzW1jPW/FCmo20XzJIdy53P7y3euw249UjruXSycNFOCCGEEOICBSwhxEqwBPjqY0DNFPOcdGClICwZKrZ17wNTThP7443QVz1l7QMrw2wDJUB3s/VZleOFKI62Wlvy+IqAc24F/vtp8bgisxpwSbgUV546FecdMwtpeFChmUWcIhFrUaVIR6vluL4jibLyCnEQ7zILQAXCosKyAw8t3oJtrYpwVB1YewhxOiHEJiA+V0bAKkLXX2Rcs4m5lM09zpw3coqLq8T7U4Wum+jd2zStM3OcJelkdpErQgghhJAeQgFLCMnPqNliGzAKPIVrzGvTPiu2oXLznL/YrBYsXcpACdCxwzpv5VjhtkbbrOf9IWDWV0xRrJmtdCaNHIIxVcWA1wdPeIjlNp8tx7KxscFyvKUtgTEjjVzXjh2KgC2BE89cdjQA4MkPtyGWTGHBsu3QMw5sqRCdqhBNJU2B6/GZea6qgPUVWcWtxK3CsBTF4RoRsr3xDfOLgH1VEKbixrtRinYNtBDijx4EFv5xb6+CEEIIGXSwDywhJD9H/QgYMROYeJI4HjkLGHuUyKuV4jakVA8OV5uFm6R4C5YAbdus81aMNcOSVfzF1mOP8k+Vz2yrg0CJyMX1h4FEF4KaVdAlutssX9PF016MHzsWWFML1C0GKmqNecKOH/vAUeWYM64KDy7agsbOGO55exNOHKahxKPk+CYiZuGrdMJcq8dnilu7A6ungKX3CAF93h3GGBcBK3Ng5ZcG958j3v3Xn812gPcVpJucigEe4z2lEwMrhHjVU6Ky8jFX7O2VEEIIIYMKOrCEkPx4PMCkk82+qOFqIaCOvdI85y82xVu4RlThBYAxR4htoAToqLfOWzLU4q5mkOJXfb7Er1yTzqmLAB0WtIblJuDF9BHlQO0RwJZFZk6uy/0AcNlJk7CtNYJ73t4EAOhOeUQebCZEWhGnUmwCwll2zIENmZ95+aPKvfkEbLV5bufK7Hn3JaSAtbyb5MByYJMx95xkQgghhPQZFLCEkD2DppnOaXgoMOE4YH6bmUsbLMluo6JpVgdW5tr67QLWxYGVzmfA5tgalMAqMALBAEZXFglR3bkD2GVUPs4hYI+eVI0vHTYmc9yR0BBJe/DWZiP8ONEt5mneYBZckmt2dGCLnV3nQkKIJXLfqSpxLja+Cbw8v2f39Ab5WdTPNNDa6KTi4mdPCCGEkH6FApYQsueQzpvqFkrsIrHYGKM6sGVGuxxVpNrH+MyCTpk5XXJY7QJj6ogqeDya2bdWOpk5BKymabj+CwfjlSuOw3mzR6MxoqMz6cGjHzUCAG56YTlw61zgxllmwSXAWmlYrVbsC7kIWAcxuvoZoG6pNYQYMPftVZDzsfpp4N1benZPb8g4sIqAHWg5sMkYBSwhhBCyF6CAJYTsOaRgUcWWRBaAAoBvvgJcbrR+kWLOGwSKjArBfpuAVR1Yvy0HFsgpQFVmjK6yrq9lEwAtWzA7MLGmBOcdOgpefxCaLwAtIFzi5z/alBnz1pp6xNLG5/F4AeiikFGWA+tQfsDuwCYiwCNfAdq2iPHFypcCsq1RTx3YeLcQl/Z+uXsaJwGbTgJ62lrYaX8mFRPvf6B8HkIIIWQ/gQKWELIHMdq6yDY7KkHFJS0ZZob9yhzZVMzMfc0SsKoDW3gOrJ0ZtUbVYukQt24W98oc28tXAN94wfX+eROrcdiEYaguK8HoGjFXSDPFYGd3FM3RtHXNeso9B1bFLmA3vmnue/3WNj+yfU5PBWyiy7rtK5IOObDSfR0oLmzmMzIPlhBCCOlPKGAJIXuefCHExUr7m6mfMfdleLC9iJOm/FOlCmG57yuyjnFh+uTJYqeo0iyypK6rYowpqN3wBgBvINOO56LJpoANIIGWqI7HltahU6Z7ppPZDqwq4lq3AG/ekC2E1r5o7su8Wsmn/wHmlwMtm3OvFQCi7cBT3wc2vyMcWMBsH9RXpBxyYGXLn4EiYKXLHGcYMSGEENKfsI0OIWTPE3ZwYAOK2FSLLnl9wMUvi/DSxX8X5+xtdGSlY3UewBSfvoAIs03lDo3VZK9aj1eI6K6GbPdW04C53wMO+KzzJId/E+hqxLjwUVj9YS1OaX4oc2lWVQK7Ov248l/LsCGwEVd5gGQiDp8q5Hwha8XhvxwktqdcY31OW525Lysbn/F74LmrzPP1H+X8vACABZcBq54UwlvmbOYTXe3bxXinLyIKwakK8UBzYOVnZB4sIYQQ0q/QgSWE7DnGHSO2TiG9QSMHtnx09rUxh4vWNtKBtVchdpoHUMSs5pxXKq8B2c6qzIN1Etun/w4YO895uoknAAefj7kTa1B+1DdQ3LExc6ky2YDJIyoxbkgx0ppYzzfuWYTWjg7zfn+Rcw5qxw7rsSr+pIA94jvApFPM84X0VW34VGw1j+m8yvZBbjx2sVUo95Sk0gdWMtAcWPmlBFvpEEIIIf0KHVhCyJ7jy48A3U1Wx1Qiw4KdBGxmjAwhzlFUSRXHUsCmE2ZeacVYYOQhwKoFAHQhRGumAcf+j3UeGcY8/CD3Z+Vh5PCR1hNdu+CpnoInvncUAktXA68Cdbua8J/X3sRXjCEf74zhwGQs+9vDTpuAVSsMqyHEahXmQtrSJJSw4USBIcTdTdacWzu6Lhxzp2rKgHMIcXqACdiMA9vH4diEEEIIsUAHlhCy5wiEgYpa52uRZrEtH+V+v1c6sLkErEMObDpliqmiSuCL9wOlI8RxqBw4609A2QjrPFLEDT/Q/Vn5CDq07/H6UBkOIFwkBOCTox/AV1JPZS4//2k7YjGH4ksdO63Hak6sVxGw6ruJKc6uG9IhjHcWngObirn3pQWAhy4ErnP5OQOm2ypdZF0feCHEdGAJIYSQvQIFLCGkf5h+LjD5VOCEn7mPkSK04BBiw41NJ80Q4kxbHkP0hSqc55Ehu7vhwDr2n5VuqbGesm0LM5ceSx2L1hHHwJvODiHuajJzXt9d34RkTBFGani0WuAq2pZ/jVJExrtNtzBfCHEynlvArnlOzBFzmSdpc2BV0ToQBKyumy4zBSwhhBDSr1DAEkL6h+Iq4Cv/AspGuo+R7WG8QfcxTkWc0klTuMpQYq8RAlvkImBlkaaaA3KvOxeODqxVwKotcw645J+49vtfRUDLzl1NKw7shXe8hx1NLeZFNSTbImDb869RDRuWDmy+wkP5HFjJtqUu99uKOKmiNVVA2PO+jvoZ+rqiMyGEEEIsUMASQvYddKOHaq6WOKpolG5nKqE4sFI4GnO4ObCnXQtcuc5aEbmnODqwNuGq5OzOGFkOzSk/GECpZjp5PzhhIoIwXVo9GcfOdkMMqu50tNXcl+JfEu8GNr1lisdYuxmWnE90JWO5e8zKAlh1i52vZwSsLOakCL5CCk/t66jFqejAEkIIIf0KBSwhZB/CEGG5BKwqGqVYVHNg5VYKCzcH1usHSmp6v1T7WtR51bXZe9o6UVRlOfzxyZNQ4k0hogsXecWWBhxz/WvY0tRtKXClRxQBa28htOAy4N4zzeOuBnM/bwhxLG9LIgBCINvRdSWE2MGBLSSEuHUL8I/PFxYivTdQq0izjQ4hhBDSr1DAEkL2HaSL6OJSArCKRlnQKFSWnQMr8z3dHNg9gVMIcSYH1lhHPrEIAGW2wlapOEKIowllAID2zi7EU2mceeNCbG43HczujhbLPRbs7qhFwOZwYNNpUTE4lwMrQ5c3v5vdUzadROaLCClkLQ6sQwjxlveAGyYBUpC/9ltg/SvAqqeyx+4LWBzY/VTA7lgO1H+8t1dBCCGE9BgKWELIvoMMIUYOAetVChqNnAWc9lvgnFvMkF25lcIqVL7Hl5nB0YG1C+kCBE642nqcjEJLx1FaNUxc9qUxc0wFOmJJ3P++mSsbhhm+2tphE6XptLlfbJvfLjrb64XbWf+xcwscyfYPgfnlYkztkWK7+R3rGFVIJ6Ning/uU9bl4MC+fp0Q2HVLjDkMkevN0cpnb6J+xv01hPi2o4Hbj9nbqyCEEEJ6DAUsIWTfoZAcWBVNA478vhCA9hzYZJ4Q4j2B2ge1qNI4ZwshlgRK4Yp9jc9fDQAorxoOAJg5shiPfHsubvnybEThLOpO+8NLuOE/H6KupRtPfbQNKdX1DNtCpaUDG+sEdq4E/jRNtMW5/RgzLNhJwH74gLk/+RSx3blcbHetBpo3Wu9LxoB1rwCvXWuec8qBlWHXUtymHHJn9yUYQkwIIYTsNXz5hxBCSH9RQAixG/YQYklfhhCrFFcDkZbsHFgAmPcD4JRfu98rxa/k40fE1nBmtVQcIb8XZx48AkPWjQccIj9vKHkQx77/Dg5a9CA64sDRwQiGGK+xGWWwZNnGO9HSFUfFR3dDe+231olaNoltKiZCutWfhRoyXTZKuN0ynPjWuWJ7xRpzTCqWncfq5MBK1zwjYA3hqhap2pdQQ4jtbjYZGOxYsXs9ogkhhPQZdGAJIfsOx/8UOOh8YOaFPb/XY/xzZndv+9KBVZFhwNKB1VR3tiK3KK8Y63xeFndSHL+5U0Y5Dj02KUJ5i+PNAAAvTKfz7R3WZ0e62jHrmpew7NP1plMt6TRDlLNcWL9ZURnBMtGTN9ZhHaOKu2Q0OwfYyVW1h1vLbaQle2xvePpy4LXfWc8te1iIlN5AB3Zgs+op4LajgBX/3tsrIYQQ4gAFLCFk36GkBjjvTufiSPmwhxBL+s2BHSK29hxYIH8ebu2Rzuel+FZFYa4euQAuOjCAa86ZgdKg+c97q6Y8P1iOtlYhDD/etCN7gs5d5n6iG2jbBqx5UfxRCzuFysQfu4BVxV0yll0wysmBla61dGtlwak9JWCX3gO8cZ313BPfESLFiY//BTz/E/f5BnobnaX3iVznwdrjttGIItjBIleEELIvQgFLCBkYuIUQ+wtoY7MnkALWKQc2aBOw338fOE1xBEce4jynFL4W11J3HJqZ+tAw/uvIcfDqpgN78qHTM/tbEqVoaWkCAPjSDnmuioCtf/0u4M/TgQfPF39i7cpnKhN/Yh1AShGl9iJOWQLWIQdWutVyfrmGPSVgVWIdQPOG3GP+/U3gvVuzz3c3C2G3/DHz3EB0YN+8QWy7GvfuOvYW8kuifTUHmxBCBjkUsISQ/YMfrQKuXOd+XQrGoGg9g7LRfb8mFX+x2HptbXSAbAe2Zgow9Qzz2OfiqlaOBw75CnCBUjzJLgDlcyUd9aICseKWjhg5JrO/0TseQ/wx3PCFgzFntO1eAJs2m+Lu9Xfetl5U3NY3tsSMEOJ2q7BN2Yo42UOIkxFRDMrSWscQwNF2IYa7hcDuEwF775nAjbN6d2/LRrFdcpfYenwDU8Dq6fxjBjKy+rVTITNCCCF7HRZxIoTsH5Q7535mkEKi0sgn/e7C/i0C5DP+p1cKV9WBDZU5jLc5w/+7GVj3MvD4xea5QDFwrs0J1G0CNlwDtG42j9/+q8ipVUWIdIeDZThu5gHAx8tx/mFjgPVewBZF7Is2ZboYhTTTTdW9QWiKgP3Js1uwcHIJvF27rO85K4TYJmBfuQZoWis+24zPiXPyZ/f+nUBxFTIuc18I2PplhY9Np6xfRNh/ZqGKgRlCLH93BqsDKf8upyhgCSFkX4QOLCFkYNC2TWxlQaTiKqBqQt8/d8LxYitdG+mQqsLH3sYGyHZdiyqyx/mKsu8bYQs3LhlqPW7bKsJ9VcoM8X/UD8ywX1235rQaDPOYVYMnhyPo1oO4OXkOtFQM6zZtylzbGfNjSX0Kkc5WPPSmWQxp8bp6sSPdSXsIcdNasVWdZFnJN94JvPQLsa95+0bA9gS7u2oXdEUVA7MKsRSwDr8fgwIZ0q5+GUMIIWSfgQKWEDIw6DLyJitdKvr2FV/9N/B/DaaAlTmgahXispHZ9zmFDcvwY4lT/u6QicD8NmCY0eIjPDR7jJ0RM4HLPgCOuVK4wXpKCEsH99CvmyKtBi1oRRjxYtGPtiy6DW+mDsIh0duRghfr2jR0trfg6cWrM/fc8bLhcBYPEX1mYzYHVhJrBxbfIfJJEw7FgspHAZF+cNBziRT72lO2saGKARpCbDjg0bbCwmjTKVHV2Sm/eX8k04eYDiwhhOyLUMASQgYWleP793kerwg5tAtYNYTY7+CkOlUTlnNI7CGrKtIdK3Fwd7Pm9Qvhq2lmjnCsPW/4aw3aEAhX4gdnHQEAGIoWzJwyHleceySevvRoTK4dgVJEUAZTxFVoQvQ1pkvR0NiADdt3Os6NaBvw7JUiZNqp2m3FWCO/1kUAF4qeu+hVJt/WCfu67GKuqHJghxDf+xng5sPzj196j6jqvOTuvl1Xf5ERsIM0hJoQQvZxKGAJIQOLosq989yMgDX+p9fezidrvHH9uP9Vztkc2FwCNmEIWNWBPXk+MPUz2WPVHrTBUrGNdeQNEfVEm1FdPRS+kurMufKKIfivuWNx0OhyzJk2DiEtgauONt95BYTgXNMZRDDdhc6Otqx5ASDWaYYH605huOOOFtu6xc6LS0SAT57NuX4A+R3E7hyVdrN62NoFbIWze6zSskkUp9qXSLsUaUrGgN+Nsb4TNb/ajQ7jS4ru5t1fW3+RjLs7/PJ3hkWcCCFkn4QClhAyMJhwggjbVcVafyLFZ8aB9bqPlcxvA074qTKHzYF1cm4lScP5U3Ngy8cA1VNyP1NWRI46OLBOrnCo3CisZCAFMJBxcyf4TTE6Z7h4/016KUq1KEaEnF2sx98ye2zGIx1Z15f6DgE0D7D5HWxvjWD+gpVo7ooD7/0NWPYI8OLPgYcvBF77HfDp8+6fVxWdTm5srlYxWQ6sUwhxHgf2njOBt/6ce0x/k3ZxFrsarBWlC0U6ttp+9L8UD10AXO+SbsAQYkII2adhFWJCyMDgoifdnaX+ICuE2BCwgZLez1mIAzv0APNcUYVVbJ53F7D9Q+t9mRDiNqsDe8o1wnl7+6/W8aFys4oxYK2oLMVsW13m1JEVbUATMGvmLGgr3kMNnF2u0ZopHLV4V6byseTmRW34bfFUDNnwNi5YchS2NHejLOTDj9feK3KKZb/dN64T2/nOTq/FRXNy1HoiYJ0c2FRctP7xuvzntKsht8u7J9n+EfD344HLlwMVY9zHWVoYpQGPITx7m8OaEbA9/PIoEQEa14gc7b5C14ENr4tia+r61r/ifo/8PRmIBboIIWQAsB99XUoIIXnw7MV/0uwhxHJbMqzwOewhvYU4sMVDgCMvFfvhGlNsevzAQV8ATrvWep8UoFves/R1xdTPOOcPh8qBItWBdRKwWzOnSlpWAUWVGD1+mjhhD8MFAM2D8d5dmcOAlsSNyXNxa/LszLkVDUm80TYMbVtXYkuzEBIPvb8V6Vgnop2tSHls4da6Dmz7IPtZFgHr4JbaWy2pLm1WCLEi/IqrzXfhVshJ14XoTfRTNd+l9wLQgTU5HGnAWoxKdWN7W3W4tw7sgsuA24/N/SXC7rL6aeAf54qCYU44iXb5fmLZkQGEEEL2PhSwhBCyJxg7T2xnXii2ZaOAQClw+nWFzzH8YOCwb4iKwT+tzx2GLEVDqAI49TfAN18VTpYUm245uFJ0vXmDNVzUHxIhyHZC5dZqyCNnZc/VuMY817wBGDLJGmpsp2w0xsBa3Gly7ShMnGSGP3cjiPCwCajR2jC3dBc+qfkJwp2b0d7Wgi31O7Gs3ioa9UW3A3ecgK1Ln8dDi7dgxTbDkVUE7Gsfrc1eS5ZIVcRdriJOIw4WvWwB9zBiKQh7IwxjHcDCPwp3Nxcv/QK4xijk5c+zHokqxNV9NyEebRPi2K0gVm8F7OZ3cj93TyAF+rqXnK87vSv5O+D05QshhJC9DkOICSFkT1A51hrGGiwBflrnPt4Jjxc4q4f5kkUVIjRy9KHiWDqw9oJQEjUEWMVfDJSPdhhfIbZn3yT66o6ZY16T4yMt4lrzBnE8ZBIQLHdfc/kooG2L5dQZsyeJkOlN4vjv3zgG8yIp4Im7ccfBnyK0dDPmH9SMkjVRdCOCj1uSmK3o++dffh5nALjx36/gX6kUDhtbiYe/PRfrtjXC8IJx04K3cYItzTfS2QqLz62KqVxFnIYfpAhGFwG2OwL2lV8Di/8OVE0EZpzrPk4N+ZaOfV4B6+LAujnFT18OrPw3MOwg8/dMpbchxNL9zFcpeneQTv2u1c7Xk1Hxd9Vyzvg504ElhJB9EjqwhBCyPyLFkz1PVubAurlh/rDzeV9ICEs7sujT7IvMysCSilqz3+3Q6eb5qol5HFiHvrj+sOUzHTVlGDSjp2/pxhcAAMeXbYcPKVT744jrVnc6FRMisjgUxNRhpViyuQVn3fQWrnrYrGI8RMsuUPTiB+vEjhRRqoh7/mrgk/+Yx7KIU/VUYO738jueUggV2mqnaT2w6imx3yrDsgsUd+mU+buQz9G0CFglhNYpxBoAWo0vG9IubrAUsD1tO6Onstezp5FzK2HuFpzelRpC3JfimhBCSK+ggCWEkP2R7ywEvnBPtusl2wi5VSN2yxP2hZxFZ9kI9zV4/cJ5BoBhM8zzNVPcnV7AWcAWVQABQ1xLYVhRK7bS2a17X1xOdqG6yPq5DzOKMX/tyLG49MRJAIBPdnTglCkVmTFTS7IFWiLSjsW3fw8d10/Dkg0NeOCtT6wDHv4yfvXAi4gmUqYD++3XgdLh+R1YKVwLbcfyt3nAoxeJfen+OvXIdSIZNZ3efJWEXUOIXQSs/Ny6S5E0OUdP285I8dyX7WpU9zvqUOjLyXXOrEcv/P0TQgjpNyhgCSFkf6R6EnDg57PPh6tF9eELHnS/91uvWlvm+IqchW3pCGDCibnXMUSIRUs15Cln5HZg1aJQkqqJpoCV25Lh1jE7lgMANOg4dazVgR3uFyJyfBkwZ7yYf864Knz/GDOvd9YQq4PYrJdgXGkac+ofQGl0B56867f4x0KbgAXQtPJ1vL2uEZ9uaxInfMa7y4TsdiOZchB3hhDq6CowFFWKrXi3Gb7a3eQ+vm6JuZ+ImGKrq1Hklzatd75PDRtOFyJgjTE7ljvPKZ3bnoZKZxzYfhKwTlWFczmwQOHuOSGEkH6DApYQQgYaB30BKKlxvz7qUKBKqTjsd2jXc/ly4Icf56/sXDXR3J76G+EM+wLWasXTbTmcsuCVSuVY09GUW48HGG3k3I47xjq+01oECt1GL9pYO4aVhfD4JUfivm/MgSdtipGDK62hqt6SGswe5kNXzSEAgKM8yzF7hK0XL4AyrRuPf1CHl5dvQQoefLStA42dMbQmRX7li8s2YtLPnkNbtzWEVgrXzo5OYOcqUTirkJDUSAvQ3Wx8LhcBu/Re4M6TzONEBEgoAvaeM4Bb5zrfqwq0VAFViKUj+dz/AM9cnn29p06zRL4Le3/dPUm+KtROn1l9P8mo+Hn8/QT3LwQIIYT0KxSwhBAyGAkoubBSMALAcVcD448V4bu+bDGXxZg5otBT1QRg3mWiOi9g7X/7+TuAK5RKxbVzgS/eDxzxXfOc1684sMp6vvE88PNG4NCvWZ/buct2vENsDefy0LFVKNr0CvDgFzNDamw5sOVDRsAT70QYQsRMLYni63Nsri+AMnTj2eU74EcScd2Hc295G8ff8Dq+9dAqAMBTi0Ue7eMf1GFXRxRdsSR0XceG7UJ8ppNR6M//BHj1N8CvKoA3/yDOp3Wk0g6CtmsX0L5N7LsJ2FeusR4nIqbDWL9MbN1ySy19YA1XWtfd+56q47saRXUYYwkAACAASURBVGsadWyilw5sWnFgE1HgP1cAnQ09myMf6pqcwoWdHFaL6I0CTRuA7R+Y75UQQshehVWICSFkMKKKVrUQ1Ak/6dk8Mz4HHHA24LX950Q6t5pHCOFSWz/c6ecAE44HFt2WvSa10JTHC8AretyqSMEqkUIsqojUxy+2jumw3RMeAjSsyYTeTgh1AlW+zPo6Rh2D0EtX47RJRbhvoxcjw16kogF8bd44PLa0DruiHiAAFGlC8Pz6mVW45j+roOtAeZEfM+PLcL8fCOgxtHkqILNxuxfejL92nol1uzqxeGMzfnbmAbhgTq25rl2rzfBa6cSqxLuBblvv1KQSQiydWK/LFxBODuzdpwFbF7mMVwTdrlXAI18VP78v3m88zxCzPXZgZQ5sHPj0WeD9O8Vn+Nxtue/rCW4OrOYVz8/VRgcwcouNMQwnJoSQfQIKWEIIGYxIh3TMEaIlTG/RtGzxKrlogTVUOWsNRp7sGCPU1cmBlZQMLWw9ausTezEj6WoCor1KqFwUS4oZBZM6d5li7PifoHToAcA712NmtYalXz0F/mefhXdNMeafPQNnHTwCnU3bgaeB68IPo77idLy9JYKjJlZjVm0F1u7sRHy1EEIhJPDe2m04xdD0H0ZH4vY3N2SWcvW/l+PFVTtxt3GcblyXCY+KdzRAlaHJVBrJ9gZkBX0noqZwlVTU2kcJnNrouIlXObcdtTrznnBgpdiOtPZsjs5dQvged7VzuLubA+vxAamUS1hxzHpPph1SHwnYLe8BLZuAmRf0zfyEEDLAoIAlhJDBiBSLh10MzPxS3zxjwnG5r3s8wCXvmkLLXoVYJewiYAMl1n6tLRuBtjrnNkLt20VubjolrgfLhGMb7xRFrRJdIkQWMAs0hcqBaBtCfq8Qe0YBp8PGVQFjSoGnAV+8DbccHcP26mMwfWSZKHY0bDaWvbgceBcIe5IY7Y9gS3A2dsV9GKu14MEvHIEFy7bj60eNx2duXIhXP9kFqUpffXcxTgbQqJehePty/POplzBtxizMm1SNp26+ArNaX8QE+2dLdAtnduRsEe4KWPNtd60Wn6lyHJBSilmlXFrjqMQdilClk0A6LX6GUvT3tB2OrlQhllEA9t67+Xjq+8DaF4GJJwG1R2Rfd3NgPT4jdNnJgU2IL1fiHYYDK9sh9aKfbyHcfZrYUsASQkhBMAeWEEIGI7KC8fAD9+46hk0HgoYb7PULZzTg0KtWtgdSmXkhcOkS67m694E/zwD+dED2+HRSzOMPCSEaKDHEmQ4MMYpRNW8UWymiDQELwOoUyvV+7z0AQMWK+zB96S+EeL3taOCT/2DmcCHKPHoCB5RGUTt6NA6dMg6jQ3HMm1SN6847GFOHl+LlHx+HBZcehYTxnXJ1YjsAIBaoQjGi+MoHF+DLdy7CY29+gPNa7sYEvS7roz3y7hp0dbYhFh4hCnDVzrO60bfOBf46EwCwq1U5n+5h71aVJqOHbm8d2Ez/2LgS/qzk1kbbs4teRVqsx7mqNAM2N9UmYO3nJKmY2f84qYjcrgbz94MQQshegwKWEEIGI9POBP6vwdq/ta859irg83fmHlM63Dlc2Ck8tKhK9Kl1clvdKK4Sbp+9760UsB8/AhRXm61+QuVARz3QutVwCoPW+WS15TXPAUvvARbdLo63f2gVdO3bgeIqaEUVQhDrekZgjq8O4+BR5fBBOJK1HlGgauSU2eIRWhJ+LYkVL9zj+rEWrtqK5pZWPPNJG/70fhT6qNlIxzpw4h9ex/MrzNzfho4YnlpqirBnPtxc0GtzZJcoYtWjKsRdjUDjOuu5ZMx0eaWQ7WwA/jAZWPOCOH79euDpHwLXjwO2LBL3JOOm6+smxO3hwBKP17p2+z2yj3EyYs7x9l+Au07N/xkJIYT0KRSwhBAyWCmkyvCe5MSfAQefn3vM158FjrmisPlkruyZfzROaNbr333b3PcZIcFFlYaADZrOL2D2s+1uBGZ91czrDZULofaXA4U7aC+MJJ06yYf/ENtdq6yCKdENFA8R42PtwEcPAH88wMy/jXdBg3AbqyDEnHbqb4CjRNuaV87owPdmWMN9GzNloYBiTwJFWgwRPYgbX1mLP7+5A55kBJsaO/Ddf5ou9eHXvozVdaZr+ejijfjmfe9b5r0sfikKouFT87MBSMQKyBFdcBlw86HA4jvMc6m48h4MB7ZpnfgCYOcKYM2LwOu/Fa2DAKDxU+ChC0VbHxkCLe/XdatLmoyaP3t7CLG8bicVN7+YSMasY7p2mSJ7T1NIiyVCCCEUsIQQQvYhKmqtzqgTUlzIPNXDvgH8tB4YMdM6Ti0gJQVqUZW4zxe09qqV1wFgzreynwUAdUuzHdhAWFS0tbNzZbY4KqoSAlZPAyufEK5jWx2w6W2gdUv2HKXDgQM+CwCoffX7GLrmQcvlyppRmf1rPjMBlb44zp83DWfPHIniUiGsf//Z8ajymGG5FcV++LVU5njCkBAWrraGJD+ddujTq9Cqh7FFr0HHlo8sIbbrtjfikfcdPgeA9mgCt7y6FvqGN8SJDa+bF5MxUxTGjHDtNmNNHTuAxX+3TpaMAc0bxB/pvMrc2aX3ADceAmxbao4tMoS+JYfVEIv5HNhEJNtZtrdwUtm1Glj3svv1XKR2I5ybEEIGERSwhBBC9g8qx4mtFJtqu51AsbXKcMlway6tFLPFVUKE+oLWvNqqCcLJvewDoHy0eT6kCNhYW7YDq2mm4B59OABNOH6tm0XOpIp0YAFg40KxbasD7v0McOdJ1rHBMhHmqgprABg6PbPrDZi1iIN6FN5UFMGiUtx44Sx899RDAABfWHMVnpyxMDPuxcuPxVkzqjPH8z8zBf/4ijVfeGipTaTbCJYNwWZ9BEo3PIvU3Z/JCPUAEvj5kytR3xbBim1t+N4DS9HYKcTf75//BA+/tBCarJSsvptUzBSg0TYhGNu2iuOOeuG4jpxlju9qFII31mmGEMt837UviW2b8buQjIo+xYDVgZX3ubXRyeTARpFVfbhzl1iDk2O68I/AU5dlny+EnuYQE0LIIIUClhBCyP7BdxYCP/4EqJ4ijj025/Pk+Yao1YQgVakcK7ahciFogmXAUEW4BUqAw79p5sJK7K6YU29VKXJHzhYh0Kf9Rhw32XI9i6tMYSR7q+5cLrayeJHsdyvHFZlhwgCAUbOd1yL7xcqwatkmafPbqF17f2bY0JIAjpmghD2nE5gzwtqQ4OUrclePLiqrgVY2QixhuxmeXBVMQ4eOXy1Yhase+xjPLt+BHz3yEVJpHRu3bsP9/usAAHV6DdJtypcNiSgiHWZxpu/87Rno0oFt3ijyj6ecLtoyeQNC/MY7hWiVIcSqAFbfTVIpyJSIAl1NwCu/Nsc5tcaxhBBHsx3Y+o+AGyYCb/05+97uJiBi69279F7gg39kj7VDAUsIIQVBAUsIIWT/IFQmijZVG65kyybr9VlfBX62Hag9MruliizKlIgAZ/4BOOvPQOkI87pb2LLq6gLZIcSAVWyOnQfUTBPH9rDgoqrsnNn6j63HsievRxGV33kTmHSK2K8Ya54/9kpzv9to/yNdZzX0WSXeaW13k0qaYbsGZSG/872SokrUnPsbNGsVSOrm/0aU+VP4/gmT8PzKHVhV344zDx6BhWsb8eU73sO4nS9hvGcnuvUg3ksfIIpaSRb+AUUrH8ocdtWvxbIVK8TBrpUQVaInibZMleORbN9htA3qNL8IkDmw0XbzcwJCFAaKRXXrZAS4cZZwSSV2B1bXHRxYm7DcYXzp8O7N2e8m0iLGx5Vqyk//EFhQQF4xBSwhhBQEBSwhhJD9i5lfFpWCZ17ofP0bzwk3FgAuXSqcu9Lh4lhPC3d2yEQR/itxE3zzfgiUjgTmfNu43yFsNCgFrBGSLHvWtm61jqsabwojf1j8qV9mHTP8YLGNtprnRswEaqaKfU0DJp0s8n4nnQzMbxNrbzGqCUuhrhaoUom1WwVsOmG6kSpfeVz0VnWiqApTJ01B6eFfhk8zWuGEKuBNxfHDkyZj/men44VTW3Dzti/i0iNrsKGxCyeWb0eXFsZBsTvR5qmEB+msaet1sfYJ2nYEu7dbrv3fWzHMX7ASK1r9WL9GVD/Wo+3QjbUv31iHv768Fl3twv1saWnEf921CE1t7aJol79IuNQ2sZ4lYOW7CRSL6taJaHb/16b1YuvUwkc64bLdT2dD9hgV9fepr/rMEkLIAMOXfwghhBCyD1E+CrhqfWFjqyeJP8m4COmdZ8tPLBkOdO5wF3xjDgeuWA2s+LcoJuRUbEmGEMtcyxIjDDjeYR0XrhYCEgBGHypcSHuY8QhDwNr7nU49Qzh+tfOAY//Hes1fBNQZobyyLZKbo9yxwxoWnUqYrqXK5JNFv9z1r1jPl9dmcpH9RcozwtVA6xZomoavHTUemC9ycK+creHKCa3Ak68gWXs4Fpx2HIoXLwc+yn5k0fDJ6Nr5MU4d2oGxbc1I6n74dLHWxzeHENm8CYf7i3GMZz2gwcynBbB8w3b8ec0afC3YAmjAPa+twOj4Ggzxr0FrfArKvEHsrNuEEfaH2lzPaLQbIQDwBoXwdXJgG9dkdvVIq2iNJIkYXzxEmsXv6fYPsj+oivplglM4MyGEkCwoYAkhhAx8fAHgpF9kn//WK8CW95xDg1WkA2oPWwZM91Y6sKEKkYOpihOJHDNmLrD5nWwBO+EE5+ePOxr4yTZnoe0LAcmdQnRVGTm8ARdBfudJ1uJV79wkQq8BYPq5wLSzzGte5Z0c/SPg3VuBb71qrkEtklVcLT5LOg20KG1sWjcDj18sllkzFTNGlgO1YxwFbEVxCBg5DUf76oGWLmD8scDGN9E0+zLce+DxOHh0BTbc9wTKti3OuvfYcUX44ZhxKH1HiEBvohO/8z8BAHhlXRuO8AItnZsxwhZ3pkc7sHhDE755/xJUhQNINm3G2yGg2xNGsSFgE/EILEHVMlwbwPdvfBR//NHFKNLiSLzxJ/ilwxtpga7r0LblEbBqS55C+ugSQgihgCWEEDKIKR8NHPSF/ONkNeCEQw9Qe8ElTRPFmNq3mUJWFmcqqgQueAgYe6RoA6My59ui0BMAzPhc9nPcXGLZTmjoNLN/rXRgx8wFOndaRaXq7jatBV7+pdj/7F+tRaO8imw7eb4Zli1RBay87z8/AsYrRaDWKQ7uAYY4lmHOdqKtQPVUYPmj4vigLwKfvwNDSodjiDFk+qQJwLbsW0cXp/CjuRXAuyIk99wDyoC14lpM9yOSDmBSUSdg04jx+pX41YKV6IgmEUukcaAm3s33nt6BP4Y8+HjlVlR5Y7A1aMrQ3daAXy5YgdO8S3DSst9nzm+pq8MZ97yA18Yuw1B5MpWwvtP6j4HbjzGPnSoiE0IIyYIClhBCCMmHLwicei0wZk72tZDNgQWAkqFCwEohO/si89q0z4jt2COBFY+J/YsWAGOPEvs/2SZc1UKRwmfYgea54irgi/8Qzq3XD2z/ELhP9JRF9VSRj7vmeXO8N5gdduxUcVlFdXkrjTZFS+81xLoGQAfWPCfO/+BDszJ0sYuAjbRYq0CXDjdzlw20kqHIIlAiijgpwnxs0Pyi4fy5kxDbuAtFTSsst230jsf45Ea071yPX372eJxx4Ah0ftgEvA4UVY1GZ4cX8Wg3GhKdgEOrXwAY7u/Gw0vqUO1dhJMUbfq355egK3USWrauygjY7975GtZ0BFBR7MdfL5iF9Y/dguPVyZIxRBMpeD0aWrsTqMnTzogQQgYrLOJECCGEFMK8S50FrBSuqrMow4rHHAFcvhw48efZ9x1wtrk/4TjFPS0x9wtBFg46/JvW89PPFmIxWGq2HgKASxcD599rHTvl1Oy2RPnCqlUHds63gMO/JfbXvyrEakWtEJXBMqBinDlWFfoqkTaz1y8gvgSwU5qVxQpUTxb5xjEll1e24QHgRwol4eyc4NEHHQsAuOsUH742bxyGl4cwKSTylv92yVkYM7QKc2vDCMLMGW7Xiy1zfHdOJX752ek4stxa0KkSndCQRq1ej2ZdCH092oapw0v/v737DpOzKv8//j4zs7O9b/qmQhLSKwQMgRhaMNJRQJoYQETqDxAU+SpgBRVFEekioBgBaSItBAgIpJOEJJBed5PNZnuZ3Zk5vz/OM5nZkgbZ7G74vK5rr6fOM2dmn0z2nvuc+7B0SyWT7prFuq1Np9tZur6Ykbe/zkn3vMvhP3+TO15a1vK1ioiIAlgREZEvZOQ5cMYD8eJNACWfuuW4i10gl1jxOCajK4w8t/Wxufvi/H/BRS80nSO2ueZVln3NAuSBJ7V8jH8P0+kkJQRzafkwxMvwbp7v5tiNBZs9R4Mv4c+NWKCfVdj0eiPObhbANs2+Ai0ysoDL/oaq49WUjb9JAEvNtlYz2kmFY8CXxODGFZjY76eqyGWe0/LwJaWQHQgzpmfqzseEsrz2ZfYA46NfaohLjujBpJymwei0gSm8/73DSDUNbMt0hbUe+Mah3H/BOP4+LYUr+2/lq32avr8PzVpOQzjKmu0ue7xwY7NCXiIiAiiAFRER+WLSC2DUuU33TfutG8OZOB60NWc+AJNu+GLP3/coGDB59+ckpboKxmc+5LYTA9hT7oXR32r5mH3pQhzMaNr9t+vQeBA/+oKmj8voCsf9BC75T3zfJa/C1+5uGsCmF7R8ztYysMmZsGO1y/yCq/5bmRDAVpfExwk3b/8hU+CTf0M04vZVFbsg2RgIpEI4RIYvnoHt0neIW0nNdcW6akvhkeNbVBselhOhZ8QN1j1snFeYq74SolHGv3Y6Pyi6nr5ma5PHnDgom2euOIq3x/+PlakXU1sfbtlmERFRACsiIrLfDfk6nPVQ65nX9mCMmx935Dfj2zHjLm7ZfRj2IoD1uhAHUl2X58ye8WPDzoCjrvLWT2/Zlkn/r2mwml3o2pDRLb6vtTalt9KtuPsIt5zzoFs2z+wmZ8YzsEnp8cyxP+C+eKjcDOtmu32VW+KvI5DsxhcnVgfO7u1dM8t1z67eCsVLmj5fbn9XoGvDh4CJdzsPVcLch+Pnla5s8rBplf9kfNpW+i39E0m2keyGVqpViYiIAlgRERFpxd4GsLHqyIndhLsNhWNvgv8r2/NYWmhawXm3bWplbPARl8GwM+Pb2b3i68feDKffH8/mJmeCz+u66w+6+XWT0mD5y2Ctm/Yn1k05yWVgm8wDm+UFtynZrit0bJqcab+FW4vhslkw6jzY8D949243tjmWmV72Ivw3YQ7fWJfnmB1r4M9H7qxqPSi0fPfvhYjIl5QCWBERkS+rXc0XC3vfhTjxGtd/AjcmzG3r28s/MxKvcdIv3c++iAWWgRQ3Hjfm8Mvc2OTeE9x2dXE8CPYluSB1wGRXkXnVm1C+AQ49zrtWMoTrdh3ApuVD1Ra33WO0u1avsfEsdzQM474dn2Zpkzd/7dmP7v61eIHt0MiKfXkHRES+NNp0Gh1jzFTgD7gC9A9ba3/V7Pj/Ay4FwkAJ8B1r7fq2bJOIiIgA33kNcvru+nisiNPo81s/3jwDC64r8OeRmHk96sq9e8zE61ylZYh3PTb+ptMBxYLHvl+J74sF5rHXN/hk+PQVeOk610V4pDeeOZjhgslo2BsPWwe9xrluyKm5TYtcxaYIAjdF0TlPemNsvxofX1u2zgXNhYfHz538I3j7F62+vEGsIxq1+HwdpBt6a5rPbZto6zJXwGxX8xeLiHxObZaBNcb4gfuAk4GhwHnGmKHNTlsIjLfWjgSeAe5CRERE2l6fIyGrlaJIMcbADzfBqX9s/XhSKmAg2HKKmr02+YfQ56h9e8xXb4VDT4ATbncBJcSn3AnXxcfEAgS8YDWWOU1KT+hC7C2HnenN17sJJl4bf0zBIKgpcVMBHT4dbit11znnCTelUmw6oNh42ERDTnHBK7ixvLEq0JndISuxi/MP4KfNuhJ7ulJGXWNkH96Y3WioddWh96cVr8CdBbCtla7OkUa4/yj45wUtj4mIfEFt2YX4CGCVtXaNtbYBeBo4LfEEa+0sa22tt/kh8Dm/uhUREZH9Ljmz9WJK4ALcYMYXy7BNvgW+8+q+PebYH8AFzzTdFwtgbXTXlZ+vXgBXz4sHrrFMbHIGnPgz6H0kjLkwfn7XhO/cAynxrseHHucyi4OmQv6hLYtUtSY2Djazh3s/h50Jx/+05ZjfdDcVU8Qk0cVUUBNqbHp8+0q4I98t98XL18FDU6Bq657P3VsrXnbLDR+2PBabm3jNrP33fPuibD089U0IVbXP84tIm2rLALYXsDFhe5O3b1emA/9tw/aIiIjI/hRM3/042gMlsXpxrCBUc/mHuAyqv1kGFlw14umvQVLCfLFdD4uvdx/e8nr9J8HV83edoU4UC4ZjGe9vPAZHX9/yPK/7c0X2YFJNA/U1zbKzS591XZo/fnrPz5loyyK3rCrat8ftTqw4V+IY4Zi6HS337S/RiDf1UXTX56x7D1a+BsVL264dItJuOkQRJ2PMBcB44O5dHL/cGDPPGDOvpKTkwDZOREREWjfxmtbnkD3QEgNYcNnW785u/dxdjdlsLrGr76Cpn69dMV29+WP9e6jIHHDz1dbkuvMbypoFnLG2Rxri+8INMOchuGfEroO62LjgqiJ49jJYvR8zo60FxbX7KYCd9QtY2+z3uOgp+Ne3Yf5ju35c7Xa3rNHfjCIHo7Ys4rQZ6J2wXejta8IYczxwK3CstTbU/DiAtfZB4EGA8ePH2/3fVBEREdlnR32/vVvgpDYbgxrrstua2BjYSHj31zQGxn/HjXVNSv1i7cvp45b15a0fT85y8+JmF8K2T2jMcsW1IlVFwJj4ebGCULEAdsUr8PR58ePrZrupe1pc3wtgty6FJTPczyX/hfXvwzE3uetu/Khpsas9qfGCxIpNLY99kQystd58vN3dVERVRW4u3VpvDHKlFzC3NvZ2Z9tKmi5F5KDSlhnYucBAY0x/Y0wQOBd4MfEEY8wY4AHgVGvttjZsi4iIiBysfD445gdw0Yt7Pjc29jXauPvzAL5+Dxz3f1+sbRCfxmfsxa0fv3k9XP42nPonOOFOavsd75pY2exPo1jQWLPdBZ0zLmx6/OOnYc07cHueCwJjYt19Y/PWAvx1Grz1M1dpecHj8NjJrmvu2nfht4fByjfi5658E6qbBYOx4LB8Y9P9RYth0T/i22EvNxFphOe/Dw8fH38drVnyDPxhJBQvdmOaq7fBB/fBH0a59ld4z1fqTddUXwnPXR4PbJu8TyUw4yKYt4epi0SkU2mzANZaGwauAl4DlgMzrLWfGGPuMMZ4de+5G8gA/mWMWWSM2Yv/eURERESamXJr69nH5k7/s5sqJ3E6m7aW2d1VGx7y9daP+3yuuFN6Pky8hkC2N1a2urjpebGusZVboHy9Gw+b6LNX4bUfgY3Axjnx/aFqt0ysRBwbu7x5AZSudutLn3VBZlURLH/J7duyCJ46C165If7Y126FDR+49YpmAewDk+DT/8S3q7zXUPIpLHoSNs2FT3dT8mTdbPe6YgF0VTGUrXVZ5xkXx1/X5gWuy/S8R2DxP+Gjv8SvEQuuF8+AZS/Ay62MNxaRTqtN54G11r4CvNJs3/8lrB/fls8vIiIi0kT+IXDmA+3dit1KySygwfrx1WyDxnqXGc0bEA/MKjdDyWdNH5Q/EEpXxrvvbl0KWJepjY1TrU6oQpxeAKFK2Dwvns2MBa0Qr3T8rleepGKz+9n+GXzwp/h5VUUu47roKZhwRcsXs3EO5PZtmhFe9SaMvdBlf9+5C8Ze5ALU7iNgi5clXv2W1+ZtrnpyWoF7rljmPFTh2hYLXH3en7TRSPx92uEF5hndW7ZLRDqtNg1gRURERGTfpCUHKCGHQG2Jy3wufNJNwRObe7aqCEq8MaB9j4b177kg8I3b4hd5t9W6mHE71rjlWz9reSw5C7Z94ubA/cyb5qh8PdwztOl5R3zXZWJfuQkaqlz2tLnnLnVz4lZ6ZVAOmeIC2E3zYesSFwzPech17b56Pmxd5s6LZXhrtrnX23MMDD8Lnr/CZdAXPw1v/yL+PJWb3bjmOwuAZuVSqovd65x0Y9NK0yLSKXWIKsQiIiIi4qQG/ZTYbPLLFsXHk1YVwbZlYHwuW/nx0y6o7THSHe82FM57Gi58Hrp4UwANnrb7J0rvAtlevc1DpsT3HzbNZUfnPuy68w4/q/WCSOkFUDjeBa+tOeVetyxb5zKwxgdTfw1p+fD3b7jxrgCRkLvGS9e47s/JWfFrRMNQssJVmh59HlyzCKb+suVzffwPuDOfFsFr34lu+e7d8WBcJCbSCFsWtncrZB8pgBURERHpQNKCAUpsLjl1G1xA960Z8YOHX+am4ylZ4QLV3H5uf3ZvGHyyy3bGprEZ/S3oOsyt929lfPDws+GK2TDsDDjuJ/H9g7/mlh/+xXW/HXNhy8eecAd85RpI79p0/+GXxdd7jnbLyi1QtcUFoV0GwXn/gLpyl7GNFdUqGOQCzEBKy67IkQbI8J4nrz+kJVSdvuI9GHlOy/bFjDo3vt7anLXy5fb6bfDg5HiPBOkUFMCKiIiIdCB+n6HU5AAQxce18/LYEXSFnWaVF/BRv++yJXME/8iezjPhr/DR0Ft5cXMGz8zfxPS/zuUTBgDwq+X5lFg3hU559yN3Xj9k3QiyD4oibKxL5q0Rv2aFb8DO4y+Ve9P+1G4n0nUYRbnj+G/3K7go/OOd55QMOIM6m8T6hvSd+3YMOofLtkyjwQSJ9JnIzE1+d2DGhbDwSSqTCqioa8R2HQrnPgUDT6Tq7H/ywbFPUTv5p+7cQ4/HDj+zxXsSbT7Xb0yXIfHphRIUJxW6laGnxXdWa8ILaWbtO26pe6NT0RhYERERkQ6mZ6++UAQlNpv5G6s4qfZOelHCksWFRPDjM0di75yj/wAAGnZJREFUt4O1G4BhsGARAL1yUrmR71PgO42P5pcz0hfla3749bvbmeQ7gjWmDxeaV0gmzKurQzx+16ydzzknOYeuppyrX9zEsclpZJlanlgV5Pa7Z2PtMWRSu/Mvxwl/+JjU4DKODpfygJdE/criqdRTyxAeIWVdgNrP1rIyJUAAVy152/ZSjr/9dXrlpDL96MNYFvwxbz+7je3VBkOUX2ecwgsrJrBw2TqWNUuxXPdyEWbdQpL8PjaU1nLq0AcIbVvNqheWcWJNP77qnVeR3INTqn/ExvpcLD4G3/8xXQtf4+FNp9BQuoXMNvydSTuxFmbe7noS9Bjl5iY2Pjdv8J7E5lSuKtr9edKhKIAVERER6WCOGTsc/gNd8/N475op1ITCBPyGmpDLNlprCfh81DVG8BkoqXbzrQ7pnoXPZwAor23AvPQ8LJ/DaaN6sLTHVZw+vDvpD78HNbVcctxo+gSH0icvjepQI09veY78dD/XNKQSWDkIShZR0Hc41/UdxOTBXfhobSl4xYFvnDqE91dt54L+42E2hAPpTBrah5tOGsw7n5bw6ifFbC6royHkJ2BcAJvXdzg39h/Ekx9u4I6Xl5GXHiQ1yc93jx1AZnKAf312JYW5aZxsDN9a9kuy0pL5c+0N+LAMHtCf3y0uIjng45AuGfx4QSYFGRMo27KJp6Oj6cYfqSQd6qGWFFKSfNQ3RslLDzJ7VSklyVmY7Qpgd2v9/+Bvp7lxxtm99v5xK99wY5DPegSM2f/tioThgz/CkFNdFfHmyjfAe/e48eLnPAmPeJOc/LRiL67tBbCVCmA7EwWwIiIiIh2N12XWJKUCkJ7s/mRLDvibnJZNEgBds1pW181JC0KuGzt6ZGEKR37F6yYcTIUa6FdYyPRB/eMPGFMYX68+DEoW8fUpx0D/gQCM6p2zM4C9cvKhXDn5UDeH7GwIZHbhoYvGAzCoWyaXHTMAay3mdhdYc9IvyRv9La5KzeHbE/uzrbKe/gXpmISA56opA3euWzvSHSs5GmbewffPOIvjyiw5qUG6Z6cQiVp8BspqG6moa+SD1aWcOronBli4oZzhvbJYV1rL6N45lFSF2Hx3Fl1rOlE30fpKqC11Y35bs2UhzH0ETvmDm0N4T7atcHPiTrx215WYZ//OBXRr33UFs/bWU2e75cl3ucJe+9uG/8GbP3U/N6+H1Jymx4sXu2V1cTx4BTdPcCTk3sfUPPjXxa4gWeK46HovyK1KmOZJOjwFsCIiIiIdTeyP9EDyF7vO0de77pFjLojvS0pzy5TsXT+uyyC3LBjcdP/5z0Jiki02NjW9S4tLJAanDDt952vKSA6Q0SVjt83e+dgug914WeCwhOlc/V6WOS89SF56kP4F8bG4Rw90QdToNNe3OT89yGJy6FFXuusnXPmGC3I+ut+9PxOvjWf7ij6G5Ew3F++B8tadsPRZuGl161nNZS/CwifgmJtcIaxnL4VT7209Q1lXBn+e4Na7DXPXS8uH7iNdcJfjVaJuqHbLsnVNHz/zThcknv0ovPpDmHIbZHaDhhp4PT4umtLV7l575ES4dCY01rp7rCD+xQS1OyCYHp9neMm/XNGwio1uvLLffSFD5RZY+JQrUjb7t/HHFy+B/pOatq/o49bfw83z4Mmz3es67T5Y+br7iUYgu9B1N44FsMrAdioKYEVEREQ6mlwv8zb24i92nbQ8OOvhpvvS891yd5m78dOh2wgXqCQaeHzT7eQMF/C1EsACMOQUWP6Sm/Knnfh8hppALikN692O9R+44Gb1LDjjAdix2mURAynxSsWlq+CSVyBUBQ8c4wKxWzY0vfCGD1016DEX7vq9bKh1c9oeMgWe/x4MOsmN1VzwhAssKzbB+Eug6xDY+okL0Aae6K5dW+q6x+b2bXndcq8tm+a6IHD9e245+RbYOMe1a/hZLljcOCf+uJLlzeb+NXDDCldNOjYH79alrt3+oHtdC5+EUCWsmumC5u4jYcLl8P4fYN6j8UuVrnJZ0MZal+1951fgC8CtxbB5PhQeDnclZJR9SRBthA/uc8ua7TDym5CUCm/e7ub6ba5iY9PtULX7fbbm5esh5AWor3iBfjQCL1zp9k1/I35uVXHr15AOSQGsiIiISEeT3Qtu3brr7p5fxOl/cZnGnmN2fU5qDgw6ce+u12vcrq911qMuoGmLsZH7IJScT0ZdGXzyvOtKGvPYyVC91a2H62HUeS7TOuvn8NlrUPKpO1ZfAY11MOsXbsqVSTfAoye5Y411Lns75wGY/KOmQf7rP4Z5j8DXfuPGiX78DxewbZobP2fla3D1AnjyLJfBHHuxm/MXXDC5/n2Xidw4B46+zu2PBbDPTo9fp3qry9o+8x23veZtl2n0BcD43ZcZq99u9s5YKF4KW5fEg72tS+FXvWHQVJfhrfaCu5VewLd5HswsbpoZBfdFQPFSt/7Or9wyGnav+cWr3VRJiaKNLvv66Stu+82fuPer+wj3ZUKisRfBgr+5gB9c1eAXvu8yquCmloqEmj5m61LoORa2LHDzDI+f7t73WJfjD+5zQXT/SS2zztKhGWvtns/qQMaPH2/nzZvX3s0QERERkU7i4ft+yaUlXkYwu7cLBMMNrvhP4vjHsx5xAfm93hy2KdnxbqZdDnOZTYCuQ11wa1tO4UPh4dB7gssmPjgZbHTPDfzWDPj7N1vu73u0y67GfPsVFxyufTe+z/ggOct1ty7f4Krv7ljjMrwxPUZBZk/47L8tn+PYm10wOvhk90XEzDvix475Abx7l1sPZrjuuFmFUF8O/Y6Gk3/tiifNfRhqt3vt8Td9XyZ8z31hkii7j3v8tN/Ac5e7+Yv/c0PLtvWb5H4nqTnw+xEuOz3pBvjnhS5gPvJ70P8YKFsPL10Tf9xRV8EHf4Kv/x7WzHLv1zWLoHIz/OdGN64WYPA018X5g/tcptiv3F5HYYyZb60d3+oxBbAiIiIicjC77d8fM/Xjq5mYtd2Nz8zyujRHo7DiZdfFeeETcNIvXLfojXPgkRPcOYdfBnMfark++gLoOdoVP5r6C5eJff57kD8QSr0xnql5blzqprlw+KUuQOw2DEo+g39f3rSRSekw9ZfxQCw1t2kQCk27Ocdc+Lzrsjv/Mbf9jb+6TPOy5+PnHH+7C8Tf+118X94A1wU32uie59uvuPfl3mbZ9GCmy2CC6y7eWOvWvzXDdYkGmHGxe760fDjply5QXPEfl62NZUFjuo+EU//o3rtEGz5yXyYsf8llkr/xV9fdOuah41yX6LJ1rr3f+Cscepw7tmWh+7Kg1zi3fdlb7guGgkEQaXQBtVcQjcY6+Lk3oPqCZ6G6BJ6/Aq6a13S8rrSr3QWw+ppBRERERA5qXbPSuLD+JgakBug2Yz2wnoDPR356EJ+vL+FIlKKK86h7aBE9s1PJy0hlanASxzTM5jfFoxg47F5qwoY3tg7m3D6ZZNRtYUnqpazf5Cfl0Bn41vrpkplM3fDn+LQ2m+kF/6XfxhdY3O8SinNGkZ21gnUpx7D6s1oKtgRJSRrNeZkj2JBzBMXZYxlQOYdteeNYXjqOowdexZaCo8ks+h/HrLuXUP4QAuVrMdEwJhwiGkjDH67d+do+auhH96QB9AUiSRnMKB9Kz5QqjuV5Pp36d9Y15LCoOp8J2RVMJh7Abk8bQHpqLamb/0eDP505of4MTsonNprZ+gKYaJjqwWeQseRvANRMvoP0N24EoL5wIovX7qBHdgq1R/ycQSf/mhKbw/bqBg4Zlk5yl8Pg6fNc8BobCw1wxewmv5vK+kbe/ayEw/uNolufCTDkNDjiu9D7iKa/xJze8Mm/3fppfybc/6sU7ailV04qvp5jqLp2JeU2g955XpGyLl4BskCw6XWSUqHLEJe5PvR42OwF1yUrOkYAG43AKze67uzN34O99favXEGsSa1ktQ8CysCKiIiIyEFt445aHnh3NdX1YT7bWk0w4CMUjlJe20AkagkGfPTITiE1GGBzWS1bK0OcWlDE1+pf5qbQZRRVhzEGumYmU1rdQFrQT2V9mLz0IHUNESJRS0Mkit9niER3/bd196wUdtQ2EI5E2c1pO/U3RRTbXBoJMNKsIcdUEyTMX4K/Z2W0F/mmgrGhB0khxFd9i/g4eghbcFWYU6mnDjeW1BiwFlIIkUcV7yRfz58jpxIgwvcDL/JoeCp3hC8CoNCUkEwDXU059wT/wvmhW5iZfBMAI+ofJokw3fyVrPP1pa4x3lW4a2YyJdUhrIVgwMfEzGIeq3Njdv9izyDZ78eXFOSZjPNZX1pDatCPwVBaE6Ix4qZF6paVQkM4SnUozLCeWXTLSmFFcRUBn+HCmse4KOIC2Cu6P83KmlRWl9RQkJHMkB6ZfLC6lHDUMrZPDhV1jQzqlklpTQPV9WEGdstg8aYKDHDC0G6sLd5Bn/wMIr4A/52/ig+5CKb8GCbdCOGQG3seDrkgN1YZublZv3CFoY65cc+/yL21eAa8fpsbd5w3AK5Z2PKchlo3tjglK77PWjfW+pAprgDc7V4V84tfgt5HtgziOwF1IRYRERER+RystUSiloi1BP0+GiJRkgN+6hsjpCT5aYxEsRa2VdWTn57M9uoQxZX1jOuTS22jC24DPoPFTSGUKBq1RK2loq6RxoilS6Z7fF1DhNz0IB+tKaWqPkyf/DTqvWCxtiFCz2AtWXndqKoPs6msjq2V9UwYkMeG0loGdsvEbwyfbKkgNeinZ04qffLSWLK5giS/j8ZIlIKyxaw1vaiMBMiIVDFqyGAWbiijuLKe7NQkwhHLtqoQCzaUMaF/HpfOdN2Kf3PUHIyB+sYI9Y1RJgzIY1NZHZkpAT5as4N++Wkc2i2TxRvLqSzfwV2rpgHwQr/beD/jBCrrwtQ0hOmTl0YoHCVq3Ws+ckA+89eVsamslrTkAMkBH++v2s62qhATDy0gHInSy1/O1asvY7O/kNtyfoXPGKYO686CDWWsK61lQv88ctOCvP3ZNtKCfuavL6NffjoN4Sjbq0McPbCATWV1LN5UwSFd0llXWkskakkO+Jjpv4pCsx2yerlCWoO/BmveccHrUVfCxOshXOem5TnyCtdV+vFT3C9x3LfhkONc0awjLndjkLevhDkPui7hy1+CAce6QlzRqOuybS2sesON303JdhWoP7zfdWOPCaS4eW+TUlw36AWPw2GnwHOXusrLx97spsmq2Oi6hz94rBvf7Q+68cExE6+DE26Pb5dvdMWvxn177+YQbicKYEVERERE5PNZPcsVUtpd5erWLHjCdSH+6o/j0zd9EdEoRBr2qjq3tXbnfMLRqMXnM0Sjlsr6RnLSgpRWh4haWF9aw3sP38CluR+TUTjMjfNdN9uNp42GXZXkI690894ueNxdPG+AOxaNQuWmpk/ca5zL3CZWmh54kivq9dadbsxtYy1g3RjpidfCu3fH5+EdfrarxPzmT9z7VlUEi55qOfYZIKcvlK+PF9jalaGnw/AzYckz8WrSV8931c47KAWwIiIiIiIizdQ3Rhj509c5eUR3vjOxP8GAj5QkP6lJfqpDYXrNvpnUJU8CEA1m4WuoBGD9iY8SPuQ4us+7m/S5f8IOOQVTsckVlALCBUMIbF/e5Llsai6RnP4EihZguw3DlG+CUAXhbiOh70T8S57GXr0IX0oWkX9eiP/Tl+MPHnaGK/5lo65I1d+/6QLtmP7Hwtp34tvT34TkTDdf7yf/dhnkmCm37d+uz21AAayIiIiIiEgrzn/4Q95fVdrqMUOUCb4VNNgAH9tD+Ib/HTbaLrwfHQGAnwgjzFoWcwgZyUlManyf03zvcWvjdM5ImUddGM70v88TdiovN4wljJ8pvoUsCY6h0G6hr387L9UOp5EAKQEIBgLkpgfZUlrJtJQlbGxIZxGD8BlDSpKf3rlpGAM9aj/lx/W/4fqGK8jNzmO9v5AUW09OtIzetoi5SeMJ+Hz4fYYelOCvWM/6QH+G1S9kRc6xPPXdo8lN77hjYxXAioiIiIiItKK2wRX32l4VIhyNUhOKUB+OkJEcoLKukcr6MGlBP+nBAPXhCA3hKIW5aYTCEapDYarqw1TXh6kOhUlP9uP3+chKCbB2ew0ZKQHqGiL4fYa8tCBJAR8Bn2Ht9hqCAR/1jREKc9Ow3ljo8tpGahrC9C9IZ0dNA/npyVgsUQsVdY0UlbtMakZKElkpAbJTk1hfWgsGfMZgAIvrNh2ORt347aglNz1IY8SSm5bEtsoQfz5/LD6fadf3fXc0jY6IiIiIiEgr0oIBRvfOae9myF7ytXcDRERERERERPaGAlgRERERERHpFBTAioiIiIiISKegAFZEREREREQ6BQWwIiIiIiIi0ikogBUREREREZFOQQGsiIiIiIiIdAoKYEVERERERKRTUAArIiIiIiIinYICWBEREREREekUFMCKiIiIiIhIp6AAVkRERERERDoFBbAiIiIiIiLSKSiAFRERERERkU5BAayIiIiIiIh0CgpgRUREREREpFNQACsiIiIiIiKdggJYERERERER6RSMtba927BPjDElwPr2bsceFADb27sR8qWn+1A6Ct2L0hHoPpSOQveidAQd/T7sa63t0tqBThfAdgbGmHnW2vHt3Q75ctN9KB2F7kXpCHQfSkehe1E6gs58H6oLsYiIiIiIiHQKCmBFRERERESkU1AA2zYebO8GiKD7UDoO3YvSEeg+lI5C96J0BJ32PtQYWBEREREREekUlIEVERERERGRTkEB7H5kjJlqjPnUGLPKGHNLe7dHDl7GmN7GmFnGmGXGmE+MMdd6+/OMMW8YY1Z6y1xvvzHG3Ovdm4uNMWPb9xXIwcYY4zfGLDTGvOxt9zfGfOTdc/80xgS9/cne9irveL/2bLccPIwxOcaYZ4wxK4wxy40xR+kzUdqDMeZ67//mpcaYfxhjUvSZKAeCMeZRY8w2Y8zShH37/DlojLnYO3+lMebi9ngtu6MAdj8xxviB+4CTgaHAecaYoe3bKjmIhYEbrLVDgSOB73v32y3ATGvtQGCmtw3uvhzo/VwO3H/gmywHuWuB5QnbvwbusdYeCpQB073904Eyb/893nki+8MfgFettYcBo3D3oz4T5YAyxvQCrgHGW2uHA37gXPSZKAfGX4Gpzfbt0+egMSYP+AkwATgC+Eks6O0oFMDuP0cAq6y1a6y1DcDTwGnt3CY5SFlri6y1C7z1Ktwfar1w99zj3mmPA6d766cBf7POh0COMabHAW62HKSMMYXANOBhb9sAU4BnvFOa34uxe/QZ4DjvfJHPzRiTDRwDPAJgrW2w1pajz0RpHwEg1RgTANKAIvSZKAeAtfZdYEez3fv6OXgS8Ia1doe1tgx4g5ZBcbtSALv/9AI2Jmxv8vaJtCmvu9EY4COgm7W2yDtUDHTz1nV/Slv6PfADIOpt5wPl1tqwt514v+28F73jFd75Il9Ef6AEeMzryv6wMSYdfSbKAWat3Qz8BtiAC1wrgPnoM1Haz75+Dnb4z0cFsCKdmDEmA3gWuM5aW5l4zLoS4yozLm3KGPN1YJu1dn57t0W+1ALAWOB+a+0YoIZ4NzlAn4lyYHhdLU/DfanSE0ing2Wv5MvrYPkcVAC7/2wGeidsF3r7RNqEMSYJF7w+Za19ztu9NdYNzltu8/br/pS2MhE41RizDjd0YgpuLGKO130Omt5vO+9F73g2UHogGywHpU3AJmvtR972M7iAVp+JcqAdD6y11pZYaxuB53Cfk/pMlPayr5+DHf7zUQHs/jMXGOhVmQviBuy/2M5tkoOUNz7mEWC5tfZ3CYdeBGLV4i4GXkjYf5FXce5IoCKhO4nI52at/aG1ttBa2w/3ufeWtfZ8YBZwtnda83sxdo+e7Z3f6b8NlvZlrS0GNhpjBnu7jgOWoc9EOfA2AEcaY9K8/6tj96I+E6W97Ovn4GvAicaYXK9HwYnevg7D6N/I/mOM+RpuLJgfeNRa+/N2bpIcpIwxRwOzgSXExx3+CDcOdgbQB1gPfNNau8P7T/RPuG5MtcAl1tp5B7zhclAzxkwGbrTWft0YMwCXkc0DFgIXWGtDxpgU4AncuO0dwLnW2jXt1WY5eBhjRuMKiQWBNcAluC/q9ZkoB5Qx5nbgHNyMAQuBS3FjCPWZKG3KGPMPYDJQAGzFVRN+nn38HDTGfAf3dyXAz621jx3I17EnCmBFRERERESkU1AXYhEREREREekUFMCKiIiIiIhIp6AAVkRERERERDoFBbAiIiIiIiLSKSiAFRERERERkU5BAayIiEgbMsZEjDGLEn5u2Y/X7meMWbq/riciItLRBdq7ASIiIge5Omvt6PZuhIiIyMFAGVgREZF2YIxZZ4y5yxizxBgzxxhzqLe/nzHmLWPMYmPMTGNMH29/N2PMv40xH3s/X/Eu5TfGPGSM+cQY87oxJtU7/xpjzDLvOk+308sUERHZrxTAioiItK3UZl2Iz0k4VmGtHQH8Cfi9t++PwOPW2pHAU8C93v57gXestaOAscAn3v6BwH3W2mFAOXCWt/8WYIx3nSva6sWJiIgcSMZa295tEBEROWgZY6qttRmt7F8HTLHWrjHGJAHF1tp8Y8x2oIe1ttHbX2StLTDGlACF1tpQwjX6AW9Yawd62zcDSdbanxljXgWqgeeB56211W38UkVERNqcMrAiIiLtx+5ifV+EEtYjxOtbTAPuw2Vr5xpjVPdCREQ6PQWwIiIi7eechOUH3vr/gHO99fOB2d76TOB7AMYYvzEme1cXNcb4gN7W2lnAzUA20CILLCIi0tno21gREZG2lWqMWZSw/aq1NjaVTq4xZjEui3qet+9q4DFjzE1ACXCJt/9a4EFjzHRcpvV7QNEuntMPPOkFuQa411pbvt9ekYiISDvRGFgREZF24I2BHW+t3d7ebREREeks1IVYREREREREOgVlYEVERERERKRTUAZWREREREREOgUFsCIiIiIiItIpKIAVERERERGRTkEBrIiIiIiIiHQKCmBFRERERESkU1AAKyIiIiIiIp3C/weiP5120pCNqgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 1152x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_P_nixcOmyM"
      },
      "source": [
        "model.save('/content/drive/MyDrive/ColabNotebooks/{size}mm_file/outfile{fnum}/Model_shuff{shuff}_bch{batchsize}__{rscore:.2f}.h5'.format(size = gsize, fnum = filenum, shuff = str(shuffle), batchsize = batch, rscore = score))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-kZ2LZl7zEb"
      },
      "source": [
        "# with open('/content/drive/MyDrive/ColabNotebooks/{size}mm_file/outfile{fnum}/Model_shuff{shuff}_bch{batchsize}_{rscore:.2f}.pickle'.format(size = gsize, fnum = filenum, shuff = str(shuffle), batchsize = batch, rscore = score), 'rb') as file_pi:\n",
        "#   history=pickle.load(file_pi)\n",
        "\n",
        "# # history['val_loss']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0x9Gif4yy4zx"
      },
      "source": [
        "from sklearn.metrics import r2_score\n",
        "# print(y_test.shape , y_pred.shape)\n",
        "# print(y_test)\n",
        "# print(((y_pred - y_test)/y_test)*100)\n",
        "# plt.figure(figsize=(160,80))\n",
        "# plt.plot(y_test, label=\"True label\")\n",
        "# plt.plot(y_pred, label=\"Pred label\")\n",
        "# plt.title(\"True vs Pred\")\n",
        "# plt.legend(loc='best')\n",
        "# # plt.show()\n",
        "\n",
        "# y_true = y_test[:2000]\n",
        "# y_pred = y_pred[:2000]\n",
        "\n",
        "# plt.figure(figsize=(16,8))\n",
        "# plt.plot(y_true, marker='o', color='red')\n",
        "# plt.plot(y_pred, marker='*', color='blue')\n",
        "# axes[0].plot(y_true, marker='o', color='red')\n",
        "# axes[1].plot(y_pred, marker='*', color='blue')\n",
        "# plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFZkcH9gw0SM"
      },
      "source": [
        "# fig, axes = plt.subplots(2, 1, figsize=(16,8))\n",
        "# axes[0].plot(y_true, marker='o', color='red')\n",
        "# axes[1].plot(y_pred, marker='*', color='blue')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nP83oM6mgbPU"
      },
      "source": [
        "# from keras.models import load_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XR1W_2gg18i"
      },
      "source": [
        "# new_model = load_model(\"/content/drive/MyDrive/ColabNotebooks/saved_model/model_20mm_4batch.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUtIxqb6hA9D"
      },
      "source": [
        "# y_pred2 = new_model.predict(x_test, verbose=1)\n",
        "# from sklearn.metrics import mean_squared_error\n",
        "# score = r2_score(y_test, y_pred2)\n",
        "# print(\"RMSE:\", mean_squared_error(y_test, y_pred2, squared=False))\n",
        "# print(\"MSE:\", mean_squared_error(y_test, y_pred2))\n",
        "# print(\"Rsqr:\",score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2ngL6CyhP6v"
      },
      "source": [
        "# score = r2_score(y_test, y_pred2)\n",
        "# print(score)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIiBU6GCaUEo"
      },
      "source": [
        "##----------------- No run needed below this line------------------------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4_ZMyIZrh5l"
      },
      "source": [
        "# score = r2_score(y_test, y_pred)\n",
        "# score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPDrx0iFaGjb"
      },
      "source": [
        "# import h5py\n",
        "# f = h5py.File('best_checkpoint.hdf5','r')   #打开h5文件  # 可以查看所有的主键  \n",
        "# for key in f.keys():      \n",
        "#   print(f[key].name)\n",
        "#   # print(f[key].value) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zx3rJnDZKWv"
      },
      "source": [
        "## 5 fold validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NuSVvd5UZTM7"
      },
      "source": [
        "# # input_x,lable_1d = create_dataset(input_x, lable)\n",
        "# # lable = np.array(lable)\n",
        "# fold = 5\n",
        "# kf = KFold(n_splits=fold, shuffle = True, random_state=seed)\n",
        "# # kf.get_n_splits(input_x)\n",
        "# # print(kf)\n",
        "# from keras import backend as K \n",
        "# K.clear_session()\n",
        "# from sklearn.metrics import r2_score\n",
        "# for train_idx, test_idx in kf.split(input_x,lable):\n",
        "#   # print(train_idx)\n",
        "#   x_train, x_test = input_x[train_idx], input_x[test_idx]\n",
        "#   y_train, y_test = lable[train_idx], lable[test_idx]\n",
        "#   # print(\"x_1:\", x_train.shape, \"y_1:\", y_train.shape)\n",
        "#   model = Sequential([\n",
        "#                       layers.LSTM(64, input_shape=(9, 1)),\n",
        "#                       layers.Dense(256),\n",
        "#                       layers.Dense(128),\n",
        "#                       layers.Dense(64), #relu\n",
        "#                       layers.Dense(1) #no act\n",
        "#                       ])\n",
        "#   opt = keras.optimizers.SGD(learning_rate=0.01) #SGD\n",
        "#   model.compile(optimizer = opt, loss=\"mae\") #or mse\n",
        "#   history = model.fit(x_train, y_train, epochs=10, batch_size = 128, validation_data = (x_test, y_test))\n",
        "#   y_pred = model.predict(x_test, verbose=1)\n",
        "#   score = r2_score(y_test, y_pred)\n",
        "#   # print(\"y_test\",y_test)\n",
        "#   # print(\"y_pred\", y_pred)\n",
        "#   print(score)\n",
        "#   # scores = model.evaluate(x_test, y_test, batch_size = 128)\n",
        "#   # print(\"train loss\",history.history['loss'])\n",
        "#   # print(\"validation loss\",history.history['val_loss'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ni5_Fv_n0faY"
      },
      "source": [
        "# LSTM Model Background Knowledge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "362Ey_FUsKHS"
      },
      "source": [
        "# data with NaN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JsWDbbA9XUik"
      },
      "source": [
        "# f_error = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/error.txt')\n",
        "# gin = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/fin_python.txt')\n",
        "\n",
        "# # f_error\n",
        "# f_error = f_error.sort_values(by = ['center_y','center_x'], ascending = (False, True))\n",
        "# d = 2\n",
        "# r = (d**2 + d**2)**0.5\n",
        "# d_xl = 2*3\n",
        "# x_axis = np.arange(-170,170,d)\n",
        "# y_axis = np.arange(-170,170,d)\n",
        "# X,Y = np.meshgrid(x_axis,y_axis)\n",
        "\n",
        "# plt.plot(X,Y, marker='+', color='grey', linestyle='none',alpha=0.2)\n",
        "# plt.plot(f_error['center_x'],f_error['center_y'], marker='.',markersize=0.4, color='red', linestyle='none')\n",
        "# plt.gca().set_aspect('equal') #set same ratio for x and y axis\n",
        "# plt.show()\n",
        "# gin.head()\n",
        "# len(f_error['center_y'].value_counts())\n",
        "# y_cord = 169\n",
        "# row = 0\n",
        "# arr = []\n",
        "# arr_col = []\n",
        "# while y_cord >= -169:\n",
        "#     arr += [row]*len(f_error[f_error['center_y']==y_cord])\n",
        "#     for col in range(0,len(f_error[f_error['center_y']==y_cord])):\n",
        "#         arr_col += [col]\n",
        "    \n",
        "#     y_cord -= 2\n",
        "#     row += 1\n",
        "    \n",
        "# arr = pd.DataFrame(arr[::-1], columns = [\"row\"])\n",
        "# f_error['row'] = arr\n",
        "\n",
        "# f_error['col'] = arr_col\n",
        "\n",
        "# # f_error = f_error[f_error['error'].notna()]\n",
        "# f_error\n",
        "# row = 0\n",
        "# z_matrix = []\n",
        "# error_matrix = []\n",
        "\n",
        "# while row <= f_error['row'].max():\n",
        "#     pt = f_error[f_error['row']==row]\n",
        "#     grid_cordinate = list(pt['center_z'])\n",
        "#     z_matrix.append(grid_cordinate)\n",
        "    \n",
        "#     error_cordinate = list(pt['error'])\n",
        "#     error_matrix.append(error_cordinate)\n",
        "#     row += 1\n",
        "\n",
        "# from itertools import zip_longest\n",
        "# # pd.DataFrame(point_matrix)\n",
        "# error_matrix = np.array(list(zip_longest(*error_matrix, fillvalue= np.nan)))\n",
        "# # z_matrix = np.array(list(zip_longest(*z_matrix, fillvalue= z_matrix[len(z_matrix)-1])))\n",
        "# z_matrix = np.array(list(zip_longest(*z_matrix, fillvalue= 0)))\n",
        "# error_matrix.shape\n",
        "# z_matrix.shape\n",
        "\n",
        "# def rolling_window(a, shape):\n",
        "#     s = (a.shape[0] - shape[0] + 1,) + (a.shape[1] - shape[1] + 1,) + shape\n",
        "#     strides = a.strides + a.strides\n",
        "#     return np.lib.stride_tricks.as_strided(a, shape=s, strides=strides)\n",
        "\n",
        "# def window2(arr, shape=(3, 3)):\n",
        "#     r_extra = np.floor(shape[0] / 2).astype(int)\n",
        "#     c_extra = np.floor(shape[1] / 2).astype(int)\n",
        "#     out = np.empty((arr.shape[0] + 2 * r_extra, arr.shape[1] + 2 * c_extra))\n",
        "#     out[:] = np.nan\n",
        "#     out[r_extra:-r_extra, c_extra:-c_extra] = arr\n",
        "#     view = rolling_window(out, shape)\n",
        "# #     print(rolling_window(out, shape))\n",
        "#     return view\n",
        "\n",
        "# # window2(z_matrix,(3,3))\n",
        "# error_sqmatrix = window2(error_matrix,(3,3))\n",
        "# z_sqmatrix = window2(z_matrix,(3,3))\n",
        "\n",
        "# # z_sqmatrix.shape\n",
        "# # output:(170, 170, 3, 3)\n",
        "\n",
        "# # z_sqmatrix\n",
        "# model_x = []\n",
        "# import itertools\n",
        "# for row in z_sqmatrix:\n",
        "#     for col in row:\n",
        "# #         print(list(itertools.chain.from_iterable(col)))\n",
        "#         model_x.append(list(itertools.chain.from_iterable(col)))\n",
        "    \n",
        "\n",
        "# # model_x\n",
        "# model_x = np.array(model_x)\n",
        "# model_x.shape\n",
        "# model_y = []\n",
        "\n",
        "# for row in error_sqmatrix:\n",
        "#     for col in row:\n",
        "# #     for col_idx in range(0,3):\n",
        "# #         print(list(itertools.chain.from_iterable(col)))\n",
        "#         model_y.append(list(itertools.chain.from_iterable(col))[4])\n",
        "    \n",
        "# # model_x\n",
        "# model_y = np.array(model_y)\n",
        "# model_y.shape\n",
        "# # model_x = np.nan_to_num(model_x)\n",
        "# # model_y = np.nan_to_num(model_y,nan = f_error['error'].mean())\n",
        "# model_x[0]\n",
        "# model_y[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mytIUSEjKQdQ"
      },
      "source": [
        "**1. LSTM函数：**\n",
        "\n",
        "https://keras.io/zh/layers/recurrent/#lstm\n",
        "\n",
        "```\n",
        "keras.layers.LSTM(units, \n",
        "                  activation='tanh', \n",
        "                  recurrent_activation='hard_sigmoid', \n",
        "                  use_bias=True, \n",
        "                  kernel_initializer='glorot_uniform', \n",
        "                  recurrent_initializer='orthogonal', \n",
        "                  bias_initializer='zeros', \n",
        "                  unit_forget_bias=True, \n",
        "                  kernel_regularizer=None, \n",
        "                  recurrent_regularizer=None, \n",
        "                  bias_regularizer=None, \n",
        "                  activity_regularizer=None, \n",
        "                  kernel_constraint=None, \n",
        "                  recurrent_constraint=None, \n",
        "                  bias_constraint=None, \n",
        "                  dropout=0.0, \n",
        "                  recurrent_dropout=0.0, \n",
        "                  implementation=1, \n",
        "                  return_sequences=False, \n",
        "                  return_state=False, \n",
        "                  go_backwards=False, \n",
        "                  stateful=False, \n",
        "                  unroll=False)\n",
        "\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yl-D6YpsFLJf"
      },
      "source": [
        "**2. Sequential函数: model = Sequential()**\n",
        "\n",
        "https://keras.io/zh/getting-started/sequential-model-guide/\n",
        "\n",
        "<br>\n",
        "\n",
        "**3. Complie函数：**\n",
        "\n",
        "<br>\n",
        "\n",
        "<b> Definition: </b><br>\n",
        "模型编译<br>\n",
        "\n",
        "<b> Parameters: </b><br>\n",
        "* 优化器 optimizer。它可以是现有优化器的字符串标识符，如 rmsprop 或 adagrad，也可以是 Optimizer 类的实例。详见：optimizers。\n",
        "\n",
        "* 损失函数 loss，模型试图最小化的目标函数。它可以是现有损失函数的字符串标识符，如 categorical_crossentropy 或 mse，也可以是一个目标函数。详见：losses。\n",
        "* 评估标准 metrics。评估标准可以是现有的标准的字符串标识符，也可以是自定义的评估标准函数。\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBXHoF9lE0Ev"
      },
      "source": [
        "**4. Dense函数：keras.layers.Dense()**\n",
        "\n",
        "https://keras.io/zh/layers/core/#dense/\n",
        "\n",
        "<br>\n",
        "<b> Definition: </b><br>\n",
        "\n",
        "Dense 实现以下操作： \n",
        "output = activation(dot(input, kernel) + bias) 其中 activation \n",
        "是按逐个元素计算的激活函数，kernel 是由网络层创建的权值矩阵，以及 bias 是其创建的偏置向量\n",
        "\n",
        "```\n",
        "keras.layers.Dense(units, \n",
        "                   activation=None, \n",
        "                   use_bias=True, \n",
        "                   kernel_initializer='glorot_uniform', \n",
        "                   bias_initializer='zeros', \n",
        "                   kernel_regularizer=None, \n",
        "                   bias_regularizer=None, \n",
        "                   activity_regularizer=None, \n",
        "                   kernel_constraint=None, \n",
        "                   bias_constraint=None)\n",
        "```\n",
        "\n",
        "<b> Parameters: </b><br>\n",
        "* units: 正整数，输出空间维度。\n",
        "\n",
        "* activation: 激活函数 (详见 activations)。 若不指定，则不使用激活函数 (即，「线性」激活: a(x) = x)。\n",
        "* use_bias: 布尔值，该层是否使用偏置向量。\n",
        "* kernel_initializer: kernel 权值矩阵的初始化器 (详见 initializers)。\n",
        "* bias_initializer: 偏置向量的初始化器 (see initializers).\n",
        "* kernel_regularizer: 运用到 kernel 权值矩阵的正则化函数 (详见 regularizer)。\n",
        "* bias_regularizer: 运用到偏置向的的正则化函数 (详见 regularizer)。\n",
        "* activity_regularizer: 运用到层的输出的正则化函数 (它的 \"activation\")。 (详见 regularizer)。\n",
        "* kernel_constraint: 运用到 kernel 权值矩阵的约束函数 (详见 constraints)。\n",
        "* bias_constraint: 运用到偏置向量的约束函数 (详见 constraints)。<br>\n",
        "\n",
        ">"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVYuFFLTLJeY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}